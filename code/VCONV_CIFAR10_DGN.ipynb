{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VCONV_CIFAR10_DGN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bd1eabf5873f482f843ea85cea3c2e79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7549508892274232a221f7e2d1a1267a",
              "IPY_MODEL_d6bdda16a1834d14b223520f370ba8e9",
              "IPY_MODEL_a19187abc24546e6a26677f7de732e87"
            ],
            "layout": "IPY_MODEL_550c090a7f7644e3b1387fc4a8d0400d"
          }
        },
        "7549508892274232a221f7e2d1a1267a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7938a4f2933441d9fec5f55a46b2b0f",
            "placeholder": "​",
            "style": "IPY_MODEL_ddfb200b45e940689f9c40b367e6e2f8",
            "value": ""
          }
        },
        "d6bdda16a1834d14b223520f370ba8e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_147bc7367d49473c8a91b1323bce9f22",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6d22c2f289b74b28af369e4fe94d0f6f",
            "value": 170498071
          }
        },
        "a19187abc24546e6a26677f7de732e87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1684fdd5c13f4766a8b95bd69094f34d",
            "placeholder": "​",
            "style": "IPY_MODEL_59b3a30559cd43feb9b734c26b465981",
            "value": " 170499072/? [00:03&lt;00:00, 85681145.47it/s]"
          }
        },
        "550c090a7f7644e3b1387fc4a8d0400d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7938a4f2933441d9fec5f55a46b2b0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddfb200b45e940689f9c40b367e6e2f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "147bc7367d49473c8a91b1323bce9f22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d22c2f289b74b28af369e4fe94d0f6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1684fdd5c13f4766a8b95bd69094f34d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59b3a30559cd43feb9b734c26b465981": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4NZGI_gTE6c3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import argparse\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as initialization\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Constants"
      ],
      "metadata": {
        "id": "dBx2W6gEKbPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_width = 32"
      ],
      "metadata": {
        "id": "oQ_HxUi5Kbe4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eps, beta = 0.1, 4"
      ],
      "metadata": {
        "id": "lUnQFDGXYjMN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load CIFAR10 Dataset"
      ],
      "metadata": {
        "id": "GX5GjvZaIMGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "bd1eabf5873f482f843ea85cea3c2e79",
            "7549508892274232a221f7e2d1a1267a",
            "d6bdda16a1834d14b223520f370ba8e9",
            "a19187abc24546e6a26677f7de732e87",
            "550c090a7f7644e3b1387fc4a8d0400d",
            "f7938a4f2933441d9fec5f55a46b2b0f",
            "ddfb200b45e940689f9c40b367e6e2f8",
            "147bc7367d49473c8a91b1323bce9f22",
            "6d22c2f289b74b28af369e4fe94d0f6f",
            "1684fdd5c13f4766a8b95bd69094f34d",
            "59b3a30559cd43feb9b734c26b465981"
          ]
        },
        "id": "e-SvFrAWFUAu",
        "outputId": "1bd64f81-fae6-4535-c150-3c95834a0a9e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd1eabf5873f482f843ea85cea3c2e79"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "E3JdJ9iPJebX",
        "outputId": "cb1c279c-c932-4e2d-f987-2e5ed3894649"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bird  truck truck cat   dog   cat   cat   plane bird  dog   plane ship  frog  frog  frog  ship  car   ship  frog  dog   car   truck truck ship  cat   deer  bird  horse plane dog   deer  frog \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')\n",
        "\n",
        "device = torch.device('cuda' if train_on_gpu else 'cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2nmfgi862xz",
        "outputId": "314aaa99-c2cb-468f-a98d-969378a374e5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available!  Training on GPU ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "      # get the inputs; data is a list of [inputs, labels]\n",
        "      inputs, labels = data\n",
        "      \n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      # zero the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # forward + backward + optimize\n",
        "      outputs = model(inputs)\n",
        "      \n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # print statistics\n",
        "      running_loss += loss.item()\n",
        "      \n",
        "  print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "            epoch, i * len(data), len(trainloader.dataset),\n",
        "            100. * i / len(trainloader), running_loss))\n",
        "\n",
        "def test():\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  test_loss = 0\n",
        "  model.eval()\n",
        "  # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "  with torch.no_grad():\n",
        "      for data in testloader:\n",
        "          images, labels = data\n",
        "\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "          # calculate outputs by running images through the network\n",
        "          outputs = model(images)\n",
        "\n",
        "          test_loss += criterion(outputs, labels).data\n",
        "          # the class with the highest energy is what we choose as prediction\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          \n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(testloader.dataset),\n",
        "        100. * correct / len(testloader.dataset)))"
      ],
      "metadata": {
        "id": "6YJNkZ3nJwQG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ReLU"
      ],
      "metadata": {
        "id": "jO8C8vMqFUg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv4Relu(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "    \n",
        "      super(Conv4Relu, self).__init__()\n",
        "      self.C1 = nn.Conv2d(3, 64, kernel_size = (3, 3), padding = 'same')\n",
        "      self.C2 = nn.Conv2d(64, 64, kernel_size = (3, 3), padding = 'same')\n",
        "      self.C3 = nn.Conv2d(64, 128, kernel_size = (3, 3), padding = 'same')\n",
        "      self.C4 = nn.Conv2d(128, 128, kernel_size = (3, 3), padding = 'same')\n",
        "\n",
        "      self.flatten = nn.Flatten()\n",
        "      self.D1 = nn.Linear(32*32*128, 256)\n",
        "      self.D2 = nn.Linear(256, 256)\n",
        "\n",
        "      #Output\n",
        "      self.outputs = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "      R = F.relu(self.C1(x))\n",
        "      R = F.relu(self.C2(R))\n",
        "      R = F.relu(self.C3(R))\n",
        "      R = F.relu(self.C4(R))\n",
        "\n",
        "      R = self.flatten(R)\n",
        "\n",
        "      R = F.relu(self.D1(R))\n",
        "      R = F.relu(self.D2(R))\n",
        "\n",
        "      return self.outputs(R)"
      ],
      "metadata": {
        "id": "x7LFMyPoFTEf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Conv4Relu()\n",
        "model.to(device)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kun5T5umKYAF",
        "outputId": "1c5038ae-d0e3-44ed-9a45-bc5674328e92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv4Relu(\n",
              "  (C1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (C2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (C3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (C4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (D1): Linear(in_features=131072, out_features=256, bias=True)\n",
              "  (D2): Linear(in_features=256, out_features=256, bias=True)\n",
              "  (outputs): Linear(in_features=256, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)"
      ],
      "metadata": {
        "id": "QKtrELlXIK3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 81):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tmb8P4VnK54W",
        "outputId": "da985aa0-418a-4c51-cebd-fdd72f07963b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [3124/50000 (100%)]\tLoss: 3066.177735\n",
            "\n",
            "Test set: Average loss: 510.6411, Accuracy: 4101/10000 (41%)\n",
            "\n",
            "Train Epoch: 2 [3124/50000 (100%)]\tLoss: 2390.412157\n",
            "\n",
            "Test set: Average loss: 454.9616, Accuracy: 4756/10000 (48%)\n",
            "\n",
            "Train Epoch: 3 [3124/50000 (100%)]\tLoss: 2120.666588\n",
            "\n",
            "Test set: Average loss: 420.6679, Accuracy: 5155/10000 (52%)\n",
            "\n",
            "Train Epoch: 4 [3124/50000 (100%)]\tLoss: 1912.311433\n",
            "\n",
            "Test set: Average loss: 390.8379, Accuracy: 5560/10000 (56%)\n",
            "\n",
            "Train Epoch: 5 [3124/50000 (100%)]\tLoss: 1725.597467\n",
            "\n",
            "Test set: Average loss: 377.2816, Accuracy: 5723/10000 (57%)\n",
            "\n",
            "Train Epoch: 6 [3124/50000 (100%)]\tLoss: 1542.315742\n",
            "\n",
            "Test set: Average loss: 341.9350, Accuracy: 6120/10000 (61%)\n",
            "\n",
            "Train Epoch: 7 [3124/50000 (100%)]\tLoss: 1345.632795\n",
            "\n",
            "Test set: Average loss: 361.8331, Accuracy: 6015/10000 (60%)\n",
            "\n",
            "Train Epoch: 8 [3124/50000 (100%)]\tLoss: 1127.646560\n",
            "\n",
            "Test set: Average loss: 367.3432, Accuracy: 6055/10000 (61%)\n",
            "\n",
            "Train Epoch: 9 [3124/50000 (100%)]\tLoss: 888.322130\n",
            "\n",
            "Test set: Average loss: 372.5543, Accuracy: 6200/10000 (62%)\n",
            "\n",
            "Train Epoch: 10 [3124/50000 (100%)]\tLoss: 641.941317\n",
            "\n",
            "Test set: Average loss: 405.7244, Accuracy: 6316/10000 (63%)\n",
            "\n",
            "Train Epoch: 11 [3124/50000 (100%)]\tLoss: 432.014033\n",
            "\n",
            "Test set: Average loss: 492.0219, Accuracy: 6033/10000 (60%)\n",
            "\n",
            "Train Epoch: 12 [3124/50000 (100%)]\tLoss: 289.380488\n",
            "\n",
            "Test set: Average loss: 523.7108, Accuracy: 6177/10000 (62%)\n",
            "\n",
            "Train Epoch: 13 [3124/50000 (100%)]\tLoss: 189.071126\n",
            "\n",
            "Test set: Average loss: 572.5990, Accuracy: 6160/10000 (62%)\n",
            "\n",
            "Train Epoch: 14 [3124/50000 (100%)]\tLoss: 130.987912\n",
            "\n",
            "Test set: Average loss: 628.8659, Accuracy: 6384/10000 (64%)\n",
            "\n",
            "Train Epoch: 15 [3124/50000 (100%)]\tLoss: 104.177825\n",
            "\n",
            "Test set: Average loss: 613.9271, Accuracy: 6421/10000 (64%)\n",
            "\n",
            "Train Epoch: 16 [3124/50000 (100%)]\tLoss: 69.385126\n",
            "\n",
            "Test set: Average loss: 693.5432, Accuracy: 6443/10000 (64%)\n",
            "\n",
            "Train Epoch: 17 [3124/50000 (100%)]\tLoss: 50.702778\n",
            "\n",
            "Test set: Average loss: 739.5023, Accuracy: 6438/10000 (64%)\n",
            "\n",
            "Train Epoch: 18 [3124/50000 (100%)]\tLoss: 57.522746\n",
            "\n",
            "Test set: Average loss: 746.8737, Accuracy: 6414/10000 (64%)\n",
            "\n",
            "Train Epoch: 19 [3124/50000 (100%)]\tLoss: 38.945415\n",
            "\n",
            "Test set: Average loss: 810.0139, Accuracy: 6416/10000 (64%)\n",
            "\n",
            "Train Epoch: 20 [3124/50000 (100%)]\tLoss: 40.041196\n",
            "\n",
            "Test set: Average loss: 765.5316, Accuracy: 6481/10000 (65%)\n",
            "\n",
            "Train Epoch: 21 [3124/50000 (100%)]\tLoss: 19.679210\n",
            "\n",
            "Test set: Average loss: 846.8740, Accuracy: 6461/10000 (65%)\n",
            "\n",
            "Train Epoch: 22 [3124/50000 (100%)]\tLoss: 18.360453\n",
            "\n",
            "Test set: Average loss: 879.3728, Accuracy: 6442/10000 (64%)\n",
            "\n",
            "Train Epoch: 23 [3124/50000 (100%)]\tLoss: 32.200908\n",
            "\n",
            "Test set: Average loss: 785.1498, Accuracy: 6547/10000 (65%)\n",
            "\n",
            "Train Epoch: 24 [3124/50000 (100%)]\tLoss: 35.449691\n",
            "\n",
            "Test set: Average loss: 796.6832, Accuracy: 6527/10000 (65%)\n",
            "\n",
            "Train Epoch: 25 [3124/50000 (100%)]\tLoss: 19.586887\n",
            "\n",
            "Test set: Average loss: 856.7688, Accuracy: 6481/10000 (65%)\n",
            "\n",
            "Train Epoch: 26 [3124/50000 (100%)]\tLoss: 12.549230\n",
            "\n",
            "Test set: Average loss: 882.4039, Accuracy: 6555/10000 (66%)\n",
            "\n",
            "Train Epoch: 27 [3124/50000 (100%)]\tLoss: 1.551232\n",
            "\n",
            "Test set: Average loss: 952.7620, Accuracy: 6576/10000 (66%)\n",
            "\n",
            "Train Epoch: 28 [3124/50000 (100%)]\tLoss: 0.299362\n",
            "\n",
            "Test set: Average loss: 973.9106, Accuracy: 6616/10000 (66%)\n",
            "\n",
            "Train Epoch: 29 [3124/50000 (100%)]\tLoss: 0.150945\n",
            "\n",
            "Test set: Average loss: 991.9608, Accuracy: 6604/10000 (66%)\n",
            "\n",
            "Train Epoch: 30 [3124/50000 (100%)]\tLoss: 0.115934\n",
            "\n",
            "Test set: Average loss: 1007.0605, Accuracy: 6606/10000 (66%)\n",
            "\n",
            "Train Epoch: 31 [3124/50000 (100%)]\tLoss: 0.096417\n",
            "\n",
            "Test set: Average loss: 1019.9094, Accuracy: 6618/10000 (66%)\n",
            "\n",
            "Train Epoch: 32 [3124/50000 (100%)]\tLoss: 0.082916\n",
            "\n",
            "Test set: Average loss: 1030.7982, Accuracy: 6616/10000 (66%)\n",
            "\n",
            "Train Epoch: 33 [3124/50000 (100%)]\tLoss: 0.072855\n",
            "\n",
            "Test set: Average loss: 1040.4517, Accuracy: 6621/10000 (66%)\n",
            "\n",
            "Train Epoch: 34 [3124/50000 (100%)]\tLoss: 0.065095\n",
            "\n",
            "Test set: Average loss: 1049.0361, Accuracy: 6617/10000 (66%)\n",
            "\n",
            "Train Epoch: 35 [3124/50000 (100%)]\tLoss: 0.059026\n",
            "\n",
            "Test set: Average loss: 1057.0862, Accuracy: 6617/10000 (66%)\n",
            "\n",
            "Train Epoch: 36 [3124/50000 (100%)]\tLoss: 0.053953\n",
            "\n",
            "Test set: Average loss: 1064.3541, Accuracy: 6616/10000 (66%)\n",
            "\n",
            "Train Epoch: 37 [3124/50000 (100%)]\tLoss: 0.049681\n",
            "\n",
            "Test set: Average loss: 1071.0710, Accuracy: 6618/10000 (66%)\n",
            "\n",
            "Train Epoch: 38 [3124/50000 (100%)]\tLoss: 0.046051\n",
            "\n",
            "Test set: Average loss: 1077.3032, Accuracy: 6619/10000 (66%)\n",
            "\n",
            "Train Epoch: 39 [3124/50000 (100%)]\tLoss: 0.042987\n",
            "\n",
            "Test set: Average loss: 1083.1946, Accuracy: 6617/10000 (66%)\n",
            "\n",
            "Train Epoch: 40 [3124/50000 (100%)]\tLoss: 0.040223\n",
            "\n",
            "Test set: Average loss: 1088.7410, Accuracy: 6617/10000 (66%)\n",
            "\n",
            "Train Epoch: 41 [3124/50000 (100%)]\tLoss: 0.037863\n",
            "\n",
            "Test set: Average loss: 1093.9886, Accuracy: 6617/10000 (66%)\n",
            "\n",
            "Train Epoch: 42 [3124/50000 (100%)]\tLoss: 0.035714\n",
            "\n",
            "Test set: Average loss: 1098.9733, Accuracy: 6621/10000 (66%)\n",
            "\n",
            "Train Epoch: 43 [3124/50000 (100%)]\tLoss: 0.033810\n",
            "\n",
            "Test set: Average loss: 1103.6746, Accuracy: 6617/10000 (66%)\n",
            "\n",
            "Train Epoch: 44 [3124/50000 (100%)]\tLoss: 0.032128\n",
            "\n",
            "Test set: Average loss: 1108.0809, Accuracy: 6617/10000 (66%)\n",
            "\n",
            "Train Epoch: 45 [3124/50000 (100%)]\tLoss: 0.030702\n",
            "\n",
            "Test set: Average loss: 1112.3264, Accuracy: 6620/10000 (66%)\n",
            "\n",
            "Train Epoch: 46 [3124/50000 (100%)]\tLoss: 0.029193\n",
            "\n",
            "Test set: Average loss: 1116.4232, Accuracy: 6621/10000 (66%)\n",
            "\n",
            "Train Epoch: 47 [3124/50000 (100%)]\tLoss: 0.027920\n",
            "\n",
            "Test set: Average loss: 1120.3574, Accuracy: 6622/10000 (66%)\n",
            "\n",
            "Train Epoch: 48 [3124/50000 (100%)]\tLoss: 0.026754\n",
            "\n",
            "Test set: Average loss: 1124.0991, Accuracy: 6622/10000 (66%)\n",
            "\n",
            "Train Epoch: 49 [3124/50000 (100%)]\tLoss: 0.025676\n",
            "\n",
            "Test set: Average loss: 1127.7227, Accuracy: 6621/10000 (66%)\n",
            "\n",
            "Train Epoch: 50 [3124/50000 (100%)]\tLoss: 0.024683\n",
            "\n",
            "Test set: Average loss: 1131.1735, Accuracy: 6622/10000 (66%)\n",
            "\n",
            "Train Epoch: 51 [3124/50000 (100%)]\tLoss: 0.023762\n",
            "\n",
            "Test set: Average loss: 1134.5475, Accuracy: 6620/10000 (66%)\n",
            "\n",
            "Train Epoch: 52 [3124/50000 (100%)]\tLoss: 0.022919\n",
            "\n",
            "Test set: Average loss: 1137.8329, Accuracy: 6620/10000 (66%)\n",
            "\n",
            "Train Epoch: 53 [3124/50000 (100%)]\tLoss: 0.022120\n",
            "\n",
            "Test set: Average loss: 1140.9717, Accuracy: 6618/10000 (66%)\n",
            "\n",
            "Train Epoch: 54 [3124/50000 (100%)]\tLoss: 0.021371\n",
            "\n",
            "Test set: Average loss: 1144.0088, Accuracy: 6617/10000 (66%)\n",
            "\n",
            "Train Epoch: 55 [3124/50000 (100%)]\tLoss: 0.020676\n",
            "\n",
            "Test set: Average loss: 1146.9796, Accuracy: 6617/10000 (66%)\n",
            "\n",
            "Train Epoch: 56 [3124/50000 (100%)]\tLoss: 0.020031\n",
            "\n",
            "Test set: Average loss: 1149.9244, Accuracy: 6615/10000 (66%)\n",
            "\n",
            "Train Epoch: 57 [3124/50000 (100%)]\tLoss: 0.019413\n",
            "\n",
            "Test set: Average loss: 1152.6707, Accuracy: 6614/10000 (66%)\n",
            "\n",
            "Train Epoch: 58 [3124/50000 (100%)]\tLoss: 0.018838\n",
            "\n",
            "Test set: Average loss: 1155.3881, Accuracy: 6614/10000 (66%)\n",
            "\n",
            "Train Epoch: 59 [3124/50000 (100%)]\tLoss: 0.018294\n",
            "\n",
            "Test set: Average loss: 1158.0392, Accuracy: 6613/10000 (66%)\n",
            "\n",
            "Train Epoch: 60 [3124/50000 (100%)]\tLoss: 0.017783\n",
            "\n",
            "Test set: Average loss: 1160.6229, Accuracy: 6612/10000 (66%)\n",
            "\n",
            "Train Epoch: 61 [3124/50000 (100%)]\tLoss: 0.017298\n",
            "\n",
            "Test set: Average loss: 1163.1261, Accuracy: 6613/10000 (66%)\n",
            "\n",
            "Train Epoch: 62 [3124/50000 (100%)]\tLoss: 0.016834\n",
            "\n",
            "Test set: Average loss: 1165.5608, Accuracy: 6612/10000 (66%)\n",
            "\n",
            "Train Epoch: 63 [3124/50000 (100%)]\tLoss: 0.016408\n",
            "\n",
            "Test set: Average loss: 1167.9109, Accuracy: 6612/10000 (66%)\n",
            "\n",
            "Train Epoch: 64 [3124/50000 (100%)]\tLoss: 0.015988\n",
            "\n",
            "Test set: Average loss: 1170.2483, Accuracy: 6613/10000 (66%)\n",
            "\n",
            "Train Epoch: 65 [3124/50000 (100%)]\tLoss: 0.015595\n",
            "\n",
            "Test set: Average loss: 1172.5131, Accuracy: 6612/10000 (66%)\n",
            "\n",
            "Train Epoch: 66 [3124/50000 (100%)]\tLoss: 0.015221\n",
            "\n",
            "Test set: Average loss: 1174.7211, Accuracy: 6611/10000 (66%)\n",
            "\n",
            "Train Epoch: 67 [3124/50000 (100%)]\tLoss: 0.014858\n",
            "\n",
            "Test set: Average loss: 1176.8696, Accuracy: 6610/10000 (66%)\n",
            "\n",
            "Train Epoch: 68 [3124/50000 (100%)]\tLoss: 0.014516\n",
            "\n",
            "Test set: Average loss: 1179.0039, Accuracy: 6611/10000 (66%)\n",
            "\n",
            "Train Epoch: 69 [3124/50000 (100%)]\tLoss: 0.014190\n",
            "\n",
            "Test set: Average loss: 1181.0862, Accuracy: 6610/10000 (66%)\n",
            "\n",
            "Train Epoch: 70 [3124/50000 (100%)]\tLoss: 0.013877\n",
            "\n",
            "Test set: Average loss: 1183.1248, Accuracy: 6611/10000 (66%)\n",
            "\n",
            "Train Epoch: 71 [3124/50000 (100%)]\tLoss: 0.013577\n",
            "\n",
            "Test set: Average loss: 1185.1116, Accuracy: 6611/10000 (66%)\n",
            "\n",
            "Train Epoch: 72 [3124/50000 (100%)]\tLoss: 0.013295\n",
            "\n",
            "Test set: Average loss: 1187.0574, Accuracy: 6611/10000 (66%)\n",
            "\n",
            "Train Epoch: 73 [3124/50000 (100%)]\tLoss: 0.013015\n",
            "\n",
            "Test set: Average loss: 1188.9618, Accuracy: 6611/10000 (66%)\n",
            "\n",
            "Train Epoch: 74 [3124/50000 (100%)]\tLoss: 0.012757\n",
            "\n",
            "Test set: Average loss: 1190.8462, Accuracy: 6609/10000 (66%)\n",
            "\n",
            "Train Epoch: 75 [3124/50000 (100%)]\tLoss: 0.012497\n",
            "\n",
            "Test set: Average loss: 1192.7108, Accuracy: 6607/10000 (66%)\n",
            "\n",
            "Train Epoch: 76 [3124/50000 (100%)]\tLoss: 0.012262\n",
            "\n",
            "Test set: Average loss: 1194.4994, Accuracy: 6609/10000 (66%)\n",
            "\n",
            "Train Epoch: 77 [3124/50000 (100%)]\tLoss: 0.012018\n",
            "\n",
            "Test set: Average loss: 1196.2720, Accuracy: 6608/10000 (66%)\n",
            "\n",
            "Train Epoch: 78 [3124/50000 (100%)]\tLoss: 0.011790\n",
            "\n",
            "Test set: Average loss: 1198.0446, Accuracy: 6608/10000 (66%)\n",
            "\n",
            "Train Epoch: 79 [3124/50000 (100%)]\tLoss: 0.011574\n",
            "\n",
            "Test set: Average loss: 1199.7549, Accuracy: 6607/10000 (66%)\n",
            "\n",
            "Train Epoch: 80 [3124/50000 (100%)]\tLoss: 0.011362\n",
            "\n",
            "Test set: Average loss: 1201.4468, Accuracy: 6608/10000 (66%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "R5CeWiXVY3vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftGate(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(SoftGate, self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(SoftGate, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def forward(self, x):\n",
        "        activation = (1 + eps)*F.sigmoid(beta*x)\n",
        "        return activation\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape"
      ],
      "metadata": {
        "id": "xU_VzpaZczZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoupled learning with Soft ReLU activation"
      ],
      "metadata": {
        "id": "zY5cFU-PYDJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoupledLearning(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "      \n",
        "      super(DecoupledLearning, self).__init__()\n",
        "\n",
        "      self.C1 = nn.Conv2d(3, 64, kernel_size = (3, 3), padding = 'same')\n",
        "      self.C2 = nn.Conv2d(64, 64, kernel_size = (3, 3), padding = 'same')\n",
        "      self.C3 = nn.Conv2d(64, 128, kernel_size = (3, 3), padding = 'same')\n",
        "      self.C4 = nn.Conv2d(128, 128, kernel_size = (3, 3), padding = 'same')\n",
        "\n",
        "      self.F1 = nn.Flatten()\n",
        "      self.D1 = nn.Linear(32*32*128, 256)\n",
        "      self.D2 = nn.Linear(256, 256)\n",
        "\n",
        "      #V2\n",
        "      self.C1_G = nn.Conv2d(3, 64, kernel_size = (3, 3), padding = 'same')\n",
        "      self.C2_G = nn.Conv2d(64, 64, kernel_size = (3, 3), padding = 'same')\n",
        "      self.C3_G = nn.Conv2d(64, 128, kernel_size = (3, 3), padding = 'same')\n",
        "      self.C4_G = nn.Conv2d(128, 128, kernel_size = (3, 3),  padding = 'same')\n",
        "      self.F1_G = nn.Flatten()\n",
        "      self.D1_G = nn.Linear(32*32*128, 256)\n",
        "      self.D2_G = nn.Linear(256, 256)\n",
        "      self.outputs = nn.Linear(256, 10)\n",
        "\n",
        "    \n",
        "  def forward(self, x):\n",
        "\n",
        "      #V1\n",
        "      C1 = self.C1(x)\n",
        "      C1_A = F.relu(C1)\n",
        "      C2 = self.C2(C1_A)\n",
        "      C2_A = F.relu(C2)\n",
        "      C3 = self.C3(C2_A)\n",
        "      C3_A = F.relu(C3)\n",
        "      C4 = self.C4(C3_A)\n",
        "      C4_A = F.relu(C4)\n",
        "\n",
        "      F1 = self.F1(C4_A)\n",
        "      D1 = self.D1(F1)\n",
        "      D1_A = F.relu(D1)\n",
        "      D2 = self.D2(D1_A)\n",
        "\n",
        "      A1 = SoftGate()(C1)\n",
        "      A2 = SoftGate()(C2)\n",
        "      A3 = SoftGate()(C3)\n",
        "      A4 = SoftGate()(C4)\n",
        "      A5 = SoftGate()(D1)\n",
        "      A6 = SoftGate()(D2)\n",
        "\n",
        "      #V2\n",
        "      C1_G = self.C1_G(x)\n",
        "      C1_G = torch.mul(C1_G, A1)\n",
        "      C2_G = self.C2_G(C1_G)\n",
        "      C2_G = torch.mul(C2_G, A2)\n",
        "      C3_G = self.C3_G(C2_G)\n",
        "      C3_G = torch.mul(C3_G, A3)\n",
        "      C4_G = self.C4_G(C3_G)\n",
        "      C4_G = torch.mul(C4_G, A4)\n",
        "\n",
        "      F1_G = self.F1_G(C4_G)\n",
        "      D1_G = self.D1_G(F1_G)\n",
        "      D1_G = torch.mul(D1_G, A5)\n",
        "      D2_G = self.D2_G(D1_G)\n",
        "      D2_G = torch.mul(D2_G, A6)\n",
        "\n",
        "      return self.outputs(D2_G)"
      ],
      "metadata": {
        "id": "2UP6jfMYX7KA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DecoupledLearning()\n",
        "model.to(device)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fo4enldo5HXD",
        "outputId": "62b26538-4bc1-4876-c61f-a3cbd9a6c34d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecoupledLearning(\n",
              "  (C1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (C2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (C3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (C4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (F1): Flatten(start_dim=1, end_dim=-1)\n",
              "  (D1): Linear(in_features=131072, out_features=256, bias=True)\n",
              "  (D2): Linear(in_features=256, out_features=256, bias=True)\n",
              "  (C1_G): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (C2_G): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (C3_G): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (C4_G): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (F1_G): Flatten(start_dim=1, end_dim=-1)\n",
              "  (D1_G): Linear(in_features=131072, out_features=256, bias=True)\n",
              "  (D2_G): Linear(in_features=256, out_features=256, bias=True)\n",
              "  (outputs): Linear(in_features=256, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "for epoch in range(1, 81):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hlWWNdUQgzk",
        "outputId": "6bb2dbe7-7cd5-417a-eb12-eb6649daa8c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [3124/50000 (100%)]\tLoss: 3463.589293\n",
            "\n",
            "Test set: Average loss: 606.6343, Accuracy: 3286/10000 (33%)\n",
            "\n",
            "Train Epoch: 2 [3124/50000 (100%)]\tLoss: 2655.470029\n",
            "\n",
            "Test set: Average loss: 485.9721, Accuracy: 4453/10000 (45%)\n",
            "\n",
            "Train Epoch: 3 [3124/50000 (100%)]\tLoss: 2243.734135\n",
            "\n",
            "Test set: Average loss: 422.9943, Accuracy: 5213/10000 (52%)\n",
            "\n",
            "Train Epoch: 4 [3124/50000 (100%)]\tLoss: 2010.825334\n",
            "\n",
            "Test set: Average loss: 397.8923, Accuracy: 5477/10000 (55%)\n",
            "\n",
            "Train Epoch: 5 [3124/50000 (100%)]\tLoss: 1813.163920\n",
            "\n",
            "Test set: Average loss: 379.4728, Accuracy: 5776/10000 (58%)\n",
            "\n",
            "Train Epoch: 6 [3124/50000 (100%)]\tLoss: 1619.147882\n",
            "\n",
            "Test set: Average loss: 353.9225, Accuracy: 6007/10000 (60%)\n",
            "\n",
            "Train Epoch: 7 [3124/50000 (100%)]\tLoss: 1395.563984\n",
            "\n",
            "Test set: Average loss: 351.5441, Accuracy: 6142/10000 (61%)\n",
            "\n",
            "Train Epoch: 8 [3124/50000 (100%)]\tLoss: 1140.208155\n",
            "\n",
            "Test set: Average loss: 357.5344, Accuracy: 6264/10000 (63%)\n",
            "\n",
            "Train Epoch: 9 [3124/50000 (100%)]\tLoss: 835.329778\n",
            "\n",
            "Test set: Average loss: 370.6104, Accuracy: 6307/10000 (63%)\n",
            "\n",
            "Train Epoch: 10 [3124/50000 (100%)]\tLoss: 555.117944\n",
            "\n",
            "Test set: Average loss: 452.6501, Accuracy: 6131/10000 (61%)\n",
            "\n",
            "Train Epoch: 11 [3124/50000 (100%)]\tLoss: 364.008069\n",
            "\n",
            "Test set: Average loss: 494.2747, Accuracy: 6251/10000 (63%)\n",
            "\n",
            "Train Epoch: 12 [3124/50000 (100%)]\tLoss: 245.281295\n",
            "\n",
            "Test set: Average loss: 560.7576, Accuracy: 6227/10000 (62%)\n",
            "\n",
            "Train Epoch: 13 [3124/50000 (100%)]\tLoss: 183.657562\n",
            "\n",
            "Test set: Average loss: 618.1487, Accuracy: 6226/10000 (62%)\n",
            "\n",
            "Train Epoch: 14 [3124/50000 (100%)]\tLoss: 139.435834\n",
            "\n",
            "Test set: Average loss: 656.3809, Accuracy: 6277/10000 (63%)\n",
            "\n",
            "Train Epoch: 15 [3124/50000 (100%)]\tLoss: 117.380142\n",
            "\n",
            "Test set: Average loss: 686.2482, Accuracy: 6200/10000 (62%)\n",
            "\n",
            "Train Epoch: 16 [3124/50000 (100%)]\tLoss: 117.547574\n",
            "\n",
            "Test set: Average loss: 688.3908, Accuracy: 6144/10000 (61%)\n",
            "\n",
            "Train Epoch: 17 [3124/50000 (100%)]\tLoss: 99.628978\n",
            "\n",
            "Test set: Average loss: 744.7277, Accuracy: 6198/10000 (62%)\n",
            "\n",
            "Train Epoch: 18 [3124/50000 (100%)]\tLoss: 71.369013\n",
            "\n",
            "Test set: Average loss: 772.6834, Accuracy: 6226/10000 (62%)\n",
            "\n",
            "Train Epoch: 19 [3124/50000 (100%)]\tLoss: 64.221407\n",
            "\n",
            "Test set: Average loss: 808.5295, Accuracy: 6205/10000 (62%)\n",
            "\n",
            "Train Epoch: 20 [3124/50000 (100%)]\tLoss: 50.219951\n",
            "\n",
            "Test set: Average loss: 847.9236, Accuracy: 6285/10000 (63%)\n",
            "\n",
            "Train Epoch: 21 [3124/50000 (100%)]\tLoss: 30.955825\n",
            "\n",
            "Test set: Average loss: 866.3740, Accuracy: 6313/10000 (63%)\n",
            "\n",
            "Train Epoch: 22 [3124/50000 (100%)]\tLoss: 41.064350\n",
            "\n",
            "Test set: Average loss: 867.4128, Accuracy: 6322/10000 (63%)\n",
            "\n",
            "Train Epoch: 23 [3124/50000 (100%)]\tLoss: 34.589407\n",
            "\n",
            "Test set: Average loss: 911.2433, Accuracy: 6260/10000 (63%)\n",
            "\n",
            "Train Epoch: 24 [3124/50000 (100%)]\tLoss: 32.904603\n",
            "\n",
            "Test set: Average loss: 909.7153, Accuracy: 6282/10000 (63%)\n",
            "\n",
            "Train Epoch: 25 [3124/50000 (100%)]\tLoss: 26.867203\n",
            "\n",
            "Test set: Average loss: 931.0165, Accuracy: 6339/10000 (63%)\n",
            "\n",
            "Train Epoch: 26 [3124/50000 (100%)]\tLoss: 30.971741\n",
            "\n",
            "Test set: Average loss: 925.1609, Accuracy: 6250/10000 (62%)\n",
            "\n",
            "Train Epoch: 27 [3124/50000 (100%)]\tLoss: 26.066788\n",
            "\n",
            "Test set: Average loss: 916.6090, Accuracy: 6269/10000 (63%)\n",
            "\n",
            "Train Epoch: 28 [3124/50000 (100%)]\tLoss: 22.961260\n",
            "\n",
            "Test set: Average loss: 941.2902, Accuracy: 6320/10000 (63%)\n",
            "\n",
            "Train Epoch: 29 [3124/50000 (100%)]\tLoss: 22.198842\n",
            "\n",
            "Test set: Average loss: 1035.2600, Accuracy: 6276/10000 (63%)\n",
            "\n",
            "Train Epoch: 30 [3124/50000 (100%)]\tLoss: 31.709987\n",
            "\n",
            "Test set: Average loss: 962.0900, Accuracy: 6285/10000 (63%)\n",
            "\n",
            "Train Epoch: 31 [3124/50000 (100%)]\tLoss: 20.499038\n",
            "\n",
            "Test set: Average loss: 997.0351, Accuracy: 6230/10000 (62%)\n",
            "\n",
            "Train Epoch: 32 [3124/50000 (100%)]\tLoss: 20.097244\n",
            "\n",
            "Test set: Average loss: 1045.9684, Accuracy: 6207/10000 (62%)\n",
            "\n",
            "Train Epoch: 33 [3124/50000 (100%)]\tLoss: 18.033069\n",
            "\n",
            "Test set: Average loss: 1017.3822, Accuracy: 6329/10000 (63%)\n",
            "\n",
            "Train Epoch: 34 [3124/50000 (100%)]\tLoss: 19.928897\n",
            "\n",
            "Test set: Average loss: 1020.6086, Accuracy: 6337/10000 (63%)\n",
            "\n",
            "Train Epoch: 35 [3124/50000 (100%)]\tLoss: 18.133669\n",
            "\n",
            "Test set: Average loss: 1054.5300, Accuracy: 6274/10000 (63%)\n",
            "\n",
            "Train Epoch: 36 [3124/50000 (100%)]\tLoss: 13.891521\n",
            "\n",
            "Test set: Average loss: 1100.0182, Accuracy: 6353/10000 (64%)\n",
            "\n",
            "Train Epoch: 37 [3124/50000 (100%)]\tLoss: 28.396090\n",
            "\n",
            "Test set: Average loss: 972.6080, Accuracy: 6289/10000 (63%)\n",
            "\n",
            "Train Epoch: 38 [3124/50000 (100%)]\tLoss: 25.694080\n",
            "\n",
            "Test set: Average loss: 1057.8131, Accuracy: 6273/10000 (63%)\n",
            "\n",
            "Train Epoch: 39 [3124/50000 (100%)]\tLoss: 12.438685\n",
            "\n",
            "Test set: Average loss: 1089.4653, Accuracy: 6383/10000 (64%)\n",
            "\n",
            "Train Epoch: 40 [3124/50000 (100%)]\tLoss: 11.211580\n",
            "\n",
            "Test set: Average loss: 1073.6875, Accuracy: 6226/10000 (62%)\n",
            "\n",
            "Train Epoch: 41 [3124/50000 (100%)]\tLoss: 15.788136\n",
            "\n",
            "Test set: Average loss: 1064.1260, Accuracy: 6220/10000 (62%)\n",
            "\n",
            "Train Epoch: 42 [3124/50000 (100%)]\tLoss: 12.808128\n",
            "\n",
            "Test set: Average loss: 1134.6719, Accuracy: 6249/10000 (62%)\n",
            "\n",
            "Train Epoch: 43 [3124/50000 (100%)]\tLoss: 10.433961\n",
            "\n",
            "Test set: Average loss: 1104.0875, Accuracy: 6340/10000 (63%)\n",
            "\n",
            "Train Epoch: 44 [3124/50000 (100%)]\tLoss: 11.328538\n",
            "\n",
            "Test set: Average loss: 1092.4265, Accuracy: 6315/10000 (63%)\n",
            "\n",
            "Train Epoch: 45 [3124/50000 (100%)]\tLoss: 15.056614\n",
            "\n",
            "Test set: Average loss: 1088.6666, Accuracy: 6282/10000 (63%)\n",
            "\n",
            "Train Epoch: 46 [3124/50000 (100%)]\tLoss: 5.208906\n",
            "\n",
            "Test set: Average loss: 1102.5149, Accuracy: 6366/10000 (64%)\n",
            "\n",
            "Train Epoch: 47 [3124/50000 (100%)]\tLoss: 6.047705\n",
            "\n",
            "Test set: Average loss: 1178.3779, Accuracy: 6353/10000 (64%)\n",
            "\n",
            "Train Epoch: 48 [3124/50000 (100%)]\tLoss: 1.035799\n",
            "\n",
            "Test set: Average loss: 1200.3743, Accuracy: 6367/10000 (64%)\n",
            "\n",
            "Train Epoch: 49 [3124/50000 (100%)]\tLoss: 0.153060\n",
            "\n",
            "Test set: Average loss: 1207.2515, Accuracy: 6373/10000 (64%)\n",
            "\n",
            "Train Epoch: 50 [3124/50000 (100%)]\tLoss: 0.072930\n",
            "\n",
            "Test set: Average loss: 1217.0848, Accuracy: 6370/10000 (64%)\n",
            "\n",
            "Train Epoch: 51 [3124/50000 (100%)]\tLoss: 0.058829\n",
            "\n",
            "Test set: Average loss: 1225.3076, Accuracy: 6377/10000 (64%)\n",
            "\n",
            "Train Epoch: 52 [3124/50000 (100%)]\tLoss: 0.050475\n",
            "\n",
            "Test set: Average loss: 1232.6703, Accuracy: 6378/10000 (64%)\n",
            "\n",
            "Train Epoch: 53 [3124/50000 (100%)]\tLoss: 0.044519\n",
            "\n",
            "Test set: Average loss: 1239.3612, Accuracy: 6379/10000 (64%)\n",
            "\n",
            "Train Epoch: 54 [3124/50000 (100%)]\tLoss: 0.039988\n",
            "\n",
            "Test set: Average loss: 1245.2484, Accuracy: 6378/10000 (64%)\n",
            "\n",
            "Train Epoch: 55 [3124/50000 (100%)]\tLoss: 0.036444\n",
            "\n",
            "Test set: Average loss: 1250.7528, Accuracy: 6375/10000 (64%)\n",
            "\n",
            "Train Epoch: 56 [3124/50000 (100%)]\tLoss: 0.033532\n",
            "\n",
            "Test set: Average loss: 1255.8007, Accuracy: 6377/10000 (64%)\n",
            "\n",
            "Train Epoch: 57 [3124/50000 (100%)]\tLoss: 0.031096\n",
            "\n",
            "Test set: Average loss: 1260.3601, Accuracy: 6380/10000 (64%)\n",
            "\n",
            "Train Epoch: 58 [3124/50000 (100%)]\tLoss: 0.029023\n",
            "\n",
            "Test set: Average loss: 1264.8335, Accuracy: 6378/10000 (64%)\n",
            "\n",
            "Train Epoch: 59 [3124/50000 (100%)]\tLoss: 0.027249\n",
            "\n",
            "Test set: Average loss: 1268.9491, Accuracy: 6381/10000 (64%)\n",
            "\n",
            "Train Epoch: 60 [3124/50000 (100%)]\tLoss: 0.025651\n",
            "\n",
            "Test set: Average loss: 1272.8059, Accuracy: 6381/10000 (64%)\n",
            "\n",
            "Train Epoch: 61 [3124/50000 (100%)]\tLoss: 0.024270\n",
            "\n",
            "Test set: Average loss: 1276.5519, Accuracy: 6382/10000 (64%)\n",
            "\n",
            "Train Epoch: 62 [3124/50000 (100%)]\tLoss: 0.023032\n",
            "\n",
            "Test set: Average loss: 1280.0833, Accuracy: 6382/10000 (64%)\n",
            "\n",
            "Train Epoch: 63 [3124/50000 (100%)]\tLoss: 0.021928\n",
            "\n",
            "Test set: Average loss: 1283.4077, Accuracy: 6384/10000 (64%)\n",
            "\n",
            "Train Epoch: 64 [3124/50000 (100%)]\tLoss: 0.020937\n",
            "\n",
            "Test set: Average loss: 1286.6169, Accuracy: 6383/10000 (64%)\n",
            "\n",
            "Train Epoch: 65 [3124/50000 (100%)]\tLoss: 0.020046\n",
            "\n",
            "Test set: Average loss: 1289.7242, Accuracy: 6384/10000 (64%)\n",
            "\n",
            "Train Epoch: 66 [3124/50000 (100%)]\tLoss: 0.019199\n",
            "\n",
            "Test set: Average loss: 1292.6969, Accuracy: 6386/10000 (64%)\n",
            "\n",
            "Train Epoch: 67 [3124/50000 (100%)]\tLoss: 0.018436\n",
            "\n",
            "Test set: Average loss: 1295.5684, Accuracy: 6388/10000 (64%)\n",
            "\n",
            "Train Epoch: 68 [3124/50000 (100%)]\tLoss: 0.017741\n",
            "\n",
            "Test set: Average loss: 1298.2902, Accuracy: 6386/10000 (64%)\n",
            "\n",
            "Train Epoch: 69 [3124/50000 (100%)]\tLoss: 0.017099\n",
            "\n",
            "Test set: Average loss: 1300.8992, Accuracy: 6386/10000 (64%)\n",
            "\n",
            "Train Epoch: 70 [3124/50000 (100%)]\tLoss: 0.016494\n",
            "\n",
            "Test set: Average loss: 1303.5020, Accuracy: 6386/10000 (64%)\n",
            "\n",
            "Train Epoch: 71 [3124/50000 (100%)]\tLoss: 0.015938\n",
            "\n",
            "Test set: Average loss: 1305.9651, Accuracy: 6386/10000 (64%)\n",
            "\n",
            "Train Epoch: 72 [3124/50000 (100%)]\tLoss: 0.015430\n",
            "\n",
            "Test set: Average loss: 1308.3674, Accuracy: 6387/10000 (64%)\n",
            "\n",
            "Train Epoch: 73 [3124/50000 (100%)]\tLoss: 0.014932\n",
            "\n",
            "Test set: Average loss: 1310.7087, Accuracy: 6388/10000 (64%)\n",
            "\n",
            "Train Epoch: 74 [3124/50000 (100%)]\tLoss: 0.014478\n",
            "\n",
            "Test set: Average loss: 1312.9535, Accuracy: 6394/10000 (64%)\n",
            "\n",
            "Train Epoch: 75 [3124/50000 (100%)]\tLoss: 0.014050\n",
            "\n",
            "Test set: Average loss: 1315.1514, Accuracy: 6395/10000 (64%)\n",
            "\n",
            "Train Epoch: 76 [3124/50000 (100%)]\tLoss: 0.013652\n",
            "\n",
            "Test set: Average loss: 1317.2898, Accuracy: 6396/10000 (64%)\n",
            "\n",
            "Train Epoch: 77 [3124/50000 (100%)]\tLoss: 0.013271\n",
            "\n",
            "Test set: Average loss: 1319.3776, Accuracy: 6396/10000 (64%)\n",
            "\n",
            "Train Epoch: 78 [3124/50000 (100%)]\tLoss: 0.012933\n",
            "\n",
            "Test set: Average loss: 1321.4160, Accuracy: 6397/10000 (64%)\n",
            "\n",
            "Train Epoch: 79 [3124/50000 (100%)]\tLoss: 0.012571\n",
            "\n",
            "Test set: Average loss: 1323.3922, Accuracy: 6397/10000 (64%)\n",
            "\n",
            "Train Epoch: 80 [3124/50000 (100%)]\tLoss: 0.012249\n",
            "\n",
            "Test set: Average loss: 1325.3182, Accuracy: 6397/10000 (64%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DGN"
      ],
      "metadata": {
        "id": "d-Wl9WC6c7vz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SignGate(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(SignGate, self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(SignGate, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = torch.sign(F.relu(x))\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape"
      ],
      "metadata": {
        "id": "wG2GoOezc9v2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv4Galu(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    \n",
        "    super(Conv4Galu, self).__init__()\n",
        "\n",
        "    #V1\n",
        "    self.N1_C1 = nn.Conv2d(3, 64, kernel_size = (3, 3), padding = 'same')\n",
        "    self.N1_C2 = nn.Conv2d(64, 64, kernel_size = (3, 3), padding = 'same')\n",
        "    self.N1_C3 = nn.Conv2d(64, 128, kernel_size = (3, 3), padding = 'same')\n",
        "    self.N1_C4 = nn.Conv2d(128, 128, kernel_size = (3, 3), padding = 'same')\n",
        "\n",
        "    self.N1_F1 = nn.Flatten()\n",
        "    self.N1_D1 = nn.Linear(32*32*128, 256)\n",
        "    self.N1_D2 = nn.Linear(256, 256)\n",
        "\n",
        "\n",
        "    #V2\n",
        "    self.N2_C1 = nn.Conv2d(3, 64, kernel_size = (3, 3), padding = 'same')\n",
        "    self.N2_C2 = nn.Conv2d(64, 64, kernel_size = (3, 3), padding = 'same')\n",
        "    self.N2_C3 = nn.Conv2d(64, 128, kernel_size = (3, 3), padding = 'same')\n",
        "    self.N2_C4 = nn.Conv2d(128, 128, kernel_size = (3, 3),  padding = 'same')\n",
        "    \n",
        "    self.N2_F1 = nn.Flatten()\n",
        "    self.N2_D1 = nn.Linear(32*32*128, 256)\n",
        "    self.N2_D2 = nn.Linear(256, 256)\n",
        "\n",
        "    self.outputs = nn.Linear(256, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    #V1\n",
        "    C1 = F.relu(self.N1_C1(x))\n",
        "    C2 = F.relu(self.N1_C2(C1))\n",
        "    C3 = F.relu(self.N1_C3(C2))\n",
        "    C4 = F.relu(self.N1_C4(C3))\n",
        "\n",
        "    F1 = self.N1_F1(C4)\n",
        "    D1 = F.relu(self.N1_D1(F1))\n",
        "    D2 = F.relu(self.N1_D2(D1))\n",
        "\n",
        "    \n",
        "    A1 = SignGate()(C1)\n",
        "    A2 = SignGate()(C2)\n",
        "    A3 = SignGate()(C3)\n",
        "    A4 = SignGate()(C4)\n",
        "    A5 = SignGate()(D1)\n",
        "    A6 = SignGate()(D2)\n",
        "\n",
        "    #V2\n",
        "    C1_G = self.N2_C1(x)\n",
        "    C1_G = torch.mul(C1_G, A1)\n",
        "    C2_G = self.N2_C2(C1_G)\n",
        "    C2_G = torch.mul(C2_G, A2)\n",
        "    C3_G = self.N2_C3(C2_G)\n",
        "    C3_G = torch.mul(C3_G, A3)\n",
        "    C4_G = self.N2_C4(C3_G)\n",
        "    C4_G = torch.mul(C4_G, A4)\n",
        "\n",
        "    F1_G = self.N2_F1(C4_G)\n",
        "    D1_G = self.N2_D1(F1_G)\n",
        "    D1_G = torch.mul(D1_G, A5)\n",
        "    D2_G = self.N2_D2(D1_G)\n",
        "    D2_G = torch.mul(D2_G, A6)\n",
        "\n",
        "    return self.outputs(D2_G)"
      ],
      "metadata": {
        "id": "kC1ZLrwBc4KG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoupled Learning with hard relu"
      ],
      "metadata": {
        "id": "iO5GLTgwQwd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Conv4Galu()\n",
        "model.to(device)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W03Nzvtl5Nmq",
        "outputId": "7bf2aee4-7552-4612-bc0f-78363dc24e28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv4Galu(\n",
              "  (N1_C1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N1_C2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N1_C3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N1_C4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N1_F1): Flatten(start_dim=1, end_dim=-1)\n",
              "  (N1_D1): Linear(in_features=131072, out_features=256, bias=True)\n",
              "  (N1_D2): Linear(in_features=256, out_features=256, bias=True)\n",
              "  (N2_C1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N2_C2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N2_C3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N2_C4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N2_F1): Flatten(start_dim=1, end_dim=-1)\n",
              "  (N2_D1): Linear(in_features=131072, out_features=256, bias=True)\n",
              "  (N2_D2): Linear(in_features=256, out_features=256, bias=True)\n",
              "  (outputs): Linear(in_features=256, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "for epoch in range(1, 81):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2F39v0AQ2WB",
        "outputId": "81cd087a-8ace-4e8a-c19d-3a00b1cfb914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [3124/50000 (100%)]\tLoss: 3008.702818\n",
            "\n",
            "Test set: Average loss: 509.3687, Accuracy: 4262/10000 (43%)\n",
            "\n",
            "Train Epoch: 2 [3124/50000 (100%)]\tLoss: 2168.051513\n",
            "\n",
            "Test set: Average loss: 409.9548, Accuracy: 5317/10000 (53%)\n",
            "\n",
            "Train Epoch: 3 [3124/50000 (100%)]\tLoss: 1779.211344\n",
            "\n",
            "Test set: Average loss: 379.4973, Accuracy: 5719/10000 (57%)\n",
            "\n",
            "Train Epoch: 4 [3124/50000 (100%)]\tLoss: 1400.781019\n",
            "\n",
            "Test set: Average loss: 394.4732, Accuracy: 5737/10000 (57%)\n",
            "\n",
            "Train Epoch: 5 [3124/50000 (100%)]\tLoss: 917.882225\n",
            "\n",
            "Test set: Average loss: 445.9763, Accuracy: 5646/10000 (56%)\n",
            "\n",
            "Train Epoch: 6 [3124/50000 (100%)]\tLoss: 456.357554\n",
            "\n",
            "Test set: Average loss: 580.4485, Accuracy: 5656/10000 (57%)\n",
            "\n",
            "Train Epoch: 7 [3124/50000 (100%)]\tLoss: 222.678165\n",
            "\n",
            "Test set: Average loss: 674.3165, Accuracy: 5677/10000 (57%)\n",
            "\n",
            "Train Epoch: 8 [3124/50000 (100%)]\tLoss: 126.785095\n",
            "\n",
            "Test set: Average loss: 786.9297, Accuracy: 5640/10000 (56%)\n",
            "\n",
            "Train Epoch: 9 [3124/50000 (100%)]\tLoss: 99.793630\n",
            "\n",
            "Test set: Average loss: 895.6403, Accuracy: 5589/10000 (56%)\n",
            "\n",
            "Train Epoch: 10 [3124/50000 (100%)]\tLoss: 57.716022\n",
            "\n",
            "Test set: Average loss: 1005.9692, Accuracy: 5534/10000 (55%)\n",
            "\n",
            "Train Epoch: 11 [3124/50000 (100%)]\tLoss: 49.476946\n",
            "\n",
            "Test set: Average loss: 1076.9698, Accuracy: 5510/10000 (55%)\n",
            "\n",
            "Train Epoch: 12 [3124/50000 (100%)]\tLoss: 61.353855\n",
            "\n",
            "Test set: Average loss: 1060.3994, Accuracy: 5517/10000 (55%)\n",
            "\n",
            "Train Epoch: 13 [3124/50000 (100%)]\tLoss: 37.549803\n",
            "\n",
            "Test set: Average loss: 1103.7290, Accuracy: 5623/10000 (56%)\n",
            "\n",
            "Train Epoch: 14 [3124/50000 (100%)]\tLoss: 24.083207\n",
            "\n",
            "Test set: Average loss: 1141.2595, Accuracy: 5633/10000 (56%)\n",
            "\n",
            "Train Epoch: 15 [3124/50000 (100%)]\tLoss: 17.375112\n",
            "\n",
            "Test set: Average loss: 1218.4344, Accuracy: 5627/10000 (56%)\n",
            "\n",
            "Train Epoch: 16 [3124/50000 (100%)]\tLoss: 8.148081\n",
            "\n",
            "Test set: Average loss: 1211.2661, Accuracy: 5713/10000 (57%)\n",
            "\n",
            "Train Epoch: 17 [3124/50000 (100%)]\tLoss: 1.080944\n",
            "\n",
            "Test set: Average loss: 1259.6196, Accuracy: 5754/10000 (58%)\n",
            "\n",
            "Train Epoch: 18 [3124/50000 (100%)]\tLoss: 0.196489\n",
            "\n",
            "Test set: Average loss: 1284.8608, Accuracy: 5746/10000 (57%)\n",
            "\n",
            "Train Epoch: 19 [3124/50000 (100%)]\tLoss: 0.121660\n",
            "\n",
            "Test set: Average loss: 1305.1229, Accuracy: 5752/10000 (58%)\n",
            "\n",
            "Train Epoch: 20 [3124/50000 (100%)]\tLoss: 0.098201\n",
            "\n",
            "Test set: Average loss: 1320.3639, Accuracy: 5748/10000 (57%)\n",
            "\n",
            "Train Epoch: 21 [3124/50000 (100%)]\tLoss: 0.083412\n",
            "\n",
            "Test set: Average loss: 1333.3547, Accuracy: 5746/10000 (57%)\n",
            "\n",
            "Train Epoch: 22 [3124/50000 (100%)]\tLoss: 0.072818\n",
            "\n",
            "Test set: Average loss: 1344.2234, Accuracy: 5748/10000 (57%)\n",
            "\n",
            "Train Epoch: 23 [3124/50000 (100%)]\tLoss: 0.064955\n",
            "\n",
            "Test set: Average loss: 1354.3319, Accuracy: 5751/10000 (58%)\n",
            "\n",
            "Train Epoch: 24 [3124/50000 (100%)]\tLoss: 0.058550\n",
            "\n",
            "Test set: Average loss: 1363.0981, Accuracy: 5755/10000 (58%)\n",
            "\n",
            "Train Epoch: 25 [3124/50000 (100%)]\tLoss: 0.053456\n",
            "\n",
            "Test set: Average loss: 1371.7205, Accuracy: 5756/10000 (58%)\n",
            "\n",
            "Train Epoch: 26 [3124/50000 (100%)]\tLoss: 0.049133\n",
            "\n",
            "Test set: Average loss: 1379.2582, Accuracy: 5758/10000 (58%)\n",
            "\n",
            "Train Epoch: 27 [3124/50000 (100%)]\tLoss: 0.045527\n",
            "\n",
            "Test set: Average loss: 1386.4744, Accuracy: 5755/10000 (58%)\n",
            "\n",
            "Train Epoch: 28 [3124/50000 (100%)]\tLoss: 0.042422\n",
            "\n",
            "Test set: Average loss: 1393.2200, Accuracy: 5753/10000 (58%)\n",
            "\n",
            "Train Epoch: 29 [3124/50000 (100%)]\tLoss: 0.039724\n",
            "\n",
            "Test set: Average loss: 1399.4094, Accuracy: 5750/10000 (58%)\n",
            "\n",
            "Train Epoch: 30 [3124/50000 (100%)]\tLoss: 0.037345\n",
            "\n",
            "Test set: Average loss: 1405.3617, Accuracy: 5750/10000 (58%)\n",
            "\n",
            "Train Epoch: 31 [3124/50000 (100%)]\tLoss: 0.035254\n",
            "\n",
            "Test set: Average loss: 1410.9500, Accuracy: 5750/10000 (58%)\n",
            "\n",
            "Train Epoch: 32 [3124/50000 (100%)]\tLoss: 0.033358\n",
            "\n",
            "Test set: Average loss: 1416.3368, Accuracy: 5752/10000 (58%)\n",
            "\n",
            "Train Epoch: 33 [3124/50000 (100%)]\tLoss: 0.031676\n",
            "\n",
            "Test set: Average loss: 1421.5201, Accuracy: 5752/10000 (58%)\n",
            "\n",
            "Train Epoch: 34 [3124/50000 (100%)]\tLoss: 0.030161\n",
            "\n",
            "Test set: Average loss: 1426.2343, Accuracy: 5754/10000 (58%)\n",
            "\n",
            "Train Epoch: 35 [3124/50000 (100%)]\tLoss: 0.028783\n",
            "\n",
            "Test set: Average loss: 1430.9180, Accuracy: 5754/10000 (58%)\n",
            "\n",
            "Train Epoch: 36 [3124/50000 (100%)]\tLoss: 0.027526\n",
            "\n",
            "Test set: Average loss: 1435.3882, Accuracy: 5754/10000 (58%)\n",
            "\n",
            "Train Epoch: 37 [3124/50000 (100%)]\tLoss: 0.026352\n",
            "\n",
            "Test set: Average loss: 1439.6975, Accuracy: 5754/10000 (58%)\n",
            "\n",
            "Train Epoch: 38 [3124/50000 (100%)]\tLoss: 0.025289\n",
            "\n",
            "Test set: Average loss: 1443.8207, Accuracy: 5751/10000 (58%)\n",
            "\n",
            "Train Epoch: 39 [3124/50000 (100%)]\tLoss: 0.024307\n",
            "\n",
            "Test set: Average loss: 1447.8303, Accuracy: 5752/10000 (58%)\n",
            "\n",
            "Train Epoch: 40 [3124/50000 (100%)]\tLoss: 0.023401\n",
            "\n",
            "Test set: Average loss: 1451.6726, Accuracy: 5750/10000 (58%)\n",
            "\n",
            "Train Epoch: 41 [3124/50000 (100%)]\tLoss: 0.022569\n",
            "\n",
            "Test set: Average loss: 1455.4160, Accuracy: 5749/10000 (57%)\n",
            "\n",
            "Train Epoch: 42 [3124/50000 (100%)]\tLoss: 0.021768\n",
            "\n",
            "Test set: Average loss: 1459.0555, Accuracy: 5748/10000 (57%)\n",
            "\n",
            "Train Epoch: 43 [3124/50000 (100%)]\tLoss: 0.021032\n",
            "\n",
            "Test set: Average loss: 1462.5648, Accuracy: 5749/10000 (57%)\n",
            "\n",
            "Train Epoch: 44 [3124/50000 (100%)]\tLoss: 0.020362\n",
            "\n",
            "Test set: Average loss: 1465.9366, Accuracy: 5746/10000 (57%)\n",
            "\n",
            "Train Epoch: 45 [3124/50000 (100%)]\tLoss: 0.019699\n",
            "\n",
            "Test set: Average loss: 1469.2085, Accuracy: 5745/10000 (57%)\n",
            "\n",
            "Train Epoch: 46 [3124/50000 (100%)]\tLoss: 0.019094\n",
            "\n",
            "Test set: Average loss: 1472.4314, Accuracy: 5744/10000 (57%)\n",
            "\n",
            "Train Epoch: 47 [3124/50000 (100%)]\tLoss: 0.018530\n",
            "\n",
            "Test set: Average loss: 1475.5781, Accuracy: 5745/10000 (57%)\n",
            "\n",
            "Train Epoch: 48 [3124/50000 (100%)]\tLoss: 0.017988\n",
            "\n",
            "Test set: Average loss: 1478.6460, Accuracy: 5745/10000 (57%)\n",
            "\n",
            "Train Epoch: 49 [3124/50000 (100%)]\tLoss: 0.017478\n",
            "\n",
            "Test set: Average loss: 1481.5747, Accuracy: 5746/10000 (57%)\n",
            "\n",
            "Train Epoch: 50 [3124/50000 (100%)]\tLoss: 0.016996\n",
            "\n",
            "Test set: Average loss: 1484.4729, Accuracy: 5746/10000 (57%)\n",
            "\n",
            "Train Epoch: 51 [3124/50000 (100%)]\tLoss: 0.016544\n",
            "\n",
            "Test set: Average loss: 1487.3301, Accuracy: 5747/10000 (57%)\n",
            "\n",
            "Train Epoch: 52 [3124/50000 (100%)]\tLoss: 0.016110\n",
            "\n",
            "Test set: Average loss: 1490.0918, Accuracy: 5746/10000 (57%)\n",
            "\n",
            "Train Epoch: 53 [3124/50000 (100%)]\tLoss: 0.015696\n",
            "\n",
            "Test set: Average loss: 1492.7816, Accuracy: 5744/10000 (57%)\n",
            "\n",
            "Train Epoch: 54 [3124/50000 (100%)]\tLoss: 0.015314\n",
            "\n",
            "Test set: Average loss: 1495.4387, Accuracy: 5745/10000 (57%)\n",
            "\n",
            "Train Epoch: 55 [3124/50000 (100%)]\tLoss: 0.014937\n",
            "\n",
            "Test set: Average loss: 1498.0117, Accuracy: 5745/10000 (57%)\n",
            "\n",
            "Train Epoch: 56 [3124/50000 (100%)]\tLoss: 0.014580\n",
            "\n",
            "Test set: Average loss: 1500.5533, Accuracy: 5747/10000 (57%)\n",
            "\n",
            "Train Epoch: 57 [3124/50000 (100%)]\tLoss: 0.014248\n",
            "\n",
            "Test set: Average loss: 1503.0194, Accuracy: 5747/10000 (57%)\n",
            "\n",
            "Train Epoch: 58 [3124/50000 (100%)]\tLoss: 0.013912\n",
            "\n",
            "Test set: Average loss: 1505.4551, Accuracy: 5747/10000 (57%)\n",
            "\n",
            "Train Epoch: 59 [3124/50000 (100%)]\tLoss: 0.013606\n",
            "\n",
            "Test set: Average loss: 1507.8124, Accuracy: 5746/10000 (57%)\n",
            "\n",
            "Train Epoch: 60 [3124/50000 (100%)]\tLoss: 0.013306\n",
            "\n",
            "Test set: Average loss: 1510.1305, Accuracy: 5744/10000 (57%)\n",
            "\n",
            "Train Epoch: 61 [3124/50000 (100%)]\tLoss: 0.013020\n",
            "\n",
            "Test set: Average loss: 1512.4211, Accuracy: 5744/10000 (57%)\n",
            "\n",
            "Train Epoch: 62 [3124/50000 (100%)]\tLoss: 0.012749\n",
            "\n",
            "Test set: Average loss: 1514.6674, Accuracy: 5744/10000 (57%)\n",
            "\n",
            "Train Epoch: 63 [3124/50000 (100%)]\tLoss: 0.012486\n",
            "\n",
            "Test set: Average loss: 1516.8551, Accuracy: 5743/10000 (57%)\n",
            "\n",
            "Train Epoch: 64 [3124/50000 (100%)]\tLoss: 0.012235\n",
            "\n",
            "Test set: Average loss: 1519.0294, Accuracy: 5743/10000 (57%)\n",
            "\n",
            "Train Epoch: 65 [3124/50000 (100%)]\tLoss: 0.011993\n",
            "\n",
            "Test set: Average loss: 1521.1437, Accuracy: 5743/10000 (57%)\n",
            "\n",
            "Train Epoch: 66 [3124/50000 (100%)]\tLoss: 0.011758\n",
            "\n",
            "Test set: Average loss: 1523.2394, Accuracy: 5744/10000 (57%)\n",
            "\n",
            "Train Epoch: 67 [3124/50000 (100%)]\tLoss: 0.011537\n",
            "\n",
            "Test set: Average loss: 1525.3013, Accuracy: 5744/10000 (57%)\n",
            "\n",
            "Train Epoch: 68 [3124/50000 (100%)]\tLoss: 0.011322\n",
            "\n",
            "Test set: Average loss: 1527.3143, Accuracy: 5745/10000 (57%)\n",
            "\n",
            "Train Epoch: 69 [3124/50000 (100%)]\tLoss: 0.011106\n",
            "\n",
            "Test set: Average loss: 1529.3082, Accuracy: 5745/10000 (57%)\n",
            "\n",
            "Train Epoch: 70 [3124/50000 (100%)]\tLoss: 0.010910\n",
            "\n",
            "Test set: Average loss: 1531.2512, Accuracy: 5744/10000 (57%)\n",
            "\n",
            "Train Epoch: 71 [3124/50000 (100%)]\tLoss: 0.010710\n",
            "\n",
            "Test set: Average loss: 1533.1628, Accuracy: 5744/10000 (57%)\n",
            "\n",
            "Train Epoch: 72 [3124/50000 (100%)]\tLoss: 0.010524\n",
            "\n",
            "Test set: Average loss: 1535.0585, Accuracy: 5744/10000 (57%)\n",
            "\n",
            "Train Epoch: 73 [3124/50000 (100%)]\tLoss: 0.010363\n",
            "\n",
            "Test set: Average loss: 1536.9080, Accuracy: 5744/10000 (57%)\n",
            "\n",
            "Train Epoch: 74 [3124/50000 (100%)]\tLoss: 0.010172\n",
            "\n",
            "Test set: Average loss: 1538.7513, Accuracy: 5743/10000 (57%)\n",
            "\n",
            "Train Epoch: 75 [3124/50000 (100%)]\tLoss: 0.009999\n",
            "\n",
            "Test set: Average loss: 1540.5629, Accuracy: 5742/10000 (57%)\n",
            "\n",
            "Train Epoch: 76 [3124/50000 (100%)]\tLoss: 0.009832\n",
            "\n",
            "Test set: Average loss: 1542.3394, Accuracy: 5742/10000 (57%)\n",
            "\n",
            "Train Epoch: 77 [3124/50000 (100%)]\tLoss: 0.009670\n",
            "\n",
            "Test set: Average loss: 1544.0919, Accuracy: 5742/10000 (57%)\n",
            "\n",
            "Train Epoch: 78 [3124/50000 (100%)]\tLoss: 0.009514\n",
            "\n",
            "Test set: Average loss: 1545.8287, Accuracy: 5742/10000 (57%)\n",
            "\n",
            "Train Epoch: 79 [3124/50000 (100%)]\tLoss: 0.009370\n",
            "\n",
            "Test set: Average loss: 1547.5454, Accuracy: 5740/10000 (57%)\n",
            "\n",
            "Train Epoch: 80 [3124/50000 (100%)]\tLoss: 0.009221\n",
            "\n",
            "Test set: Average loss: 1549.2288, Accuracy: 5742/10000 (57%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FRNPF(II)"
      ],
      "metadata": {
        "id": "mf8eB2zVQqZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Conv4Galu()\n",
        "model.to(device)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8jVcgLOQp1L",
        "outputId": "a3748500-09f1-4691-b3ab-c98400a92d56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv4Galu(\n",
              "  (N1_C1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N1_C2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N1_C3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N1_C4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N1_F1): Flatten(start_dim=1, end_dim=-1)\n",
              "  (N1_D1): Linear(in_features=131072, out_features=256, bias=True)\n",
              "  (N1_D2): Linear(in_features=256, out_features=256, bias=True)\n",
              "  (N2_C1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N2_C2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N2_C3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N2_C4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N2_F1): Flatten(start_dim=1, end_dim=-1)\n",
              "  (N2_D1): Linear(in_features=131072, out_features=256, bias=True)\n",
              "  (N2_D2): Linear(in_features=256, out_features=256, bias=True)\n",
              "  (outputs): Linear(in_features=256, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if name[0:2]=='N1':\n",
        "        param.requires_grad = False"
      ],
      "metadata": {
        "id": "iySAVoWb5P7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "      print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eN-Pn6yRrFa",
        "outputId": "2284fcb7-1519-47a5-be74-e5ab4062dc4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N2_C1.weight\n",
            "N2_C1.bias\n",
            "N2_C2.weight\n",
            "N2_C2.bias\n",
            "N2_C3.weight\n",
            "N2_C3.bias\n",
            "N2_C4.weight\n",
            "N2_C4.bias\n",
            "N2_D1.weight\n",
            "N2_D1.bias\n",
            "N2_D2.weight\n",
            "N2_D2.bias\n",
            "outputs.weight\n",
            "outputs.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "for epoch in range(1, 81):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCsn8ohzRvHa",
        "outputId": "6e49217a-d1f2-4883-a9fa-5f59dacb0924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [3124/50000 (100%)]\tLoss: 2978.537833\n",
            "\n",
            "Test set: Average loss: 476.8409, Accuracy: 4492/10000 (45%)\n",
            "\n",
            "Train Epoch: 2 [3124/50000 (100%)]\tLoss: 2162.187945\n",
            "\n",
            "Test set: Average loss: 410.8533, Accuracy: 5284/10000 (53%)\n",
            "\n",
            "Train Epoch: 3 [3124/50000 (100%)]\tLoss: 1805.238190\n",
            "\n",
            "Test set: Average loss: 409.1857, Accuracy: 5475/10000 (55%)\n",
            "\n",
            "Train Epoch: 4 [3124/50000 (100%)]\tLoss: 1448.303231\n",
            "\n",
            "Test set: Average loss: 402.5682, Accuracy: 5575/10000 (56%)\n",
            "\n",
            "Train Epoch: 5 [3124/50000 (100%)]\tLoss: 989.863473\n",
            "\n",
            "Test set: Average loss: 481.5861, Accuracy: 5334/10000 (53%)\n",
            "\n",
            "Train Epoch: 6 [3124/50000 (100%)]\tLoss: 513.677821\n",
            "\n",
            "Test set: Average loss: 560.6436, Accuracy: 5451/10000 (55%)\n",
            "\n",
            "Train Epoch: 7 [3124/50000 (100%)]\tLoss: 248.603340\n",
            "\n",
            "Test set: Average loss: 723.5992, Accuracy: 5486/10000 (55%)\n",
            "\n",
            "Train Epoch: 8 [3124/50000 (100%)]\tLoss: 143.641600\n",
            "\n",
            "Test set: Average loss: 830.1932, Accuracy: 5487/10000 (55%)\n",
            "\n",
            "Train Epoch: 9 [3124/50000 (100%)]\tLoss: 91.248585\n",
            "\n",
            "Test set: Average loss: 896.2172, Accuracy: 5462/10000 (55%)\n",
            "\n",
            "Train Epoch: 10 [3124/50000 (100%)]\tLoss: 63.912333\n",
            "\n",
            "Test set: Average loss: 970.7737, Accuracy: 5504/10000 (55%)\n",
            "\n",
            "Train Epoch: 11 [3124/50000 (100%)]\tLoss: 53.023184\n",
            "\n",
            "Test set: Average loss: 1034.0576, Accuracy: 5461/10000 (55%)\n",
            "\n",
            "Train Epoch: 12 [3124/50000 (100%)]\tLoss: 39.811897\n",
            "\n",
            "Test set: Average loss: 1029.2288, Accuracy: 5522/10000 (55%)\n",
            "\n",
            "Train Epoch: 13 [3124/50000 (100%)]\tLoss: 23.899132\n",
            "\n",
            "Test set: Average loss: 1162.6536, Accuracy: 5519/10000 (55%)\n",
            "\n",
            "Train Epoch: 14 [3124/50000 (100%)]\tLoss: 12.836648\n",
            "\n",
            "Test set: Average loss: 1233.2880, Accuracy: 5546/10000 (55%)\n",
            "\n",
            "Train Epoch: 15 [3124/50000 (100%)]\tLoss: 5.740059\n",
            "\n",
            "Test set: Average loss: 1293.2372, Accuracy: 5597/10000 (56%)\n",
            "\n",
            "Train Epoch: 16 [3124/50000 (100%)]\tLoss: 7.860824\n",
            "\n",
            "Test set: Average loss: 1309.5632, Accuracy: 5586/10000 (56%)\n",
            "\n",
            "Train Epoch: 17 [3124/50000 (100%)]\tLoss: 32.153062\n",
            "\n",
            "Test set: Average loss: 1278.0909, Accuracy: 5429/10000 (54%)\n",
            "\n",
            "Train Epoch: 18 [3124/50000 (100%)]\tLoss: 44.388726\n",
            "\n",
            "Test set: Average loss: 1238.2214, Accuracy: 5486/10000 (55%)\n",
            "\n",
            "Train Epoch: 19 [3124/50000 (100%)]\tLoss: 29.376692\n",
            "\n",
            "Test set: Average loss: 1218.6052, Accuracy: 5555/10000 (56%)\n",
            "\n",
            "Train Epoch: 20 [3124/50000 (100%)]\tLoss: 15.620575\n",
            "\n",
            "Test set: Average loss: 1229.4556, Accuracy: 5575/10000 (56%)\n",
            "\n",
            "Train Epoch: 21 [3124/50000 (100%)]\tLoss: 4.730139\n",
            "\n",
            "Test set: Average loss: 1285.8932, Accuracy: 5594/10000 (56%)\n",
            "\n",
            "Train Epoch: 22 [3124/50000 (100%)]\tLoss: 0.830969\n",
            "\n",
            "Test set: Average loss: 1342.8068, Accuracy: 5609/10000 (56%)\n",
            "\n",
            "Train Epoch: 23 [3124/50000 (100%)]\tLoss: 0.183423\n",
            "\n",
            "Test set: Average loss: 1363.2118, Accuracy: 5608/10000 (56%)\n",
            "\n",
            "Train Epoch: 24 [3124/50000 (100%)]\tLoss: 0.113952\n",
            "\n",
            "Test set: Average loss: 1381.0254, Accuracy: 5611/10000 (56%)\n",
            "\n",
            "Train Epoch: 25 [3124/50000 (100%)]\tLoss: 0.091988\n",
            "\n",
            "Test set: Average loss: 1395.5499, Accuracy: 5612/10000 (56%)\n",
            "\n",
            "Train Epoch: 26 [3124/50000 (100%)]\tLoss: 0.078257\n",
            "\n",
            "Test set: Average loss: 1408.0049, Accuracy: 5618/10000 (56%)\n",
            "\n",
            "Train Epoch: 27 [3124/50000 (100%)]\tLoss: 0.068423\n",
            "\n",
            "Test set: Average loss: 1418.9865, Accuracy: 5621/10000 (56%)\n",
            "\n",
            "Train Epoch: 28 [3124/50000 (100%)]\tLoss: 0.060958\n",
            "\n",
            "Test set: Average loss: 1428.9594, Accuracy: 5620/10000 (56%)\n",
            "\n",
            "Train Epoch: 29 [3124/50000 (100%)]\tLoss: 0.055084\n",
            "\n",
            "Test set: Average loss: 1437.7832, Accuracy: 5622/10000 (56%)\n",
            "\n",
            "Train Epoch: 30 [3124/50000 (100%)]\tLoss: 0.050273\n",
            "\n",
            "Test set: Average loss: 1446.0521, Accuracy: 5620/10000 (56%)\n",
            "\n",
            "Train Epoch: 31 [3124/50000 (100%)]\tLoss: 0.046278\n",
            "\n",
            "Test set: Average loss: 1453.6833, Accuracy: 5614/10000 (56%)\n",
            "\n",
            "Train Epoch: 32 [3124/50000 (100%)]\tLoss: 0.042889\n",
            "\n",
            "Test set: Average loss: 1460.7838, Accuracy: 5617/10000 (56%)\n",
            "\n",
            "Train Epoch: 33 [3124/50000 (100%)]\tLoss: 0.039978\n",
            "\n",
            "Test set: Average loss: 1467.4006, Accuracy: 5621/10000 (56%)\n",
            "\n",
            "Train Epoch: 34 [3124/50000 (100%)]\tLoss: 0.037440\n",
            "\n",
            "Test set: Average loss: 1473.7924, Accuracy: 5620/10000 (56%)\n",
            "\n",
            "Train Epoch: 35 [3124/50000 (100%)]\tLoss: 0.035230\n",
            "\n",
            "Test set: Average loss: 1479.7894, Accuracy: 5618/10000 (56%)\n",
            "\n",
            "Train Epoch: 36 [3124/50000 (100%)]\tLoss: 0.033254\n",
            "\n",
            "Test set: Average loss: 1485.4138, Accuracy: 5619/10000 (56%)\n",
            "\n",
            "Train Epoch: 37 [3124/50000 (100%)]\tLoss: 0.031481\n",
            "\n",
            "Test set: Average loss: 1490.7887, Accuracy: 5619/10000 (56%)\n",
            "\n",
            "Train Epoch: 38 [3124/50000 (100%)]\tLoss: 0.029901\n",
            "\n",
            "Test set: Average loss: 1495.9669, Accuracy: 5620/10000 (56%)\n",
            "\n",
            "Train Epoch: 39 [3124/50000 (100%)]\tLoss: 0.028473\n",
            "\n",
            "Test set: Average loss: 1500.9266, Accuracy: 5621/10000 (56%)\n",
            "\n",
            "Train Epoch: 40 [3124/50000 (100%)]\tLoss: 0.027168\n",
            "\n",
            "Test set: Average loss: 1505.6429, Accuracy: 5623/10000 (56%)\n",
            "\n",
            "Train Epoch: 41 [3124/50000 (100%)]\tLoss: 0.025983\n",
            "\n",
            "Test set: Average loss: 1510.1595, Accuracy: 5623/10000 (56%)\n",
            "\n",
            "Train Epoch: 42 [3124/50000 (100%)]\tLoss: 0.024918\n",
            "\n",
            "Test set: Average loss: 1514.5592, Accuracy: 5621/10000 (56%)\n",
            "\n",
            "Train Epoch: 43 [3124/50000 (100%)]\tLoss: 0.023899\n",
            "\n",
            "Test set: Average loss: 1518.8101, Accuracy: 5621/10000 (56%)\n",
            "\n",
            "Train Epoch: 44 [3124/50000 (100%)]\tLoss: 0.022967\n",
            "\n",
            "Test set: Average loss: 1522.8541, Accuracy: 5621/10000 (56%)\n",
            "\n",
            "Train Epoch: 45 [3124/50000 (100%)]\tLoss: 0.022123\n",
            "\n",
            "Test set: Average loss: 1526.8033, Accuracy: 5624/10000 (56%)\n",
            "\n",
            "Train Epoch: 46 [3124/50000 (100%)]\tLoss: 0.021324\n",
            "\n",
            "Test set: Average loss: 1530.6012, Accuracy: 5625/10000 (56%)\n",
            "\n",
            "Train Epoch: 47 [3124/50000 (100%)]\tLoss: 0.020577\n",
            "\n",
            "Test set: Average loss: 1534.2913, Accuracy: 5623/10000 (56%)\n",
            "\n",
            "Train Epoch: 48 [3124/50000 (100%)]\tLoss: 0.019896\n",
            "\n",
            "Test set: Average loss: 1537.8811, Accuracy: 5623/10000 (56%)\n",
            "\n",
            "Train Epoch: 49 [3124/50000 (100%)]\tLoss: 0.019239\n",
            "\n",
            "Test set: Average loss: 1541.3438, Accuracy: 5624/10000 (56%)\n",
            "\n",
            "Train Epoch: 50 [3124/50000 (100%)]\tLoss: 0.018632\n",
            "\n",
            "Test set: Average loss: 1544.7281, Accuracy: 5624/10000 (56%)\n",
            "\n",
            "Train Epoch: 51 [3124/50000 (100%)]\tLoss: 0.018060\n",
            "\n",
            "Test set: Average loss: 1548.0280, Accuracy: 5624/10000 (56%)\n",
            "\n",
            "Train Epoch: 52 [3124/50000 (100%)]\tLoss: 0.017524\n",
            "\n",
            "Test set: Average loss: 1551.2388, Accuracy: 5624/10000 (56%)\n",
            "\n",
            "Train Epoch: 53 [3124/50000 (100%)]\tLoss: 0.017026\n",
            "\n",
            "Test set: Average loss: 1554.3561, Accuracy: 5624/10000 (56%)\n",
            "\n",
            "Train Epoch: 54 [3124/50000 (100%)]\tLoss: 0.016541\n",
            "\n",
            "Test set: Average loss: 1557.4097, Accuracy: 5623/10000 (56%)\n",
            "\n",
            "Train Epoch: 55 [3124/50000 (100%)]\tLoss: 0.016085\n",
            "\n",
            "Test set: Average loss: 1560.3978, Accuracy: 5625/10000 (56%)\n",
            "\n",
            "Train Epoch: 56 [3124/50000 (100%)]\tLoss: 0.015660\n",
            "\n",
            "Test set: Average loss: 1563.2955, Accuracy: 5625/10000 (56%)\n",
            "\n",
            "Train Epoch: 57 [3124/50000 (100%)]\tLoss: 0.015253\n",
            "\n",
            "Test set: Average loss: 1566.1089, Accuracy: 5624/10000 (56%)\n",
            "\n",
            "Train Epoch: 58 [3124/50000 (100%)]\tLoss: 0.014870\n",
            "\n",
            "Test set: Average loss: 1568.8875, Accuracy: 5624/10000 (56%)\n",
            "\n",
            "Train Epoch: 59 [3124/50000 (100%)]\tLoss: 0.014490\n",
            "\n",
            "Test set: Average loss: 1571.5919, Accuracy: 5624/10000 (56%)\n",
            "\n",
            "Train Epoch: 60 [3124/50000 (100%)]\tLoss: 0.014139\n",
            "\n",
            "Test set: Average loss: 1574.2565, Accuracy: 5624/10000 (56%)\n",
            "\n",
            "Train Epoch: 61 [3124/50000 (100%)]\tLoss: 0.013806\n",
            "\n",
            "Test set: Average loss: 1576.8464, Accuracy: 5625/10000 (56%)\n",
            "\n",
            "Train Epoch: 62 [3124/50000 (100%)]\tLoss: 0.013487\n",
            "\n",
            "Test set: Average loss: 1579.3809, Accuracy: 5625/10000 (56%)\n",
            "\n",
            "Train Epoch: 63 [3124/50000 (100%)]\tLoss: 0.013185\n",
            "\n",
            "Test set: Average loss: 1581.8630, Accuracy: 5626/10000 (56%)\n",
            "\n",
            "Train Epoch: 64 [3124/50000 (100%)]\tLoss: 0.012886\n",
            "\n",
            "Test set: Average loss: 1584.3113, Accuracy: 5628/10000 (56%)\n",
            "\n",
            "Train Epoch: 65 [3124/50000 (100%)]\tLoss: 0.012606\n",
            "\n",
            "Test set: Average loss: 1586.7083, Accuracy: 5628/10000 (56%)\n",
            "\n",
            "Train Epoch: 66 [3124/50000 (100%)]\tLoss: 0.012338\n",
            "\n",
            "Test set: Average loss: 1589.0433, Accuracy: 5628/10000 (56%)\n",
            "\n",
            "Train Epoch: 67 [3124/50000 (100%)]\tLoss: 0.012087\n",
            "\n",
            "Test set: Average loss: 1591.3441, Accuracy: 5627/10000 (56%)\n",
            "\n",
            "Train Epoch: 68 [3124/50000 (100%)]\tLoss: 0.011837\n",
            "\n",
            "Test set: Average loss: 1593.6019, Accuracy: 5627/10000 (56%)\n",
            "\n",
            "Train Epoch: 69 [3124/50000 (100%)]\tLoss: 0.011598\n",
            "\n",
            "Test set: Average loss: 1595.8263, Accuracy: 5625/10000 (56%)\n",
            "\n",
            "Train Epoch: 70 [3124/50000 (100%)]\tLoss: 0.011366\n",
            "\n",
            "Test set: Average loss: 1598.0234, Accuracy: 5624/10000 (56%)\n",
            "\n",
            "Train Epoch: 71 [3124/50000 (100%)]\tLoss: 0.011150\n",
            "\n",
            "Test set: Average loss: 1600.1737, Accuracy: 5624/10000 (56%)\n",
            "\n",
            "Train Epoch: 72 [3124/50000 (100%)]\tLoss: 0.010934\n",
            "\n",
            "Test set: Average loss: 1602.2802, Accuracy: 5622/10000 (56%)\n",
            "\n",
            "Train Epoch: 73 [3124/50000 (100%)]\tLoss: 0.010729\n",
            "\n",
            "Test set: Average loss: 1604.3563, Accuracy: 5624/10000 (56%)\n",
            "\n",
            "Train Epoch: 74 [3124/50000 (100%)]\tLoss: 0.010535\n",
            "\n",
            "Test set: Average loss: 1606.3983, Accuracy: 5626/10000 (56%)\n",
            "\n",
            "Train Epoch: 75 [3124/50000 (100%)]\tLoss: 0.010345\n",
            "\n",
            "Test set: Average loss: 1608.4010, Accuracy: 5626/10000 (56%)\n",
            "\n",
            "Train Epoch: 76 [3124/50000 (100%)]\tLoss: 0.010159\n",
            "\n",
            "Test set: Average loss: 1610.3734, Accuracy: 5625/10000 (56%)\n",
            "\n",
            "Train Epoch: 77 [3124/50000 (100%)]\tLoss: 0.009983\n",
            "\n",
            "Test set: Average loss: 1612.3177, Accuracy: 5626/10000 (56%)\n",
            "\n",
            "Train Epoch: 78 [3124/50000 (100%)]\tLoss: 0.009813\n",
            "\n",
            "Test set: Average loss: 1614.2300, Accuracy: 5627/10000 (56%)\n",
            "\n",
            "Train Epoch: 79 [3124/50000 (100%)]\tLoss: 0.009645\n",
            "\n",
            "Test set: Average loss: 1616.1163, Accuracy: 5627/10000 (56%)\n",
            "\n",
            "Train Epoch: 80 [3124/50000 (100%)]\tLoss: 0.009483\n",
            "\n",
            "Test set: Average loss: 1617.9766, Accuracy: 5625/10000 (56%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FRNPF(DI)"
      ],
      "metadata": {
        "id": "Aauyr0myRv0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Conv4Galu()\n",
        "model.to(device)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lc8_R6vmRxHD",
        "outputId": "c53dc5c6-645c-4e71-c164-f0a489f4106b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv4Galu(\n",
              "  (N1_C1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N1_C2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N1_C3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N1_C4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N1_F1): Flatten(start_dim=1, end_dim=-1)\n",
              "  (N1_D1): Linear(in_features=131072, out_features=256, bias=True)\n",
              "  (N1_D2): Linear(in_features=256, out_features=256, bias=True)\n",
              "  (N2_C1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N2_C2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N2_C3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N2_C4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N2_F1): Flatten(start_dim=1, end_dim=-1)\n",
              "  (N2_D1): Linear(in_features=131072, out_features=256, bias=True)\n",
              "  (N2_D2): Linear(in_features=256, out_features=256, bias=True)\n",
              "  (outputs): Linear(in_features=256, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "  print(name, param)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAz4l7skSRqg",
        "outputId": "023a86d7-9b26-4daa-8551-96bd2675f17c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N1_C1.weight Parameter containing:\n",
            "tensor([[[[-1.5751e-02,  1.8377e-01,  1.5067e-01],\n",
            "          [-5.2458e-02, -1.8347e-01,  1.5553e-01],\n",
            "          [ 8.5427e-02,  8.4333e-03, -1.1791e-01]],\n",
            "\n",
            "         [[ 9.3870e-02,  1.5532e-01,  1.0040e-01],\n",
            "          [-5.4865e-02,  7.3119e-02, -1.5564e-01],\n",
            "          [-8.2653e-02, -1.4993e-01, -1.4855e-01]],\n",
            "\n",
            "         [[-1.7436e-01, -3.4497e-02, -2.8740e-02],\n",
            "          [-1.0539e-01,  5.4372e-02, -9.3557e-02],\n",
            "          [-2.3659e-02,  1.7833e-01, -5.9122e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 5.8111e-02,  1.2392e-01, -1.5378e-01],\n",
            "          [-9.8643e-03,  1.6460e-01,  4.6094e-02],\n",
            "          [-4.1165e-02, -7.9933e-02,  1.2361e-01]],\n",
            "\n",
            "         [[-9.4299e-02, -5.6796e-02, -9.6843e-02],\n",
            "          [ 1.5443e-01,  1.0551e-01, -1.2970e-01],\n",
            "          [-6.8913e-02, -1.8086e-01,  3.8319e-02]],\n",
            "\n",
            "         [[ 2.3538e-02,  1.7668e-01,  1.6701e-01],\n",
            "          [-8.1817e-02, -1.8101e-02, -1.2344e-01],\n",
            "          [ 3.7531e-02,  1.3823e-01,  8.9921e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.8629e-01,  1.0086e-02,  1.5948e-02],\n",
            "          [-5.4252e-02,  3.4405e-02, -4.0503e-02],\n",
            "          [-1.1004e-01, -1.7539e-02, -9.1759e-02]],\n",
            "\n",
            "         [[ 5.6471e-02,  5.2879e-02,  4.6940e-02],\n",
            "          [ 1.6432e-01, -1.2782e-01,  8.7223e-02],\n",
            "          [ 1.4965e-01,  2.6719e-02,  1.2975e-01]],\n",
            "\n",
            "         [[ 1.4524e-01, -5.2260e-02,  8.3553e-02],\n",
            "          [-4.5893e-02,  7.5249e-05,  3.7214e-02],\n",
            "          [-1.0916e-01, -9.3558e-02,  1.6216e-01]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-5.0360e-02, -1.0091e-01, -1.3989e-01],\n",
            "          [-2.3886e-02, -1.0310e-01,  1.5216e-01],\n",
            "          [ 1.8302e-01, -1.4883e-01, -3.8753e-02]],\n",
            "\n",
            "         [[ 8.1115e-03,  1.2052e-02,  1.6082e-01],\n",
            "          [ 1.7090e-01,  8.1719e-02,  6.1602e-02],\n",
            "          [-1.1682e-01,  6.0915e-02,  1.6584e-01]],\n",
            "\n",
            "         [[-1.3601e-01,  2.6996e-02,  9.7761e-02],\n",
            "          [-3.4323e-02,  5.6999e-02,  1.6071e-01],\n",
            "          [-1.6614e-01,  2.8728e-03, -1.4690e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.5961e-01,  1.9241e-01, -9.7117e-02],\n",
            "          [ 6.4183e-02,  1.7203e-01,  1.2531e-01],\n",
            "          [ 1.2804e-01,  1.2463e-01,  2.7679e-02]],\n",
            "\n",
            "         [[ 7.4301e-02,  3.0653e-04,  1.7253e-01],\n",
            "          [-1.5410e-01, -1.6842e-01, -1.9046e-01],\n",
            "          [ 5.8921e-02, -4.8850e-03,  1.6892e-01]],\n",
            "\n",
            "         [[-3.2244e-02, -8.9767e-02, -1.8330e-01],\n",
            "          [ 1.6410e-02,  6.0630e-02,  1.0509e-02],\n",
            "          [ 1.8865e-01,  1.4779e-01, -1.5674e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 4.7032e-02,  3.8626e-02,  4.8792e-02],\n",
            "          [ 1.7731e-01, -9.3070e-02, -1.7046e-01],\n",
            "          [-1.5299e-01,  1.4744e-01,  1.3680e-01]],\n",
            "\n",
            "         [[-5.2710e-02,  7.1119e-02,  5.5679e-02],\n",
            "          [-2.7893e-02,  9.0514e-02, -5.5448e-02],\n",
            "          [ 8.0949e-02,  1.1803e-01, -1.2192e-01]],\n",
            "\n",
            "         [[ 1.4326e-02, -6.8382e-03,  7.7612e-03],\n",
            "          [-9.2268e-02, -3.3531e-02,  6.7799e-02],\n",
            "          [-1.7701e-01,  1.4952e-02, -1.7578e-01]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "N1_C1.bias Parameter containing:\n",
            "tensor([ 0.0870, -0.0017, -0.0927,  0.0648,  0.0768,  0.1078, -0.1209,  0.1441,\n",
            "        -0.1149, -0.1024, -0.1861, -0.0038, -0.1548, -0.0150,  0.0055,  0.1721,\n",
            "         0.1875,  0.0874, -0.1132,  0.0408,  0.0288,  0.0738,  0.0299,  0.1701,\n",
            "        -0.0704,  0.1697, -0.0761,  0.1665,  0.0874, -0.0484,  0.0317, -0.1540,\n",
            "         0.0124,  0.0292,  0.0921, -0.0623, -0.0295, -0.1806, -0.1339, -0.1153,\n",
            "        -0.0454,  0.1353,  0.1777,  0.1024,  0.0885,  0.1480,  0.1762, -0.0993,\n",
            "         0.0917,  0.1497, -0.0214,  0.1856,  0.1483, -0.1826,  0.0236, -0.1445,\n",
            "        -0.0613, -0.1846, -0.1515, -0.0051, -0.0599,  0.0066, -0.1160, -0.1048],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "N1_C2.weight Parameter containing:\n",
            "tensor([[[[ 4.1502e-02,  7.1083e-03, -2.4157e-02],\n",
            "          [ 3.3427e-02, -2.7943e-02,  1.7035e-02],\n",
            "          [ 1.7370e-02, -1.1468e-02, -2.5543e-02]],\n",
            "\n",
            "         [[ 2.2976e-02,  1.4994e-02, -3.7699e-02],\n",
            "          [-2.2270e-02,  3.4062e-02, -1.6890e-02],\n",
            "          [ 7.4861e-03,  2.5710e-02,  2.6456e-02]],\n",
            "\n",
            "         [[-3.4497e-02,  4.1087e-02, -2.8228e-03],\n",
            "          [ 2.0597e-02, -7.6403e-03, -3.8109e-02],\n",
            "          [ 3.7459e-02,  4.1619e-02, -1.7453e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.7048e-03, -3.3121e-02,  3.5061e-02],\n",
            "          [-2.8068e-03, -1.9001e-02, -2.9970e-02],\n",
            "          [ 6.5759e-03,  3.6482e-02, -2.4134e-02]],\n",
            "\n",
            "         [[-2.9745e-02, -3.5235e-02, -1.5721e-02],\n",
            "          [-1.2583e-02,  1.4629e-02,  1.7555e-02],\n",
            "          [ 2.4412e-02, -6.3056e-03, -2.3247e-02]],\n",
            "\n",
            "         [[ 1.7494e-02,  3.4728e-02, -3.2107e-02],\n",
            "          [-2.7720e-02, -3.2116e-02, -4.0153e-02],\n",
            "          [-3.4495e-02,  3.0219e-03,  1.3208e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.1404e-02, -3.2572e-02, -1.1163e-02],\n",
            "          [ 2.4361e-03, -2.4812e-03,  2.9446e-02],\n",
            "          [ 1.7109e-02, -4.0020e-02, -6.7605e-04]],\n",
            "\n",
            "         [[-7.3125e-03, -6.5241e-03, -2.4482e-02],\n",
            "          [ 1.2700e-02, -4.0161e-02, -1.9801e-02],\n",
            "          [ 6.1035e-03, -2.5378e-02,  1.4701e-02]],\n",
            "\n",
            "         [[-2.2802e-03,  3.0293e-02,  2.3430e-02],\n",
            "          [-1.8270e-02, -3.8768e-02,  1.1177e-02],\n",
            "          [ 6.6339e-03, -1.1933e-03, -4.1292e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.4542e-02, -2.7308e-02,  4.3648e-03],\n",
            "          [ 1.5020e-02, -3.7714e-02, -3.8469e-02],\n",
            "          [-2.5455e-02, -3.9763e-02,  4.0543e-02]],\n",
            "\n",
            "         [[ 1.5787e-02, -1.2452e-02,  3.8947e-02],\n",
            "          [ 1.2046e-02, -2.1721e-02, -9.6500e-03],\n",
            "          [-6.1636e-03, -4.0110e-02, -3.9815e-03]],\n",
            "\n",
            "         [[-1.0360e-03,  3.9399e-02, -1.1802e-02],\n",
            "          [ 1.3679e-02,  3.4735e-02, -3.8970e-02],\n",
            "          [ 1.3516e-02, -4.1253e-02, -1.3538e-02]]],\n",
            "\n",
            "\n",
            "        [[[-3.8760e-02, -9.6932e-03,  7.5454e-03],\n",
            "          [-1.3119e-02, -3.9605e-02, -1.3451e-02],\n",
            "          [ 3.8865e-02, -1.5800e-02,  2.3358e-02]],\n",
            "\n",
            "         [[ 1.8103e-02, -3.7730e-03, -1.7142e-02],\n",
            "          [-2.4787e-03,  6.8227e-04, -2.2773e-02],\n",
            "          [ 1.1074e-02,  9.7435e-03, -1.9535e-02]],\n",
            "\n",
            "         [[-2.9178e-02,  3.1846e-02,  1.0892e-02],\n",
            "          [-4.0926e-02, -2.1412e-02,  2.0808e-02],\n",
            "          [ 2.3783e-02,  5.9932e-03,  2.8702e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.6980e-02, -1.5117e-02,  2.1090e-02],\n",
            "          [-2.4403e-02,  8.1152e-03,  1.6771e-02],\n",
            "          [-3.4027e-02,  1.7177e-02,  1.7862e-02]],\n",
            "\n",
            "         [[ 3.0411e-03, -2.9233e-02,  2.4907e-02],\n",
            "          [ 1.9163e-02,  1.7407e-02, -3.9323e-02],\n",
            "          [ 1.6006e-02,  2.1578e-02,  9.2053e-03]],\n",
            "\n",
            "         [[-2.1882e-02, -2.9427e-02,  2.8159e-02],\n",
            "          [-2.8639e-02, -3.0917e-02,  2.2623e-02],\n",
            "          [ 3.7783e-02,  6.6833e-03,  7.0215e-03]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-3.8842e-02, -5.7740e-03, -1.8748e-02],\n",
            "          [ 2.4397e-02, -1.6717e-02, -3.4589e-02],\n",
            "          [-2.3527e-02, -3.6133e-02,  3.4325e-02]],\n",
            "\n",
            "         [[-8.4007e-03, -2.0810e-02, -8.0304e-03],\n",
            "          [-3.6747e-02,  3.4110e-02,  1.4661e-02],\n",
            "          [ 2.1937e-02,  3.2526e-02, -4.0912e-03]],\n",
            "\n",
            "         [[-4.0558e-02, -4.3750e-03,  3.2731e-02],\n",
            "          [-1.5066e-02,  3.4459e-02,  1.0287e-02],\n",
            "          [-2.3456e-02,  8.8511e-03, -1.6334e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.5162e-02,  2.4794e-02,  1.0652e-02],\n",
            "          [ 7.2941e-03, -2.6837e-02, -3.7812e-03],\n",
            "          [-1.6982e-05, -3.6396e-02, -1.0202e-02]],\n",
            "\n",
            "         [[-1.2544e-02, -1.9127e-02, -2.9386e-02],\n",
            "          [ 1.3843e-02, -3.1915e-02,  8.9132e-03],\n",
            "          [ 7.2003e-03, -3.8074e-02,  1.7401e-02]],\n",
            "\n",
            "         [[ 7.9858e-03,  7.3728e-04,  3.3469e-02],\n",
            "          [ 3.2716e-02,  3.6114e-02, -1.7558e-02],\n",
            "          [ 2.4493e-02, -2.5981e-02, -7.4869e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 2.1188e-02, -1.9513e-02,  2.8295e-02],\n",
            "          [-7.0852e-03, -1.9225e-02, -1.5520e-02],\n",
            "          [-2.9467e-02, -1.1199e-02,  2.9379e-02]],\n",
            "\n",
            "         [[ 1.2896e-02, -6.5617e-03, -1.0106e-02],\n",
            "          [ 3.8588e-02,  2.4919e-02, -1.5191e-02],\n",
            "          [-2.4854e-02,  3.0710e-02,  3.1708e-03]],\n",
            "\n",
            "         [[-2.2520e-02,  2.4395e-02,  2.6819e-02],\n",
            "          [ 1.1038e-02, -1.1692e-02,  2.5666e-02],\n",
            "          [-8.9987e-03,  3.1612e-02, -1.6417e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.4466e-02,  2.3254e-02,  1.1318e-02],\n",
            "          [-3.8448e-02,  1.8485e-02, -2.4996e-02],\n",
            "          [-3.0311e-02, -2.3193e-02, -4.0817e-02]],\n",
            "\n",
            "         [[-2.0826e-02,  1.3776e-02,  1.9066e-02],\n",
            "          [-2.2521e-02,  4.1661e-02, -3.4977e-02],\n",
            "          [-2.9047e-02, -4.0930e-02, -3.5936e-02]],\n",
            "\n",
            "         [[ 2.7107e-02,  4.9051e-03, -1.8275e-02],\n",
            "          [-3.7546e-02,  2.3778e-02,  1.3427e-02],\n",
            "          [ 3.8060e-02,  8.0353e-03,  4.1361e-02]]],\n",
            "\n",
            "\n",
            "        [[[-3.1586e-02, -4.1302e-04,  9.7970e-03],\n",
            "          [-1.8644e-02,  1.0069e-02, -2.3913e-02],\n",
            "          [ 2.9329e-02,  3.3192e-02, -2.0143e-02]],\n",
            "\n",
            "         [[-6.4607e-03,  2.5720e-02, -8.9529e-03],\n",
            "          [-3.7884e-02,  1.2695e-02,  3.7998e-02],\n",
            "          [ 1.0695e-02,  7.8590e-03,  4.9304e-03]],\n",
            "\n",
            "         [[-3.0906e-02,  4.0179e-02, -3.8435e-02],\n",
            "          [-2.9079e-03, -3.0287e-02, -4.0541e-02],\n",
            "          [-5.9644e-03, -2.9095e-02, -1.1719e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.3702e-02,  3.7044e-02,  3.3597e-02],\n",
            "          [-3.3850e-02,  3.4561e-02, -2.6860e-02],\n",
            "          [-2.7591e-02,  1.4174e-02,  7.3149e-03]],\n",
            "\n",
            "         [[ 5.7843e-03, -8.7042e-03, -2.0154e-02],\n",
            "          [ 3.5245e-02,  4.5760e-03,  3.5278e-02],\n",
            "          [-2.4300e-02, -3.5348e-02,  2.7713e-02]],\n",
            "\n",
            "         [[ 3.8318e-02,  2.2408e-02, -1.0511e-02],\n",
            "          [ 2.1686e-02, -3.6417e-02,  1.6669e-02],\n",
            "          [ 2.4729e-03, -2.6200e-02,  2.3393e-02]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "N1_C2.bias Parameter containing:\n",
            "tensor([-0.0320, -0.0365, -0.0340,  0.0394, -0.0121,  0.0074,  0.0058,  0.0192,\n",
            "         0.0406, -0.0082,  0.0183, -0.0262,  0.0210,  0.0097, -0.0098, -0.0030,\n",
            "        -0.0153, -0.0321,  0.0363,  0.0099,  0.0003,  0.0179,  0.0131, -0.0290,\n",
            "        -0.0371, -0.0115, -0.0314,  0.0106,  0.0259, -0.0129,  0.0233,  0.0270,\n",
            "        -0.0309, -0.0130, -0.0318,  0.0257, -0.0363,  0.0229, -0.0096,  0.0004,\n",
            "        -0.0156,  0.0043,  0.0196, -0.0259,  0.0341, -0.0293, -0.0386, -0.0357,\n",
            "        -0.0254, -0.0073,  0.0083,  0.0386, -0.0337, -0.0350, -0.0089,  0.0226,\n",
            "         0.0348, -0.0274,  0.0106, -0.0330, -0.0234, -0.0216,  0.0010, -0.0200],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "N1_C3.weight Parameter containing:\n",
            "tensor([[[[-0.0098,  0.0250, -0.0173],\n",
            "          [-0.0153,  0.0006, -0.0378],\n",
            "          [ 0.0222,  0.0311,  0.0038]],\n",
            "\n",
            "         [[-0.0258, -0.0368,  0.0330],\n",
            "          [ 0.0117, -0.0361, -0.0175],\n",
            "          [ 0.0169,  0.0202,  0.0409]],\n",
            "\n",
            "         [[ 0.0340,  0.0234, -0.0403],\n",
            "          [ 0.0084,  0.0084, -0.0105],\n",
            "          [-0.0330,  0.0274, -0.0384]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0189, -0.0290,  0.0161],\n",
            "          [-0.0212,  0.0270, -0.0362],\n",
            "          [ 0.0104, -0.0286, -0.0284]],\n",
            "\n",
            "         [[ 0.0317, -0.0315, -0.0346],\n",
            "          [-0.0079,  0.0288,  0.0172],\n",
            "          [-0.0150, -0.0319,  0.0394]],\n",
            "\n",
            "         [[-0.0157,  0.0003,  0.0345],\n",
            "          [-0.0106,  0.0224,  0.0256],\n",
            "          [-0.0354, -0.0023,  0.0294]]],\n",
            "\n",
            "\n",
            "        [[[-0.0169,  0.0286, -0.0303],\n",
            "          [-0.0159, -0.0356,  0.0277],\n",
            "          [-0.0112,  0.0073,  0.0303]],\n",
            "\n",
            "         [[ 0.0080,  0.0205,  0.0241],\n",
            "          [-0.0077, -0.0360, -0.0329],\n",
            "          [-0.0385,  0.0227,  0.0387]],\n",
            "\n",
            "         [[ 0.0081,  0.0152,  0.0212],\n",
            "          [-0.0279,  0.0259, -0.0297],\n",
            "          [-0.0184,  0.0203, -0.0184]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0251,  0.0405,  0.0383],\n",
            "          [-0.0258,  0.0414, -0.0070],\n",
            "          [-0.0069, -0.0117,  0.0005]],\n",
            "\n",
            "         [[ 0.0049,  0.0251, -0.0303],\n",
            "          [ 0.0395, -0.0322, -0.0018],\n",
            "          [ 0.0009, -0.0001, -0.0089]],\n",
            "\n",
            "         [[-0.0190,  0.0263, -0.0382],\n",
            "          [ 0.0019, -0.0161,  0.0295],\n",
            "          [-0.0409,  0.0301,  0.0140]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0041,  0.0045,  0.0014],\n",
            "          [-0.0216,  0.0393,  0.0267],\n",
            "          [ 0.0013,  0.0182, -0.0399]],\n",
            "\n",
            "         [[ 0.0409, -0.0243, -0.0098],\n",
            "          [-0.0354, -0.0052, -0.0329],\n",
            "          [-0.0296, -0.0115,  0.0066]],\n",
            "\n",
            "         [[ 0.0380,  0.0413,  0.0056],\n",
            "          [ 0.0289,  0.0222, -0.0112],\n",
            "          [ 0.0097, -0.0068,  0.0186]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0193, -0.0333, -0.0035],\n",
            "          [-0.0030,  0.0359,  0.0139],\n",
            "          [-0.0151, -0.0223, -0.0334]],\n",
            "\n",
            "         [[-0.0176,  0.0334, -0.0344],\n",
            "          [ 0.0350, -0.0154, -0.0400],\n",
            "          [-0.0167, -0.0325, -0.0135]],\n",
            "\n",
            "         [[ 0.0384,  0.0021, -0.0142],\n",
            "          [ 0.0044, -0.0180, -0.0341],\n",
            "          [ 0.0274, -0.0311,  0.0317]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0062, -0.0112,  0.0098],\n",
            "          [-0.0065,  0.0029,  0.0186],\n",
            "          [ 0.0411, -0.0140, -0.0097]],\n",
            "\n",
            "         [[ 0.0002, -0.0354, -0.0043],\n",
            "          [ 0.0377,  0.0325,  0.0110],\n",
            "          [-0.0203,  0.0113, -0.0241]],\n",
            "\n",
            "         [[-0.0325, -0.0109,  0.0296],\n",
            "          [-0.0391,  0.0077,  0.0257],\n",
            "          [-0.0328,  0.0035,  0.0368]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0090,  0.0160,  0.0355],\n",
            "          [-0.0017,  0.0325,  0.0077],\n",
            "          [ 0.0206, -0.0072, -0.0274]],\n",
            "\n",
            "         [[ 0.0397,  0.0330,  0.0094],\n",
            "          [-0.0165,  0.0300, -0.0094],\n",
            "          [ 0.0039,  0.0024, -0.0327]],\n",
            "\n",
            "         [[ 0.0304,  0.0087, -0.0217],\n",
            "          [ 0.0178,  0.0076,  0.0053],\n",
            "          [ 0.0125,  0.0120, -0.0180]]],\n",
            "\n",
            "\n",
            "        [[[-0.0264,  0.0201,  0.0197],\n",
            "          [ 0.0212,  0.0102,  0.0345],\n",
            "          [ 0.0282,  0.0069, -0.0209]],\n",
            "\n",
            "         [[-0.0274,  0.0158,  0.0347],\n",
            "          [ 0.0049,  0.0184,  0.0115],\n",
            "          [ 0.0165,  0.0115,  0.0202]],\n",
            "\n",
            "         [[ 0.0347, -0.0182, -0.0327],\n",
            "          [-0.0186,  0.0245,  0.0093],\n",
            "          [ 0.0213, -0.0310, -0.0371]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0256,  0.0291, -0.0416],\n",
            "          [ 0.0237, -0.0374, -0.0256],\n",
            "          [-0.0003, -0.0289,  0.0054]],\n",
            "\n",
            "         [[ 0.0367,  0.0304,  0.0226],\n",
            "          [-0.0247, -0.0200,  0.0205],\n",
            "          [ 0.0271,  0.0049,  0.0146]],\n",
            "\n",
            "         [[-0.0188, -0.0138, -0.0125],\n",
            "          [ 0.0282,  0.0211, -0.0309],\n",
            "          [ 0.0177,  0.0326, -0.0147]]],\n",
            "\n",
            "\n",
            "        [[[-0.0239,  0.0111, -0.0326],\n",
            "          [-0.0378,  0.0393, -0.0177],\n",
            "          [-0.0048, -0.0379, -0.0151]],\n",
            "\n",
            "         [[ 0.0251, -0.0329,  0.0226],\n",
            "          [-0.0374, -0.0029, -0.0117],\n",
            "          [ 0.0221,  0.0274,  0.0380]],\n",
            "\n",
            "         [[ 0.0300,  0.0374, -0.0295],\n",
            "          [ 0.0270, -0.0399, -0.0257],\n",
            "          [-0.0298,  0.0058, -0.0086]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0038, -0.0162,  0.0068],\n",
            "          [ 0.0113,  0.0020, -0.0378],\n",
            "          [ 0.0005,  0.0407, -0.0077]],\n",
            "\n",
            "         [[-0.0337,  0.0213,  0.0090],\n",
            "          [ 0.0127,  0.0242, -0.0195],\n",
            "          [ 0.0290, -0.0303,  0.0361]],\n",
            "\n",
            "         [[ 0.0123,  0.0343,  0.0174],\n",
            "          [ 0.0033,  0.0385, -0.0125],\n",
            "          [-0.0048,  0.0103, -0.0230]]]], device='cuda:0', requires_grad=True)\n",
            "N1_C3.bias Parameter containing:\n",
            "tensor([-0.0349, -0.0029, -0.0175, -0.0385,  0.0119, -0.0407, -0.0084,  0.0115,\n",
            "        -0.0126, -0.0155, -0.0225,  0.0131, -0.0121, -0.0087,  0.0051,  0.0414,\n",
            "         0.0112,  0.0304,  0.0318, -0.0366, -0.0342,  0.0034, -0.0068,  0.0011,\n",
            "         0.0130, -0.0063, -0.0098,  0.0273,  0.0013, -0.0415, -0.0057, -0.0095,\n",
            "        -0.0243, -0.0351,  0.0301, -0.0359, -0.0293,  0.0094,  0.0108, -0.0370,\n",
            "        -0.0267,  0.0363, -0.0373, -0.0293,  0.0034,  0.0388,  0.0180,  0.0415,\n",
            "         0.0401,  0.0134,  0.0251,  0.0013, -0.0272,  0.0409,  0.0139, -0.0204,\n",
            "        -0.0123,  0.0391, -0.0381,  0.0041,  0.0128,  0.0048, -0.0406, -0.0059,\n",
            "         0.0258,  0.0159,  0.0306, -0.0386, -0.0014,  0.0036,  0.0302, -0.0209,\n",
            "         0.0310,  0.0362, -0.0155,  0.0387,  0.0144,  0.0380, -0.0349, -0.0316,\n",
            "         0.0330, -0.0114,  0.0212,  0.0348,  0.0269,  0.0012, -0.0179, -0.0391,\n",
            "         0.0194, -0.0055, -0.0341, -0.0375, -0.0380, -0.0024, -0.0094,  0.0042,\n",
            "        -0.0171,  0.0375,  0.0216,  0.0040,  0.0009,  0.0217, -0.0251,  0.0195,\n",
            "         0.0113,  0.0195,  0.0159,  0.0143, -0.0209,  0.0019, -0.0295,  0.0323,\n",
            "         0.0005,  0.0281,  0.0295, -0.0046,  0.0225,  0.0115,  0.0326, -0.0023,\n",
            "        -0.0086,  0.0128, -0.0118, -0.0203, -0.0358,  0.0343, -0.0044,  0.0139],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "N1_C4.weight Parameter containing:\n",
            "tensor([[[[ 0.0097, -0.0268,  0.0199],\n",
            "          [-0.0258, -0.0075, -0.0270],\n",
            "          [ 0.0216, -0.0022,  0.0146]],\n",
            "\n",
            "         [[ 0.0140, -0.0095,  0.0100],\n",
            "          [ 0.0090, -0.0027, -0.0097],\n",
            "          [ 0.0177,  0.0056,  0.0041]],\n",
            "\n",
            "         [[-0.0250,  0.0196,  0.0196],\n",
            "          [ 0.0210,  0.0080,  0.0212],\n",
            "          [ 0.0095, -0.0177,  0.0146]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0248, -0.0207, -0.0142],\n",
            "          [ 0.0244, -0.0091,  0.0113],\n",
            "          [-0.0138, -0.0289,  0.0219]],\n",
            "\n",
            "         [[ 0.0251,  0.0192, -0.0291],\n",
            "          [-0.0088, -0.0011, -0.0008],\n",
            "          [-0.0140, -0.0033,  0.0102]],\n",
            "\n",
            "         [[-0.0151,  0.0226, -0.0205],\n",
            "          [ 0.0117,  0.0270,  0.0282],\n",
            "          [-0.0118,  0.0081, -0.0133]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0205,  0.0195,  0.0114],\n",
            "          [-0.0032, -0.0134, -0.0124],\n",
            "          [ 0.0294,  0.0284, -0.0116]],\n",
            "\n",
            "         [[-0.0083, -0.0114,  0.0016],\n",
            "          [ 0.0071,  0.0098, -0.0153],\n",
            "          [ 0.0007,  0.0073, -0.0004]],\n",
            "\n",
            "         [[-0.0207,  0.0219, -0.0190],\n",
            "          [ 0.0035,  0.0052, -0.0220],\n",
            "          [-0.0152,  0.0005,  0.0252]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0010, -0.0102,  0.0253],\n",
            "          [ 0.0057,  0.0078,  0.0097],\n",
            "          [-0.0294, -0.0202, -0.0017]],\n",
            "\n",
            "         [[ 0.0101,  0.0131, -0.0180],\n",
            "          [-0.0110,  0.0280,  0.0020],\n",
            "          [-0.0189,  0.0135, -0.0233]],\n",
            "\n",
            "         [[-0.0208, -0.0284,  0.0023],\n",
            "          [-0.0110, -0.0099, -0.0091],\n",
            "          [-0.0185,  0.0007,  0.0271]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0136, -0.0071,  0.0049],\n",
            "          [-0.0010,  0.0083,  0.0261],\n",
            "          [-0.0175, -0.0274,  0.0118]],\n",
            "\n",
            "         [[ 0.0135,  0.0062, -0.0132],\n",
            "          [ 0.0228, -0.0288,  0.0071],\n",
            "          [ 0.0199, -0.0015,  0.0168]],\n",
            "\n",
            "         [[-0.0253,  0.0070,  0.0203],\n",
            "          [-0.0142, -0.0114,  0.0167],\n",
            "          [ 0.0176, -0.0256,  0.0218]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0135,  0.0244, -0.0155],\n",
            "          [-0.0051, -0.0129,  0.0135],\n",
            "          [-0.0194, -0.0129,  0.0065]],\n",
            "\n",
            "         [[-0.0128, -0.0140, -0.0071],\n",
            "          [-0.0286, -0.0132,  0.0017],\n",
            "          [ 0.0137,  0.0143, -0.0198]],\n",
            "\n",
            "         [[-0.0184, -0.0168,  0.0291],\n",
            "          [ 0.0263,  0.0004, -0.0205],\n",
            "          [-0.0118, -0.0184,  0.0118]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0239,  0.0079, -0.0187],\n",
            "          [-0.0221,  0.0013, -0.0045],\n",
            "          [ 0.0095, -0.0271, -0.0232]],\n",
            "\n",
            "         [[ 0.0097,  0.0245, -0.0121],\n",
            "          [-0.0291,  0.0080, -0.0159],\n",
            "          [-0.0229, -0.0170,  0.0233]],\n",
            "\n",
            "         [[-0.0091, -0.0103,  0.0251],\n",
            "          [ 0.0019,  0.0190, -0.0031],\n",
            "          [ 0.0258,  0.0104,  0.0105]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0207,  0.0285,  0.0276],\n",
            "          [-0.0157,  0.0036, -0.0089],\n",
            "          [-0.0203, -0.0154, -0.0074]],\n",
            "\n",
            "         [[ 0.0044, -0.0209, -0.0085],\n",
            "          [-0.0192,  0.0291,  0.0076],\n",
            "          [ 0.0224, -0.0133, -0.0086]],\n",
            "\n",
            "         [[-0.0103, -0.0235,  0.0105],\n",
            "          [-0.0292, -0.0092, -0.0112],\n",
            "          [-0.0053,  0.0123,  0.0054]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0192,  0.0226,  0.0085],\n",
            "          [-0.0141, -0.0216, -0.0107],\n",
            "          [-0.0088, -0.0022,  0.0178]],\n",
            "\n",
            "         [[-0.0186,  0.0205, -0.0093],\n",
            "          [ 0.0008,  0.0276,  0.0224],\n",
            "          [ 0.0283,  0.0275,  0.0171]],\n",
            "\n",
            "         [[-0.0046,  0.0192, -0.0196],\n",
            "          [ 0.0038, -0.0111, -0.0083],\n",
            "          [ 0.0063, -0.0286, -0.0015]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0137,  0.0273,  0.0063],\n",
            "          [ 0.0187,  0.0172, -0.0010],\n",
            "          [ 0.0007, -0.0165,  0.0278]],\n",
            "\n",
            "         [[-0.0121, -0.0136,  0.0092],\n",
            "          [ 0.0095,  0.0041, -0.0143],\n",
            "          [ 0.0226,  0.0290,  0.0086]],\n",
            "\n",
            "         [[ 0.0234,  0.0027, -0.0188],\n",
            "          [-0.0046, -0.0056,  0.0243],\n",
            "          [-0.0063, -0.0251, -0.0152]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0043,  0.0019,  0.0028],\n",
            "          [-0.0273, -0.0163,  0.0173],\n",
            "          [-0.0187, -0.0126,  0.0111]],\n",
            "\n",
            "         [[-0.0111,  0.0293,  0.0245],\n",
            "          [-0.0248,  0.0193,  0.0179],\n",
            "          [ 0.0085,  0.0186,  0.0028]],\n",
            "\n",
            "         [[-0.0254,  0.0029,  0.0287],\n",
            "          [-0.0258,  0.0284, -0.0271],\n",
            "          [ 0.0093,  0.0014,  0.0004]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0044,  0.0113, -0.0200],\n",
            "          [ 0.0056,  0.0054, -0.0141],\n",
            "          [ 0.0150,  0.0055, -0.0034]],\n",
            "\n",
            "         [[ 0.0046,  0.0077,  0.0175],\n",
            "          [ 0.0062, -0.0119,  0.0190],\n",
            "          [-0.0057,  0.0250,  0.0201]],\n",
            "\n",
            "         [[-0.0046, -0.0160, -0.0292],\n",
            "          [ 0.0095,  0.0018, -0.0115],\n",
            "          [-0.0284, -0.0217, -0.0029]]]], device='cuda:0', requires_grad=True)\n",
            "N1_C4.bias Parameter containing:\n",
            "tensor([-0.0270, -0.0107, -0.0081, -0.0145, -0.0107, -0.0271,  0.0080,  0.0110,\n",
            "        -0.0045,  0.0266,  0.0255, -0.0091, -0.0034, -0.0233, -0.0169,  0.0143,\n",
            "        -0.0169,  0.0170,  0.0265, -0.0173,  0.0104, -0.0138, -0.0055,  0.0288,\n",
            "        -0.0246, -0.0086, -0.0291, -0.0014, -0.0043,  0.0031,  0.0043, -0.0145,\n",
            "        -0.0010, -0.0001, -0.0204, -0.0110, -0.0107,  0.0089, -0.0253, -0.0196,\n",
            "        -0.0078,  0.0198,  0.0052,  0.0074, -0.0172, -0.0218,  0.0106,  0.0193,\n",
            "         0.0052,  0.0280, -0.0095,  0.0174, -0.0200,  0.0193, -0.0213,  0.0227,\n",
            "         0.0221,  0.0116, -0.0232, -0.0251,  0.0161, -0.0249, -0.0177,  0.0149,\n",
            "        -0.0158,  0.0194, -0.0098, -0.0205, -0.0212, -0.0241, -0.0183, -0.0293,\n",
            "         0.0183,  0.0290, -0.0107, -0.0144, -0.0181,  0.0211,  0.0025, -0.0030,\n",
            "        -0.0229,  0.0013, -0.0013,  0.0197, -0.0027,  0.0110,  0.0130, -0.0254,\n",
            "        -0.0152,  0.0019,  0.0169,  0.0192,  0.0119,  0.0029,  0.0011, -0.0142,\n",
            "         0.0031, -0.0293, -0.0219, -0.0183,  0.0088,  0.0283, -0.0270, -0.0034,\n",
            "        -0.0044, -0.0145,  0.0228, -0.0258, -0.0045, -0.0252, -0.0046,  0.0204,\n",
            "        -0.0163, -0.0038,  0.0028,  0.0055,  0.0186, -0.0043,  0.0153,  0.0157,\n",
            "        -0.0249,  0.0177,  0.0169, -0.0134, -0.0202,  0.0257, -0.0041,  0.0021],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "N1_D1.weight Parameter containing:\n",
            "tensor([[ 3.7357e-04,  1.7096e-03, -2.1988e-03,  ...,  2.5870e-03,\n",
            "          9.9286e-04, -1.7564e-03],\n",
            "        [-2.2589e-03, -4.7503e-04, -2.3234e-03,  ..., -1.8000e-03,\n",
            "          2.4525e-04,  1.1320e-05],\n",
            "        [ 1.7556e-03, -6.6452e-04, -2.2938e-03,  ...,  2.3762e-03,\n",
            "         -1.0299e-03, -2.1110e-03],\n",
            "        ...,\n",
            "        [-3.0780e-04, -9.0102e-04, -2.5982e-03,  ...,  1.6005e-03,\n",
            "         -9.0005e-04, -2.5777e-03],\n",
            "        [-1.7391e-03,  2.4062e-03, -2.3959e-03,  ..., -1.7922e-03,\n",
            "         -1.5699e-03,  1.9379e-04],\n",
            "        [ 2.8220e-04,  2.0818e-03, -2.0051e-03,  ...,  4.2431e-04,\n",
            "          1.6805e-03,  8.2262e-05]], device='cuda:0', requires_grad=True)\n",
            "N1_D1.bias Parameter containing:\n",
            "tensor([-9.1868e-04,  1.4574e-03, -5.5385e-04,  1.4733e-03, -1.7231e-03,\n",
            "         5.4734e-04,  1.6950e-03, -1.3566e-03,  1.6466e-03, -1.9099e-03,\n",
            "        -1.3473e-03, -3.6733e-04,  5.1408e-04,  1.1366e-03, -1.6935e-03,\n",
            "        -1.6355e-03,  2.4473e-03, -1.5924e-03, -1.0013e-03, -1.6439e-03,\n",
            "        -1.6171e-03,  4.1490e-05, -1.7567e-03,  1.5868e-03, -5.2722e-04,\n",
            "        -1.7984e-03,  7.5654e-04,  2.5318e-03, -2.3294e-03, -2.0705e-03,\n",
            "        -9.4504e-04, -1.6983e-03,  1.7405e-03,  1.2383e-03, -2.9174e-04,\n",
            "        -2.1479e-03, -1.0964e-03,  2.6667e-03, -4.0577e-04, -8.7674e-04,\n",
            "        -1.5883e-03, -7.5382e-05,  6.7722e-04, -1.0014e-04, -2.6537e-03,\n",
            "         2.3118e-03, -1.5702e-04, -1.1645e-03,  5.0328e-04,  1.6346e-03,\n",
            "         2.7272e-03,  1.4342e-03, -1.1181e-04, -1.2765e-03, -2.1248e-03,\n",
            "        -5.7680e-04,  1.6541e-03, -5.4752e-05,  9.5570e-04,  2.6980e-03,\n",
            "         1.3190e-03, -2.4348e-03, -1.7621e-04, -2.1300e-03, -2.7596e-03,\n",
            "        -1.6300e-03,  1.2577e-03,  1.9700e-03, -1.6161e-03,  1.6687e-03,\n",
            "        -1.2907e-03, -1.1548e-03, -2.1969e-03, -1.3804e-03,  9.6211e-04,\n",
            "        -1.0161e-03, -1.1275e-03,  4.3588e-05,  7.2956e-05,  2.2526e-03,\n",
            "        -1.1568e-03,  1.7921e-04,  5.0960e-04, -1.8156e-03,  9.9168e-04,\n",
            "        -2.2297e-04, -2.2548e-03,  2.2986e-03, -1.4638e-03, -2.1378e-03,\n",
            "        -2.0261e-03,  9.1921e-05, -1.9008e-03,  4.3690e-04, -7.3609e-04,\n",
            "         2.0922e-04,  2.6934e-04,  1.1953e-03, -1.3756e-03, -1.1764e-03,\n",
            "         1.6893e-03,  4.7321e-04, -2.2731e-03,  3.3659e-04,  1.4339e-03,\n",
            "         1.0423e-03, -1.0751e-03,  1.7727e-03,  9.7590e-04, -1.0601e-03,\n",
            "        -1.0412e-03,  2.4413e-03, -2.6352e-03, -3.8094e-06, -4.4429e-04,\n",
            "        -5.6216e-04, -6.5745e-04,  1.9698e-05,  1.1417e-04, -1.4591e-03,\n",
            "         1.0164e-03, -2.3474e-04, -5.2458e-04,  2.3132e-04,  9.2057e-04,\n",
            "         8.9168e-04,  4.1289e-04, -1.9708e-03, -4.7252e-04, -9.0755e-04,\n",
            "        -2.5871e-03, -2.5936e-03,  1.4053e-03,  4.2521e-04,  5.2367e-04,\n",
            "        -1.7727e-04, -1.2485e-03, -9.6552e-04,  1.8101e-03, -2.2110e-03,\n",
            "         1.2767e-04,  7.0151e-04, -1.2500e-03,  2.6557e-04, -1.1620e-03,\n",
            "         2.7406e-03,  5.7843e-04, -1.6013e-03, -2.0505e-03, -4.0701e-04,\n",
            "         5.0865e-04,  1.0841e-03,  1.8780e-03,  1.3510e-03,  2.1950e-03,\n",
            "         2.4844e-04, -2.0711e-03,  1.6855e-03, -2.6135e-03, -1.3662e-03,\n",
            "         2.1992e-03, -2.1937e-03, -2.3526e-03,  2.0159e-03, -8.6581e-04,\n",
            "        -1.7421e-03, -1.3774e-03, -1.7236e-03,  1.2536e-03, -1.9218e-03,\n",
            "         7.2590e-04, -1.3427e-03, -1.9518e-03,  1.1070e-04,  7.3577e-04,\n",
            "         9.7200e-04, -1.5807e-03,  2.0201e-03,  6.5586e-05, -1.9303e-03,\n",
            "        -2.6975e-03, -2.4591e-03, -6.8761e-04,  2.0532e-03,  1.0111e-03,\n",
            "         2.4207e-03,  3.6892e-05, -2.3325e-04, -2.7169e-03,  5.5253e-04,\n",
            "         1.0049e-04,  8.8130e-04, -2.6016e-03,  2.6658e-03, -9.3600e-05,\n",
            "         1.2633e-03,  2.7070e-03,  1.1873e-03,  2.1842e-03, -2.5888e-03,\n",
            "        -8.2858e-04, -1.1602e-03, -1.1969e-03, -1.5265e-03,  2.7282e-03,\n",
            "         5.9046e-05,  1.5396e-03, -1.0444e-04, -1.6544e-03,  3.3296e-04,\n",
            "         1.3833e-03,  3.5873e-04, -3.8270e-04,  1.4788e-03,  1.8972e-03,\n",
            "        -1.5724e-03, -1.5718e-05, -1.7366e-03,  1.1990e-03, -7.5734e-04,\n",
            "         2.6902e-03,  1.9442e-03, -2.2762e-03,  2.3105e-04,  2.5580e-03,\n",
            "        -1.8026e-03,  6.2258e-04, -1.6599e-03, -1.1594e-03, -1.4149e-03,\n",
            "        -1.8114e-03,  1.1038e-03,  2.3373e-03,  1.3618e-03,  1.6223e-03,\n",
            "         2.0874e-03, -1.4773e-05,  1.8139e-03,  7.8020e-04,  3.3073e-04,\n",
            "         1.2319e-03,  2.5674e-03,  1.6732e-03, -6.0876e-04, -7.7866e-04,\n",
            "        -6.6400e-04, -8.2588e-04, -1.8477e-03,  2.3334e-03, -8.4138e-04,\n",
            "         2.6644e-03, -2.0026e-04,  1.1403e-03, -1.2640e-03, -1.6609e-03,\n",
            "         9.0035e-04], device='cuda:0', requires_grad=True)\n",
            "N1_D2.weight Parameter containing:\n",
            "tensor([[ 0.0236, -0.0199,  0.0252,  ..., -0.0410, -0.0093,  0.0112],\n",
            "        [ 0.0037, -0.0062,  0.0216,  ...,  0.0193,  0.0506, -0.0093],\n",
            "        [ 0.0536,  0.0043, -0.0238,  ...,  0.0476, -0.0220,  0.0347],\n",
            "        ...,\n",
            "        [-0.0344,  0.0528, -0.0372,  ..., -0.0555,  0.0552,  0.0229],\n",
            "        [-0.0192,  0.0370,  0.0302,  ...,  0.0390,  0.0179, -0.0029],\n",
            "        [ 0.0170,  0.0578,  0.0387,  ...,  0.0397,  0.0436, -0.0518]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "N1_D2.bias Parameter containing:\n",
            "tensor([-2.4847e-02,  2.3339e-02,  4.9683e-02, -4.9786e-02,  5.5806e-02,\n",
            "        -2.9661e-05, -3.4366e-02,  5.9643e-02, -4.8723e-02,  5.7998e-02,\n",
            "         2.6015e-02, -1.5867e-02, -5.8647e-02, -2.4784e-02,  4.0612e-02,\n",
            "        -3.2564e-02,  5.4145e-02, -6.1836e-02,  3.9066e-02,  1.5048e-02,\n",
            "         1.1802e-03,  1.1590e-02,  1.7936e-02, -1.4069e-02,  4.7171e-02,\n",
            "        -3.3515e-02, -6.1843e-02,  2.4605e-02,  8.5264e-03, -2.4134e-03,\n",
            "        -5.3071e-02, -4.0628e-02,  1.4374e-03,  1.6547e-02, -5.7313e-03,\n",
            "        -3.1454e-03,  4.9312e-02, -5.5504e-02,  4.3424e-02, -4.8686e-02,\n",
            "        -3.2078e-02,  1.0307e-02, -8.2878e-03,  2.5438e-02,  1.3125e-02,\n",
            "         4.3168e-02, -4.3921e-02,  4.1184e-02, -2.1784e-03, -1.6320e-02,\n",
            "         3.6484e-02,  5.4648e-02, -9.7992e-03,  1.9078e-02, -2.2768e-02,\n",
            "        -7.9316e-03, -7.6596e-03, -4.6760e-02, -1.1376e-02,  2.9389e-02,\n",
            "         1.5848e-02,  4.3119e-02, -5.0201e-03,  1.2645e-02,  5.8342e-02,\n",
            "         1.2690e-02,  2.3650e-02,  2.0369e-02,  3.8358e-02, -1.5943e-03,\n",
            "        -3.0832e-02,  3.7596e-02, -4.5866e-02, -3.5358e-02, -3.6772e-02,\n",
            "         4.1099e-02,  3.8491e-02, -2.5977e-02,  4.2973e-02,  5.0279e-02,\n",
            "         1.2284e-02,  6.0259e-02,  2.0227e-03,  5.7192e-02,  5.3267e-02,\n",
            "        -5.3125e-02,  3.2041e-02, -4.5696e-02,  7.3455e-04, -5.0952e-02,\n",
            "        -4.7391e-02,  5.0175e-02,  1.9999e-02,  5.1543e-02, -2.0639e-02,\n",
            "        -6.0194e-02, -3.4724e-02, -4.2808e-02, -2.3015e-02, -4.4702e-03,\n",
            "        -3.1839e-02, -5.7567e-02, -1.8739e-02,  1.9134e-02,  2.2331e-03,\n",
            "        -9.2405e-03, -5.0656e-02,  1.5865e-02, -2.0730e-02, -5.5441e-02,\n",
            "         4.6055e-02,  3.8297e-02,  2.2296e-02,  2.0826e-02,  6.1404e-02,\n",
            "         9.2390e-03, -4.9099e-02,  2.7514e-02,  1.0079e-02,  5.0305e-02,\n",
            "        -2.5877e-02,  4.9937e-02, -3.1312e-03,  4.9573e-02, -5.9881e-02,\n",
            "         6.4170e-03, -1.3882e-02, -8.3657e-03,  8.8212e-03,  2.4294e-02,\n",
            "        -5.7440e-02,  5.5393e-02,  1.7823e-02, -6.0983e-02, -1.6849e-02,\n",
            "        -4.7554e-02,  2.0478e-02,  3.3336e-02,  5.9956e-02,  2.9706e-02,\n",
            "         2.9894e-02, -5.8266e-02,  4.6659e-02,  2.6252e-02, -3.0851e-02,\n",
            "        -1.9391e-02,  2.7986e-02, -5.7654e-02, -3.4511e-02,  4.0961e-02,\n",
            "        -3.6477e-03,  2.6669e-02,  1.2560e-02, -4.6550e-02, -1.3010e-02,\n",
            "        -2.6965e-02,  5.7760e-02,  1.6093e-02, -5.3390e-02, -2.5434e-02,\n",
            "         6.1871e-02,  5.5310e-02, -6.1169e-02, -3.8062e-02, -3.5796e-02,\n",
            "        -1.3469e-02, -2.9284e-02,  2.4300e-02,  3.1576e-02,  3.8441e-02,\n",
            "        -4.4873e-02,  2.2475e-02, -4.5123e-04, -1.4943e-02, -6.4897e-03,\n",
            "         5.7129e-02,  3.5309e-02, -5.6442e-02,  4.2531e-02,  2.7175e-02,\n",
            "        -2.2534e-02,  6.9944e-04, -5.2422e-02, -4.2936e-02, -6.1909e-02,\n",
            "         1.4573e-02, -2.5479e-02,  5.3059e-02, -1.0752e-02,  5.4261e-02,\n",
            "         3.1495e-02,  3.0467e-03, -5.6737e-02, -5.9330e-02,  1.8354e-02,\n",
            "         1.0440e-02,  3.9779e-02, -3.5122e-02,  4.9201e-02, -4.9555e-02,\n",
            "         3.4163e-02,  6.2259e-02,  1.7473e-02, -2.6943e-02,  2.4829e-02,\n",
            "        -3.6488e-02,  4.8242e-02, -2.6539e-03, -1.6685e-02, -7.4063e-03,\n",
            "        -3.1100e-02,  5.2564e-02,  4.4944e-02,  9.2145e-03, -2.8206e-02,\n",
            "        -4.4559e-02, -1.2724e-02, -1.9860e-02, -3.4485e-02,  5.3997e-02,\n",
            "        -5.1313e-02, -1.1193e-02,  4.0470e-02, -6.6018e-03,  5.3207e-02,\n",
            "        -1.8625e-03, -9.7669e-03, -5.5743e-02, -6.0926e-02,  5.5239e-03,\n",
            "         2.9686e-02,  3.6732e-02,  3.8418e-02, -3.9391e-02, -5.8046e-02,\n",
            "        -1.6461e-02,  5.7965e-02, -1.0977e-02, -3.4164e-02,  5.3287e-02,\n",
            "         5.1770e-02,  3.0626e-02,  3.0532e-02,  5.3687e-02, -2.1632e-02,\n",
            "        -1.3592e-02,  3.7990e-02, -5.5273e-02,  1.3898e-02, -2.2502e-02,\n",
            "        -2.5869e-02, -5.5740e-02,  2.0784e-02,  2.7145e-02,  5.2498e-02,\n",
            "        -5.1665e-02], device='cuda:0', requires_grad=True)\n",
            "N2_C1.weight Parameter containing:\n",
            "tensor([[[[ 0.1655, -0.1023, -0.0654],\n",
            "          [ 0.1320,  0.1459, -0.0293],\n",
            "          [ 0.1307, -0.0038, -0.1443]],\n",
            "\n",
            "         [[ 0.1255, -0.1350,  0.0066],\n",
            "          [-0.0885,  0.1231, -0.1065],\n",
            "          [-0.0700, -0.0737,  0.1228]],\n",
            "\n",
            "         [[ 0.1619,  0.1924,  0.1041],\n",
            "          [ 0.1877,  0.1497,  0.0791],\n",
            "          [-0.0548,  0.0440, -0.0788]]],\n",
            "\n",
            "\n",
            "        [[[-0.1344, -0.0201,  0.0196],\n",
            "          [-0.0519,  0.0757, -0.0161],\n",
            "          [-0.1060, -0.0630, -0.0215]],\n",
            "\n",
            "         [[ 0.0048, -0.1831,  0.1577],\n",
            "          [-0.1467,  0.1101,  0.1224],\n",
            "          [-0.0428,  0.1472, -0.1545]],\n",
            "\n",
            "         [[ 0.1256,  0.1142,  0.1272],\n",
            "          [-0.0127, -0.0286,  0.1315],\n",
            "          [ 0.1065,  0.0171,  0.1037]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0077, -0.0407, -0.0247],\n",
            "          [ 0.0596,  0.0894, -0.1481],\n",
            "          [ 0.1123, -0.1364, -0.1826]],\n",
            "\n",
            "         [[ 0.0936, -0.0706, -0.1452],\n",
            "          [ 0.1321, -0.0645,  0.0175],\n",
            "          [-0.0288,  0.1139,  0.0435]],\n",
            "\n",
            "         [[-0.1189, -0.1822,  0.1268],\n",
            "          [ 0.0598,  0.1577, -0.0791],\n",
            "          [-0.0528, -0.0397,  0.1527]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0128, -0.0542, -0.1227],\n",
            "          [ 0.0888,  0.0861, -0.1001],\n",
            "          [-0.0162, -0.1255, -0.1477]],\n",
            "\n",
            "         [[ 0.0792, -0.1393,  0.1429],\n",
            "          [ 0.0856, -0.1878,  0.1810],\n",
            "          [ 0.0429, -0.0350, -0.0249]],\n",
            "\n",
            "         [[ 0.1902,  0.1473, -0.1349],\n",
            "          [-0.1016,  0.0181,  0.0599],\n",
            "          [-0.0560, -0.0279, -0.1527]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0145, -0.1641,  0.1091],\n",
            "          [ 0.0806, -0.1125,  0.0487],\n",
            "          [ 0.0226,  0.0997, -0.0657]],\n",
            "\n",
            "         [[ 0.1541, -0.0840,  0.1102],\n",
            "          [-0.1436, -0.0688, -0.0991],\n",
            "          [-0.0961,  0.1541,  0.0594]],\n",
            "\n",
            "         [[ 0.0446,  0.1434,  0.0999],\n",
            "          [-0.0178,  0.1110, -0.0866],\n",
            "          [ 0.0818,  0.1547, -0.0135]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0570,  0.0230,  0.1441],\n",
            "          [ 0.1004,  0.1370, -0.1546],\n",
            "          [-0.1582,  0.1786,  0.1786]],\n",
            "\n",
            "         [[-0.0714, -0.1461, -0.0242],\n",
            "          [-0.1655,  0.0348,  0.0625],\n",
            "          [-0.0380, -0.0692,  0.1028]],\n",
            "\n",
            "         [[-0.0100,  0.1184,  0.0539],\n",
            "          [ 0.0535,  0.0673, -0.0018],\n",
            "          [-0.0840,  0.0528, -0.0486]]]], device='cuda:0', requires_grad=True)\n",
            "N2_C1.bias Parameter containing:\n",
            "tensor([ 0.0149,  0.1442,  0.0307,  0.1423,  0.0271,  0.0737, -0.0683,  0.0026,\n",
            "         0.1769,  0.0853, -0.0475,  0.0976,  0.1593, -0.0339,  0.0075, -0.1508,\n",
            "         0.1101, -0.0543, -0.0705,  0.1912,  0.0731,  0.1866, -0.0827,  0.1098,\n",
            "        -0.0151, -0.0588, -0.0326, -0.1420, -0.0818, -0.1791, -0.1114,  0.1399,\n",
            "        -0.0034,  0.0182,  0.1790,  0.0359,  0.0357,  0.0303,  0.0446,  0.0245,\n",
            "        -0.0075, -0.0333, -0.1813,  0.0199, -0.1550,  0.1054, -0.0339,  0.1648,\n",
            "        -0.1775,  0.0882,  0.0131,  0.1332, -0.1767,  0.1763, -0.0458, -0.0123,\n",
            "         0.0622, -0.0902, -0.0139,  0.0092, -0.1347, -0.1719,  0.1017, -0.1107],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "N2_C2.weight Parameter containing:\n",
            "tensor([[[[-0.0389,  0.0170, -0.0208],\n",
            "          [-0.0350,  0.0032, -0.0136],\n",
            "          [ 0.0393,  0.0085,  0.0415]],\n",
            "\n",
            "         [[-0.0110,  0.0319, -0.0036],\n",
            "          [ 0.0129, -0.0233,  0.0194],\n",
            "          [-0.0002, -0.0313, -0.0224]],\n",
            "\n",
            "         [[ 0.0323, -0.0017,  0.0073],\n",
            "          [-0.0393, -0.0385,  0.0015],\n",
            "          [ 0.0160,  0.0142, -0.0150]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0226, -0.0023, -0.0325],\n",
            "          [-0.0067, -0.0313,  0.0050],\n",
            "          [-0.0100, -0.0309,  0.0306]],\n",
            "\n",
            "         [[ 0.0046, -0.0414,  0.0237],\n",
            "          [ 0.0085, -0.0149, -0.0062],\n",
            "          [ 0.0201, -0.0125, -0.0394]],\n",
            "\n",
            "         [[ 0.0250, -0.0098, -0.0378],\n",
            "          [-0.0013,  0.0068,  0.0200],\n",
            "          [-0.0110, -0.0193,  0.0377]]],\n",
            "\n",
            "\n",
            "        [[[-0.0164,  0.0298, -0.0226],\n",
            "          [-0.0053,  0.0150, -0.0170],\n",
            "          [ 0.0113, -0.0204,  0.0118]],\n",
            "\n",
            "         [[ 0.0299,  0.0228,  0.0225],\n",
            "          [ 0.0184,  0.0150,  0.0334],\n",
            "          [ 0.0132,  0.0409, -0.0052]],\n",
            "\n",
            "         [[-0.0277,  0.0085, -0.0128],\n",
            "          [ 0.0103,  0.0330,  0.0330],\n",
            "          [ 0.0278, -0.0142, -0.0263]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0029, -0.0032, -0.0070],\n",
            "          [-0.0194, -0.0160,  0.0381],\n",
            "          [ 0.0061, -0.0302,  0.0266]],\n",
            "\n",
            "         [[-0.0111,  0.0157, -0.0345],\n",
            "          [ 0.0276, -0.0384,  0.0203],\n",
            "          [ 0.0272, -0.0292, -0.0107]],\n",
            "\n",
            "         [[ 0.0149,  0.0013,  0.0218],\n",
            "          [ 0.0063,  0.0019, -0.0257],\n",
            "          [ 0.0194, -0.0030, -0.0356]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0145, -0.0224,  0.0394],\n",
            "          [-0.0002,  0.0065,  0.0179],\n",
            "          [-0.0270,  0.0035,  0.0280]],\n",
            "\n",
            "         [[-0.0197, -0.0115,  0.0117],\n",
            "          [-0.0013, -0.0271, -0.0352],\n",
            "          [ 0.0191, -0.0184,  0.0320]],\n",
            "\n",
            "         [[ 0.0158,  0.0251, -0.0025],\n",
            "          [-0.0406, -0.0224, -0.0075],\n",
            "          [-0.0168, -0.0329, -0.0249]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0058,  0.0263,  0.0394],\n",
            "          [ 0.0162,  0.0064, -0.0033],\n",
            "          [-0.0091,  0.0195, -0.0409]],\n",
            "\n",
            "         [[-0.0392, -0.0037,  0.0083],\n",
            "          [-0.0383,  0.0139,  0.0160],\n",
            "          [-0.0271, -0.0305, -0.0244]],\n",
            "\n",
            "         [[ 0.0059, -0.0162, -0.0148],\n",
            "          [-0.0089, -0.0217,  0.0171],\n",
            "          [-0.0296, -0.0173,  0.0333]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.0287, -0.0300, -0.0012],\n",
            "          [-0.0388, -0.0136, -0.0271],\n",
            "          [-0.0228, -0.0080,  0.0064]],\n",
            "\n",
            "         [[-0.0050,  0.0318, -0.0335],\n",
            "          [ 0.0124,  0.0028,  0.0271],\n",
            "          [ 0.0005, -0.0091, -0.0405]],\n",
            "\n",
            "         [[ 0.0098, -0.0109,  0.0025],\n",
            "          [ 0.0098, -0.0340,  0.0115],\n",
            "          [ 0.0413,  0.0371, -0.0307]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0064,  0.0322,  0.0144],\n",
            "          [ 0.0067, -0.0266,  0.0162],\n",
            "          [-0.0416,  0.0024, -0.0378]],\n",
            "\n",
            "         [[ 0.0230,  0.0106, -0.0379],\n",
            "          [ 0.0189,  0.0054,  0.0410],\n",
            "          [ 0.0075,  0.0299, -0.0270]],\n",
            "\n",
            "         [[-0.0022,  0.0031, -0.0366],\n",
            "          [-0.0252,  0.0054,  0.0013],\n",
            "          [ 0.0150, -0.0282, -0.0341]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0172, -0.0369, -0.0054],\n",
            "          [ 0.0355,  0.0364, -0.0211],\n",
            "          [-0.0103,  0.0209, -0.0041]],\n",
            "\n",
            "         [[-0.0023,  0.0336, -0.0006],\n",
            "          [ 0.0347,  0.0007,  0.0207],\n",
            "          [ 0.0409, -0.0385,  0.0415]],\n",
            "\n",
            "         [[ 0.0261,  0.0291,  0.0019],\n",
            "          [ 0.0415,  0.0307,  0.0290],\n",
            "          [ 0.0298, -0.0058, -0.0374]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0367, -0.0008, -0.0172],\n",
            "          [-0.0336,  0.0351, -0.0410],\n",
            "          [-0.0164, -0.0321,  0.0103]],\n",
            "\n",
            "         [[ 0.0354,  0.0046,  0.0350],\n",
            "          [ 0.0161, -0.0114,  0.0070],\n",
            "          [-0.0070,  0.0252, -0.0335]],\n",
            "\n",
            "         [[ 0.0043,  0.0020,  0.0344],\n",
            "          [-0.0318,  0.0090,  0.0079],\n",
            "          [-0.0332, -0.0240,  0.0127]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0213,  0.0002,  0.0230],\n",
            "          [ 0.0220, -0.0171, -0.0318],\n",
            "          [-0.0267,  0.0074, -0.0368]],\n",
            "\n",
            "         [[ 0.0336, -0.0401,  0.0207],\n",
            "          [ 0.0137, -0.0208, -0.0291],\n",
            "          [-0.0146,  0.0050,  0.0302]],\n",
            "\n",
            "         [[ 0.0125,  0.0210,  0.0008],\n",
            "          [ 0.0089, -0.0084, -0.0107],\n",
            "          [ 0.0090,  0.0283,  0.0221]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0366,  0.0021, -0.0137],\n",
            "          [-0.0019,  0.0200, -0.0047],\n",
            "          [ 0.0043,  0.0349,  0.0014]],\n",
            "\n",
            "         [[ 0.0228, -0.0173,  0.0271],\n",
            "          [-0.0259,  0.0283, -0.0281],\n",
            "          [ 0.0036, -0.0072,  0.0374]],\n",
            "\n",
            "         [[-0.0078,  0.0276, -0.0326],\n",
            "          [-0.0349,  0.0222,  0.0149],\n",
            "          [-0.0135,  0.0273,  0.0118]]]], device='cuda:0', requires_grad=True)\n",
            "N2_C2.bias Parameter containing:\n",
            "tensor([-0.0362, -0.0107,  0.0200, -0.0066,  0.0123, -0.0369,  0.0246, -0.0410,\n",
            "        -0.0131,  0.0193, -0.0004, -0.0232, -0.0260,  0.0127, -0.0149, -0.0141,\n",
            "         0.0182, -0.0347, -0.0321, -0.0086, -0.0108, -0.0146, -0.0298,  0.0286,\n",
            "        -0.0156,  0.0028,  0.0255, -0.0264,  0.0237,  0.0093,  0.0390, -0.0025,\n",
            "         0.0050,  0.0258, -0.0268,  0.0300,  0.0208,  0.0199,  0.0124, -0.0149,\n",
            "         0.0063,  0.0361,  0.0230,  0.0362,  0.0387,  0.0081,  0.0069, -0.0302,\n",
            "         0.0285,  0.0226,  0.0228,  0.0167, -0.0019, -0.0169,  0.0065, -0.0353,\n",
            "        -0.0096,  0.0106,  0.0151,  0.0279,  0.0012, -0.0284, -0.0333,  0.0218],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "N2_C3.weight Parameter containing:\n",
            "tensor([[[[ 1.6478e-02,  3.6358e-02, -2.4660e-03],\n",
            "          [ 1.4462e-02,  7.8154e-03, -8.7056e-03],\n",
            "          [ 2.8477e-02,  8.8503e-03, -3.6299e-02]],\n",
            "\n",
            "         [[ 3.2571e-02,  1.0216e-03, -3.5548e-02],\n",
            "          [ 2.6030e-03,  7.9593e-03,  2.7901e-02],\n",
            "          [ 3.2491e-02,  2.8091e-04, -2.6749e-02]],\n",
            "\n",
            "         [[-3.7332e-02,  2.2772e-02, -2.1687e-02],\n",
            "          [ 8.6482e-03, -2.7712e-02, -1.8260e-02],\n",
            "          [-1.8952e-02,  4.1075e-02,  3.1533e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.3446e-02,  6.7809e-04,  1.0093e-02],\n",
            "          [-5.1189e-03, -4.0486e-02,  3.2720e-02],\n",
            "          [-1.2961e-02,  5.4246e-03,  3.0900e-02]],\n",
            "\n",
            "         [[ 8.2329e-03, -3.8339e-02, -2.4085e-02],\n",
            "          [ 1.4021e-02, -3.5651e-02,  3.3019e-02],\n",
            "          [ 2.1793e-02, -3.8337e-02,  3.4147e-02]],\n",
            "\n",
            "         [[-2.4871e-03, -3.0027e-02, -5.4283e-03],\n",
            "          [-2.2348e-02, -1.2380e-02,  1.6490e-02],\n",
            "          [-2.1183e-02,  1.9155e-02,  1.4722e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.3230e-02,  2.0484e-02, -2.0927e-02],\n",
            "          [-1.5153e-02,  3.0153e-02, -2.5913e-02],\n",
            "          [ 3.3377e-02,  2.9313e-02,  1.6354e-03]],\n",
            "\n",
            "         [[-2.4203e-02,  3.5340e-02,  1.8368e-02],\n",
            "          [-3.4976e-02, -2.2532e-02, -2.5597e-02],\n",
            "          [-1.5514e-02, -1.7802e-02, -2.9605e-02]],\n",
            "\n",
            "         [[ 3.6059e-02, -3.1973e-02, -4.0246e-02],\n",
            "          [ 1.3481e-04, -1.2641e-02, -2.5085e-02],\n",
            "          [ 1.0859e-02, -6.5656e-03,  3.9958e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.4300e-02, -3.3946e-02, -1.2755e-02],\n",
            "          [-5.5192e-03,  3.1808e-02, -4.6633e-03],\n",
            "          [-2.5712e-02,  4.7626e-03, -2.6625e-03]],\n",
            "\n",
            "         [[ 3.0555e-02, -2.1988e-02,  2.3051e-03],\n",
            "          [-2.6687e-02,  9.9558e-03,  3.4667e-02],\n",
            "          [ 4.1517e-02,  3.9132e-02, -1.3778e-02]],\n",
            "\n",
            "         [[-3.6121e-03, -8.3959e-03, -3.0312e-02],\n",
            "          [-7.9862e-03,  2.0249e-02, -3.5809e-02],\n",
            "          [ 9.6957e-04, -3.6163e-03,  2.3563e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.1184e-02,  3.8366e-02,  2.2504e-02],\n",
            "          [ 8.3281e-03, -4.1128e-02,  1.8214e-02],\n",
            "          [-2.6699e-02, -3.7198e-02,  7.4583e-03]],\n",
            "\n",
            "         [[-1.2468e-02, -3.3490e-02,  3.5394e-02],\n",
            "          [-3.2529e-02,  3.8864e-02, -3.7815e-02],\n",
            "          [-4.1045e-02, -2.5406e-02, -3.3510e-02]],\n",
            "\n",
            "         [[ 2.5755e-02,  7.6556e-03,  1.3890e-02],\n",
            "          [ 4.3359e-03,  3.8102e-02,  1.3977e-02],\n",
            "          [-1.1940e-02, -3.4510e-02,  3.2808e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.6105e-04, -3.8821e-02, -3.7192e-03],\n",
            "          [ 5.8216e-03, -3.1990e-02,  2.8387e-02],\n",
            "          [ 3.4090e-02, -2.2243e-03,  1.0163e-02]],\n",
            "\n",
            "         [[ 2.2208e-02,  4.9898e-03,  5.8362e-03],\n",
            "          [ 3.8052e-02,  2.7821e-02, -4.1032e-02],\n",
            "          [ 8.1057e-03, -3.0102e-02,  2.3647e-02]],\n",
            "\n",
            "         [[-1.7566e-02, -2.0587e-02,  2.4119e-02],\n",
            "          [ 3.1486e-02, -1.3044e-02,  2.0060e-02],\n",
            "          [-3.5179e-02, -2.5455e-02,  1.6286e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 2.6174e-02,  3.9409e-02,  9.9017e-03],\n",
            "          [-3.1703e-02, -1.9336e-02,  1.6662e-02],\n",
            "          [-2.1843e-02, -2.0828e-02,  3.2698e-02]],\n",
            "\n",
            "         [[ 2.1283e-02,  1.3489e-02,  3.7753e-04],\n",
            "          [-3.2695e-02,  2.1647e-02, -7.6396e-03],\n",
            "          [-1.2643e-02, -3.3591e-02, -2.2501e-02]],\n",
            "\n",
            "         [[ 2.7703e-03,  1.0814e-02, -1.3784e-02],\n",
            "          [ 3.1234e-02,  9.4862e-03, -2.3197e-02],\n",
            "          [-1.7959e-02, -3.9530e-02, -3.3960e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.2728e-02, -3.2200e-02, -4.0592e-02],\n",
            "          [-2.6847e-02, -1.0617e-02,  1.9950e-02],\n",
            "          [-4.0580e-02, -1.9289e-02, -1.7786e-02]],\n",
            "\n",
            "         [[ 3.3329e-02,  3.6435e-02,  3.0664e-02],\n",
            "          [ 3.8426e-02,  4.8271e-03,  3.7567e-02],\n",
            "          [ 2.0650e-02,  1.5700e-02,  3.4344e-02]],\n",
            "\n",
            "         [[-1.4099e-02,  3.1830e-02,  2.0700e-02],\n",
            "          [ 1.4313e-02, -3.0187e-02,  3.7834e-02],\n",
            "          [ 1.4758e-02,  2.8110e-02,  2.1674e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.1996e-02, -2.3614e-02, -2.8718e-02],\n",
            "          [ 1.1023e-02,  3.1554e-02, -1.8433e-02],\n",
            "          [ 9.4037e-03, -2.7395e-03, -3.8592e-02]],\n",
            "\n",
            "         [[-3.9353e-02,  1.6781e-02,  1.9607e-02],\n",
            "          [ 1.1022e-02,  2.5836e-02, -2.6190e-02],\n",
            "          [ 7.3778e-04, -4.3795e-03, -4.0791e-02]],\n",
            "\n",
            "         [[ 2.8989e-03,  2.5238e-02, -1.9186e-02],\n",
            "          [ 4.0651e-02,  2.2085e-02,  1.8422e-02],\n",
            "          [-3.6972e-03, -1.8189e-02,  7.2231e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.9173e-02,  3.4628e-02, -7.4682e-03],\n",
            "          [ 2.3930e-02, -1.2837e-02, -1.0575e-02],\n",
            "          [-2.0783e-02,  5.8234e-03, -2.4170e-02]],\n",
            "\n",
            "         [[-4.0537e-03, -7.8612e-03, -2.1826e-02],\n",
            "          [ 9.6693e-03, -4.0850e-02, -2.9677e-02],\n",
            "          [-5.2926e-03,  3.6731e-02,  3.3701e-02]],\n",
            "\n",
            "         [[ 1.4566e-02, -1.3287e-03, -2.6809e-02],\n",
            "          [-3.1466e-02, -9.1707e-03,  1.6904e-02],\n",
            "          [ 2.0809e-02, -2.7789e-03, -2.5178e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.5481e-02,  5.9936e-03, -2.0217e-02],\n",
            "          [ 2.8088e-02,  3.6297e-02,  2.0679e-02],\n",
            "          [-9.1550e-03, -3.4913e-02,  3.6179e-04]],\n",
            "\n",
            "         [[-1.0259e-02, -9.1384e-03,  6.0563e-03],\n",
            "          [-9.1801e-04, -3.6729e-03, -1.1379e-02],\n",
            "          [ 1.0903e-02, -4.5910e-03, -3.8480e-02]],\n",
            "\n",
            "         [[-3.3089e-02, -1.8743e-02,  1.4444e-02],\n",
            "          [ 2.3021e-02,  2.4650e-02,  4.9097e-03],\n",
            "          [ 1.9988e-02, -1.7949e-02,  1.9873e-05]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.4252e-02,  8.8538e-03, -2.3385e-02],\n",
            "          [ 3.6371e-03,  1.7729e-02, -1.2878e-02],\n",
            "          [-1.1424e-03, -2.1906e-02,  1.4050e-02]],\n",
            "\n",
            "         [[ 4.0424e-02, -2.6381e-02, -1.2884e-02],\n",
            "          [ 2.9701e-02,  2.2360e-02,  3.2802e-02],\n",
            "          [ 1.6789e-02,  2.3137e-02,  3.4536e-03]],\n",
            "\n",
            "         [[-2.1963e-02, -3.2222e-02,  1.4629e-03],\n",
            "          [-3.1903e-02,  3.0015e-02,  2.9056e-02],\n",
            "          [-2.5362e-02,  7.8096e-03,  4.0350e-02]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "N2_C3.bias Parameter containing:\n",
            "tensor([ 3.7400e-02,  2.2330e-02,  7.5857e-03,  3.4346e-02,  3.8973e-02,\n",
            "        -4.0468e-02,  2.8967e-02, -3.3560e-02, -5.2724e-03,  3.2445e-02,\n",
            "        -1.2989e-02,  3.6399e-03,  2.5010e-03,  3.8952e-02,  3.1423e-02,\n",
            "        -1.9982e-03, -4.0373e-02,  2.9264e-02,  2.2828e-03, -2.1672e-02,\n",
            "         3.4702e-02, -3.7287e-02,  3.0474e-02,  8.0284e-03, -3.0461e-02,\n",
            "         2.8217e-02, -3.7426e-02, -2.5867e-02,  3.4013e-02, -3.7819e-02,\n",
            "         8.1956e-06,  3.6788e-02,  1.0747e-02,  3.6385e-02,  7.9151e-03,\n",
            "         3.2359e-02,  3.4644e-02,  2.2197e-02,  2.1669e-03, -9.2013e-03,\n",
            "         1.3430e-02,  3.1674e-02,  2.3730e-02, -2.9902e-03, -2.1184e-02,\n",
            "         5.2862e-03, -1.5511e-02,  1.8235e-02,  1.6588e-02, -3.0538e-02,\n",
            "        -3.0428e-02,  4.7907e-04,  4.0648e-03, -1.9085e-02, -1.6336e-02,\n",
            "         2.6100e-02,  3.4864e-02,  2.9851e-02,  2.7967e-02,  3.1120e-02,\n",
            "         1.9592e-02, -2.1125e-03, -3.7226e-02, -3.2400e-02, -3.9092e-02,\n",
            "        -5.3194e-03, -9.4866e-03, -2.2082e-02, -6.7455e-03,  2.3775e-02,\n",
            "        -4.1426e-02,  6.2402e-03, -6.6530e-03, -4.1000e-02,  3.7361e-02,\n",
            "         3.1407e-02, -2.5361e-02, -3.0789e-03,  2.1527e-02,  5.3238e-03,\n",
            "         8.0929e-03,  3.6402e-02, -8.3920e-04, -1.0485e-02, -9.5819e-03,\n",
            "         2.1498e-03,  3.9272e-02, -3.3492e-02, -1.7910e-02, -4.0119e-02,\n",
            "        -2.9607e-03,  3.2599e-02, -4.0779e-03, -9.3085e-03, -1.0358e-02,\n",
            "         7.1023e-03, -2.9775e-02,  3.9179e-02,  2.4351e-02, -2.2705e-02,\n",
            "        -3.9021e-02, -1.9963e-02,  9.8012e-03,  3.0689e-02, -3.3135e-04,\n",
            "         3.4110e-02, -3.9835e-02,  1.4346e-02,  1.1599e-02,  9.7044e-03,\n",
            "        -1.7031e-02, -1.9734e-02, -2.0406e-02, -7.3752e-03, -1.2504e-02,\n",
            "        -3.0380e-02, -2.9908e-02, -1.9683e-02, -5.5387e-04, -4.0003e-02,\n",
            "         2.9565e-02,  2.2016e-02, -3.8433e-03,  9.5005e-03,  1.2060e-02,\n",
            "         3.8807e-02, -2.4303e-02,  3.9190e-02], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "N2_C4.weight Parameter containing:\n",
            "tensor([[[[-0.0123, -0.0256, -0.0132],\n",
            "          [ 0.0006,  0.0058, -0.0254],\n",
            "          [ 0.0134,  0.0024, -0.0269]],\n",
            "\n",
            "         [[ 0.0112, -0.0080, -0.0101],\n",
            "          [-0.0286, -0.0269, -0.0004],\n",
            "          [-0.0086, -0.0230,  0.0020]],\n",
            "\n",
            "         [[ 0.0177,  0.0237,  0.0150],\n",
            "          [-0.0108, -0.0179, -0.0261],\n",
            "          [ 0.0251,  0.0068, -0.0251]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0028, -0.0267, -0.0092],\n",
            "          [ 0.0204,  0.0116, -0.0184],\n",
            "          [-0.0154, -0.0079,  0.0020]],\n",
            "\n",
            "         [[-0.0196,  0.0285,  0.0185],\n",
            "          [ 0.0281, -0.0182,  0.0262],\n",
            "          [-0.0250, -0.0072,  0.0190]],\n",
            "\n",
            "         [[-0.0112, -0.0157,  0.0097],\n",
            "          [-0.0165, -0.0288, -0.0025],\n",
            "          [ 0.0106,  0.0208, -0.0142]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0132,  0.0248,  0.0228],\n",
            "          [-0.0069,  0.0004,  0.0280],\n",
            "          [ 0.0016, -0.0046, -0.0027]],\n",
            "\n",
            "         [[-0.0086,  0.0073,  0.0138],\n",
            "          [ 0.0044, -0.0074,  0.0090],\n",
            "          [ 0.0038, -0.0275, -0.0195]],\n",
            "\n",
            "         [[-0.0029, -0.0217, -0.0283],\n",
            "          [-0.0005, -0.0090,  0.0093],\n",
            "          [ 0.0159, -0.0130,  0.0213]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0212,  0.0048,  0.0194],\n",
            "          [ 0.0169, -0.0176, -0.0132],\n",
            "          [ 0.0110, -0.0283,  0.0065]],\n",
            "\n",
            "         [[ 0.0101, -0.0212, -0.0145],\n",
            "          [-0.0089, -0.0030,  0.0262],\n",
            "          [-0.0269,  0.0275, -0.0160]],\n",
            "\n",
            "         [[-0.0055, -0.0276,  0.0283],\n",
            "          [-0.0260,  0.0154,  0.0182],\n",
            "          [ 0.0159, -0.0104, -0.0220]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0291,  0.0051,  0.0119],\n",
            "          [-0.0150,  0.0261,  0.0294],\n",
            "          [ 0.0093, -0.0274, -0.0275]],\n",
            "\n",
            "         [[ 0.0067,  0.0043,  0.0049],\n",
            "          [-0.0109, -0.0052, -0.0043],\n",
            "          [ 0.0064, -0.0229, -0.0236]],\n",
            "\n",
            "         [[-0.0173, -0.0189, -0.0208],\n",
            "          [ 0.0218,  0.0163, -0.0288],\n",
            "          [ 0.0234,  0.0290,  0.0110]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0220, -0.0007,  0.0010],\n",
            "          [ 0.0251,  0.0193, -0.0206],\n",
            "          [-0.0208, -0.0013,  0.0214]],\n",
            "\n",
            "         [[ 0.0069,  0.0165,  0.0123],\n",
            "          [-0.0003,  0.0126,  0.0037],\n",
            "          [ 0.0244, -0.0042, -0.0063]],\n",
            "\n",
            "         [[-0.0141, -0.0149,  0.0205],\n",
            "          [ 0.0007,  0.0048,  0.0216],\n",
            "          [-0.0199, -0.0218,  0.0045]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0254,  0.0215, -0.0143],\n",
            "          [ 0.0222,  0.0107, -0.0261],\n",
            "          [-0.0097, -0.0039, -0.0041]],\n",
            "\n",
            "         [[ 0.0241,  0.0148,  0.0264],\n",
            "          [-0.0200, -0.0274, -0.0278],\n",
            "          [-0.0156,  0.0270,  0.0015]],\n",
            "\n",
            "         [[-0.0120, -0.0057, -0.0221],\n",
            "          [ 0.0137,  0.0012,  0.0080],\n",
            "          [-0.0156, -0.0275, -0.0173]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0091, -0.0086,  0.0179],\n",
            "          [ 0.0002,  0.0088,  0.0240],\n",
            "          [-0.0017,  0.0212,  0.0238]],\n",
            "\n",
            "         [[ 0.0087, -0.0252,  0.0164],\n",
            "          [-0.0289,  0.0145,  0.0115],\n",
            "          [-0.0166, -0.0276, -0.0145]],\n",
            "\n",
            "         [[-0.0011, -0.0058, -0.0235],\n",
            "          [-0.0238, -0.0100,  0.0112],\n",
            "          [ 0.0234, -0.0013, -0.0017]]],\n",
            "\n",
            "\n",
            "        [[[-0.0070, -0.0172, -0.0121],\n",
            "          [-0.0146,  0.0003,  0.0068],\n",
            "          [ 0.0131, -0.0281, -0.0255]],\n",
            "\n",
            "         [[-0.0256, -0.0018,  0.0129],\n",
            "          [ 0.0062,  0.0259,  0.0217],\n",
            "          [ 0.0043, -0.0108, -0.0230]],\n",
            "\n",
            "         [[-0.0182,  0.0052,  0.0092],\n",
            "          [-0.0260,  0.0060,  0.0087],\n",
            "          [ 0.0277,  0.0063,  0.0262]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0169,  0.0276,  0.0005],\n",
            "          [-0.0203,  0.0086, -0.0235],\n",
            "          [ 0.0049, -0.0267, -0.0098]],\n",
            "\n",
            "         [[ 0.0248, -0.0215,  0.0198],\n",
            "          [-0.0053,  0.0290, -0.0121],\n",
            "          [ 0.0047,  0.0016,  0.0055]],\n",
            "\n",
            "         [[ 0.0159, -0.0182, -0.0226],\n",
            "          [-0.0151, -0.0118,  0.0248],\n",
            "          [ 0.0136,  0.0263, -0.0187]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0277, -0.0124, -0.0089],\n",
            "          [-0.0212, -0.0148, -0.0264],\n",
            "          [-0.0262, -0.0131,  0.0045]],\n",
            "\n",
            "         [[-0.0113,  0.0064,  0.0107],\n",
            "          [ 0.0240,  0.0064, -0.0186],\n",
            "          [-0.0167, -0.0234,  0.0186]],\n",
            "\n",
            "         [[-0.0188,  0.0277,  0.0084],\n",
            "          [-0.0035,  0.0226, -0.0091],\n",
            "          [ 0.0229,  0.0011,  0.0212]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0144,  0.0088, -0.0265],\n",
            "          [ 0.0235,  0.0219,  0.0281],\n",
            "          [-0.0221,  0.0220, -0.0098]],\n",
            "\n",
            "         [[ 0.0284, -0.0241,  0.0250],\n",
            "          [-0.0256, -0.0168, -0.0146],\n",
            "          [-0.0126, -0.0082,  0.0070]],\n",
            "\n",
            "         [[ 0.0241,  0.0044,  0.0041],\n",
            "          [ 0.0164,  0.0130, -0.0264],\n",
            "          [ 0.0294,  0.0289,  0.0028]]]], device='cuda:0', requires_grad=True)\n",
            "N2_C4.bias Parameter containing:\n",
            "tensor([ 0.0288,  0.0054, -0.0187, -0.0147, -0.0242,  0.0225, -0.0208,  0.0119,\n",
            "        -0.0012,  0.0060, -0.0022,  0.0130, -0.0086,  0.0030,  0.0022, -0.0225,\n",
            "        -0.0232,  0.0166,  0.0293, -0.0019, -0.0012, -0.0044,  0.0284,  0.0247,\n",
            "        -0.0110, -0.0165,  0.0211, -0.0185,  0.0265,  0.0156, -0.0259,  0.0095,\n",
            "         0.0135,  0.0124, -0.0256,  0.0159,  0.0080, -0.0238, -0.0045,  0.0251,\n",
            "        -0.0210, -0.0006,  0.0073,  0.0241,  0.0091, -0.0026, -0.0205, -0.0191,\n",
            "        -0.0026,  0.0126,  0.0052, -0.0057,  0.0050,  0.0037, -0.0063,  0.0219,\n",
            "        -0.0148,  0.0013,  0.0010,  0.0037, -0.0193,  0.0283, -0.0171,  0.0025,\n",
            "         0.0049,  0.0252,  0.0200,  0.0150, -0.0199,  0.0078,  0.0280,  0.0142,\n",
            "         0.0243,  0.0162,  0.0140,  0.0177, -0.0006,  0.0010,  0.0258,  0.0150,\n",
            "         0.0096, -0.0178, -0.0137, -0.0227,  0.0044, -0.0003, -0.0134,  0.0005,\n",
            "         0.0261, -0.0287,  0.0122, -0.0184, -0.0180,  0.0064,  0.0108,  0.0250,\n",
            "        -0.0089,  0.0047,  0.0245, -0.0049, -0.0156,  0.0221, -0.0214, -0.0277,\n",
            "         0.0281, -0.0082,  0.0172,  0.0211, -0.0116, -0.0221,  0.0011,  0.0271,\n",
            "        -0.0081, -0.0085, -0.0094, -0.0081,  0.0286, -0.0142, -0.0270,  0.0131,\n",
            "        -0.0039,  0.0069,  0.0065, -0.0063, -0.0141,  0.0204,  0.0224, -0.0151],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "N2_D1.weight Parameter containing:\n",
            "tensor([[ 2.4536e-03,  2.2222e-03,  8.0004e-05,  ..., -1.7189e-03,\n",
            "          3.7672e-04,  6.4686e-04],\n",
            "        [-1.8015e-03,  1.6396e-03,  1.1643e-04,  ..., -2.1851e-03,\n",
            "          2.7415e-03,  2.4166e-03],\n",
            "        [-6.1342e-04,  1.8467e-03,  3.8234e-04,  ..., -2.7104e-03,\n",
            "         -6.6150e-04, -2.2750e-03],\n",
            "        ...,\n",
            "        [-1.3594e-03,  2.2981e-03,  2.3157e-03,  ...,  2.4002e-03,\n",
            "         -1.6155e-03,  1.6008e-03],\n",
            "        [ 1.6782e-03,  2.3875e-03,  2.1182e-03,  ..., -1.0080e-03,\n",
            "          1.4187e-04,  2.0821e-03],\n",
            "        [-9.8886e-04, -4.9304e-04,  2.0161e-03,  ...,  2.7328e-03,\n",
            "          1.9217e-03, -3.7604e-04]], device='cuda:0', requires_grad=True)\n",
            "N2_D1.bias Parameter containing:\n",
            "tensor([ 2.0141e-03,  1.3051e-03, -6.5517e-04, -1.0302e-03, -2.1088e-03,\n",
            "        -2.6833e-03, -2.6688e-03, -1.7081e-03, -2.2935e-03, -6.1256e-04,\n",
            "        -2.4479e-03, -1.3640e-03,  2.1083e-03,  2.7320e-03, -4.0296e-05,\n",
            "         2.0057e-03,  2.3247e-03,  2.1355e-04, -1.3956e-03, -1.5677e-03,\n",
            "         2.3106e-03, -1.7177e-04, -6.0970e-04, -1.4758e-03,  2.5509e-03,\n",
            "         2.3712e-03, -1.3542e-03,  2.6383e-03, -1.9184e-03,  3.3045e-04,\n",
            "        -2.6471e-03,  2.4566e-03,  1.7707e-03,  1.0866e-03, -1.2713e-03,\n",
            "        -3.4732e-04,  8.8777e-04, -1.7947e-04,  2.6929e-03,  2.4575e-03,\n",
            "        -1.1955e-03, -1.0550e-03, -2.6678e-03, -5.6417e-05,  2.1283e-03,\n",
            "        -2.2932e-03,  2.3626e-03,  1.3360e-03,  6.1900e-04,  5.0774e-04,\n",
            "        -7.3697e-05, -2.3220e-03,  1.5369e-03,  4.5875e-04, -1.8032e-03,\n",
            "        -1.7618e-04,  1.6662e-03, -2.4716e-03,  5.1413e-04, -1.4701e-03,\n",
            "         5.3089e-04, -2.5814e-03,  2.5969e-03,  2.4294e-04, -1.5948e-03,\n",
            "        -4.7022e-05,  2.4837e-03,  1.8827e-03, -2.7229e-03, -1.6333e-03,\n",
            "        -1.2172e-03, -8.1095e-04,  2.1233e-03, -1.7493e-03, -1.0726e-04,\n",
            "         1.8228e-03,  1.1752e-03, -2.1465e-03, -1.4527e-03,  6.9293e-04,\n",
            "         2.2636e-03,  6.6714e-04,  2.2816e-03,  2.7560e-03,  1.0259e-03,\n",
            "         2.7445e-03, -6.8191e-04, -1.3280e-03,  3.5246e-04,  8.7752e-04,\n",
            "         4.1938e-04, -4.3437e-04, -2.5514e-03, -9.4640e-04,  9.1219e-04,\n",
            "         4.6513e-04,  2.4664e-03,  1.7291e-03, -2.4677e-03, -2.9188e-05,\n",
            "         2.2228e-03,  2.6860e-03,  2.1353e-04,  7.6689e-04,  6.5679e-05,\n",
            "         1.4407e-03,  2.6844e-03, -2.5244e-03,  2.7194e-03,  4.9753e-04,\n",
            "        -1.1694e-03,  1.0155e-03, -1.2513e-03,  4.3793e-04, -2.3984e-04,\n",
            "         1.2339e-03,  1.1677e-03, -2.5666e-03,  7.3597e-05, -4.0814e-04,\n",
            "        -7.5760e-04,  9.2105e-04, -3.0272e-04,  3.1720e-04,  1.1164e-03,\n",
            "         7.4154e-04,  1.1039e-03,  2.2778e-04, -5.0626e-04,  9.9434e-04,\n",
            "         4.2273e-04, -2.2524e-04,  1.5244e-04,  1.4243e-03, -2.2416e-03,\n",
            "         2.3328e-03,  1.7140e-03,  2.2082e-03, -2.6811e-03, -9.6654e-04,\n",
            "         1.7576e-03, -2.7598e-04,  1.0345e-03,  1.4120e-03, -5.5726e-04,\n",
            "        -8.9680e-04,  8.2907e-04,  1.8753e-03, -8.0066e-04,  1.0474e-03,\n",
            "        -1.0118e-03,  1.5999e-03, -6.0197e-04, -2.0183e-03, -2.7921e-04,\n",
            "        -6.0939e-04, -1.4124e-04,  2.3940e-03,  6.3232e-04, -1.7456e-03,\n",
            "         2.6540e-03,  4.2652e-04, -1.8505e-03,  1.6481e-03, -2.2308e-03,\n",
            "        -2.3387e-03, -4.8033e-05, -2.2836e-03,  1.8273e-03, -2.2781e-04,\n",
            "         1.8837e-03, -6.0818e-04,  5.6497e-04,  1.4428e-03,  1.2082e-03,\n",
            "        -8.8641e-04, -2.6025e-03,  3.5849e-04, -6.0860e-04, -1.0280e-03,\n",
            "        -6.3791e-04, -2.6645e-04,  1.5479e-03,  1.4573e-03,  1.8856e-03,\n",
            "        -1.4610e-03, -1.4303e-03,  2.2273e-04, -4.9616e-04, -1.5390e-03,\n",
            "         1.3128e-03,  2.3298e-03,  2.6536e-04, -2.0736e-03,  1.7052e-03,\n",
            "        -2.1047e-03,  1.5212e-03,  1.8618e-03, -6.9653e-04, -1.2382e-03,\n",
            "        -7.0889e-04,  2.6492e-03,  6.3019e-04,  7.2117e-04, -1.0767e-03,\n",
            "         1.7747e-03,  1.9380e-03, -6.6208e-04,  1.7717e-03,  2.1400e-03,\n",
            "        -7.5901e-04,  2.0075e-03, -2.0101e-03, -1.0262e-03,  1.3106e-03,\n",
            "         7.1158e-04,  1.9428e-03,  1.6448e-03, -1.1298e-03,  3.9570e-05,\n",
            "         2.0196e-04,  8.3856e-04,  2.0556e-03, -1.1921e-04, -1.1479e-03,\n",
            "         1.1449e-03,  1.3362e-03, -4.6807e-04,  1.1662e-03, -7.9076e-04,\n",
            "         7.0761e-04, -1.5940e-03,  2.5693e-03, -1.2726e-03,  1.9806e-03,\n",
            "         1.3790e-03, -9.9193e-04,  7.5753e-04,  1.9252e-03, -8.0542e-04,\n",
            "        -8.8500e-04,  1.4393e-03, -1.2260e-03,  4.9930e-04, -1.3808e-03,\n",
            "         1.7437e-03,  1.2999e-03,  1.9781e-03,  7.0358e-04,  2.4743e-03,\n",
            "        -1.8291e-04,  2.5422e-03,  8.2705e-05, -6.1309e-05, -2.7169e-04,\n",
            "         1.4140e-03], device='cuda:0', requires_grad=True)\n",
            "N2_D2.weight Parameter containing:\n",
            "tensor([[ 0.0560,  0.0379, -0.0072,  ...,  0.0560, -0.0076, -0.0552],\n",
            "        [ 0.0157,  0.0608,  0.0539,  ..., -0.0485,  0.0403, -0.0390],\n",
            "        [-0.0145, -0.0561,  0.0016,  ..., -0.0375, -0.0612, -0.0181],\n",
            "        ...,\n",
            "        [ 0.0303, -0.0399, -0.0010,  ...,  0.0076, -0.0044,  0.0545],\n",
            "        [-0.0398, -0.0180,  0.0341,  ..., -0.0530,  0.0032, -0.0227],\n",
            "        [-0.0377,  0.0466, -0.0588,  ...,  0.0597,  0.0032,  0.0589]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "N2_D2.bias Parameter containing:\n",
            "tensor([-0.0480, -0.0466, -0.0068, -0.0081,  0.0177, -0.0155, -0.0186, -0.0124,\n",
            "         0.0274,  0.0535,  0.0375,  0.0354,  0.0338, -0.0126, -0.0199, -0.0527,\n",
            "         0.0417, -0.0169,  0.0048, -0.0329,  0.0395, -0.0418, -0.0484, -0.0260,\n",
            "         0.0508, -0.0153, -0.0352,  0.0026,  0.0024, -0.0037, -0.0121,  0.0247,\n",
            "        -0.0287, -0.0563,  0.0319, -0.0407, -0.0492, -0.0599, -0.0420, -0.0560,\n",
            "         0.0260, -0.0597,  0.0106, -0.0206, -0.0307,  0.0299,  0.0609,  0.0479,\n",
            "         0.0096,  0.0403, -0.0473, -0.0423, -0.0247, -0.0045,  0.0174,  0.0200,\n",
            "         0.0508, -0.0587, -0.0041, -0.0142,  0.0328,  0.0448,  0.0136,  0.0136,\n",
            "         0.0528,  0.0278, -0.0288,  0.0610, -0.0486, -0.0120, -0.0565,  0.0028,\n",
            "         0.0286,  0.0247, -0.0421, -0.0078, -0.0262,  0.0070,  0.0170,  0.0115,\n",
            "         0.0598,  0.0228, -0.0368, -0.0302, -0.0014, -0.0526, -0.0446,  0.0242,\n",
            "        -0.0610, -0.0543, -0.0038,  0.0490, -0.0093, -0.0592, -0.0224,  0.0225,\n",
            "         0.0416,  0.0604, -0.0389, -0.0388, -0.0053, -0.0120, -0.0625,  0.0142,\n",
            "         0.0366,  0.0186, -0.0034, -0.0454,  0.0576, -0.0376,  0.0443, -0.0338,\n",
            "        -0.0111,  0.0219,  0.0078,  0.0119, -0.0419, -0.0233,  0.0107,  0.0178,\n",
            "         0.0353,  0.0570,  0.0486, -0.0541, -0.0051, -0.0580,  0.0294,  0.0004,\n",
            "         0.0140,  0.0449, -0.0152,  0.0232,  0.0114,  0.0028,  0.0020, -0.0545,\n",
            "         0.0606, -0.0344,  0.0500,  0.0211, -0.0226,  0.0260, -0.0107, -0.0443,\n",
            "        -0.0186,  0.0612,  0.0616,  0.0128, -0.0012, -0.0181,  0.0622,  0.0419,\n",
            "        -0.0114, -0.0464,  0.0468,  0.0624,  0.0419, -0.0208,  0.0292,  0.0303,\n",
            "        -0.0524, -0.0519,  0.0203, -0.0521,  0.0292,  0.0022, -0.0380, -0.0024,\n",
            "         0.0298,  0.0314,  0.0469, -0.0004, -0.0355, -0.0447,  0.0613, -0.0205,\n",
            "        -0.0611,  0.0362,  0.0118,  0.0235, -0.0325,  0.0214,  0.0039, -0.0460,\n",
            "         0.0464,  0.0173,  0.0044,  0.0332, -0.0172,  0.0617,  0.0513, -0.0046,\n",
            "         0.0442,  0.0138, -0.0600, -0.0043, -0.0196, -0.0090,  0.0470,  0.0496,\n",
            "        -0.0483,  0.0318, -0.0517,  0.0611,  0.0228,  0.0590, -0.0451,  0.0204,\n",
            "        -0.0561,  0.0028,  0.0416,  0.0016, -0.0334, -0.0067,  0.0430,  0.0122,\n",
            "        -0.0395,  0.0546, -0.0515, -0.0262,  0.0260,  0.0325, -0.0255, -0.0010,\n",
            "        -0.0189,  0.0121,  0.0125, -0.0252,  0.0313, -0.0288, -0.0300,  0.0331,\n",
            "        -0.0205,  0.0189, -0.0592, -0.0590,  0.0432, -0.0043,  0.0465, -0.0028,\n",
            "        -0.0105,  0.0540,  0.0401,  0.0143, -0.0442, -0.0210,  0.0101,  0.0566,\n",
            "         0.0622,  0.0219,  0.0323,  0.0417, -0.0377, -0.0584,  0.0359,  0.0025],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "outputs.weight Parameter containing:\n",
            "tensor([[-0.0611, -0.0595,  0.0130,  ...,  0.0593,  0.0199, -0.0611],\n",
            "        [-0.0308, -0.0124, -0.0070,  ...,  0.0618, -0.0509, -0.0332],\n",
            "        [-0.0554,  0.0586,  0.0396,  ...,  0.0471,  0.0182, -0.0520],\n",
            "        ...,\n",
            "        [-0.0335, -0.0299,  0.0330,  ..., -0.0391,  0.0591,  0.0079],\n",
            "        [ 0.0488,  0.0556, -0.0508,  ...,  0.0161,  0.0378, -0.0610],\n",
            "        [ 0.0403, -0.0405,  0.0587,  ..., -0.0385, -0.0493,  0.0620]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "outputs.bias Parameter containing:\n",
            "tensor([ 0.0305,  0.0343, -0.0474,  0.0407, -0.0427, -0.0057,  0.0512, -0.0527,\n",
            "        -0.0526,  0.0618], device='cuda:0', requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.state_dict().get(\"N1_C1.weight\").data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50UY56F2STua",
        "outputId": "c600e1bf-c9a4-4bb2-d45e-f12346db82a2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-1.5751e-02,  1.8377e-01,  1.5067e-01],\n",
              "          [-5.2458e-02, -1.8347e-01,  1.5553e-01],\n",
              "          [ 8.5427e-02,  8.4333e-03, -1.1791e-01]],\n",
              "\n",
              "         [[ 9.3870e-02,  1.5532e-01,  1.0040e-01],\n",
              "          [-5.4865e-02,  7.3119e-02, -1.5564e-01],\n",
              "          [-8.2653e-02, -1.4993e-01, -1.4855e-01]],\n",
              "\n",
              "         [[-1.7436e-01, -3.4497e-02, -2.8740e-02],\n",
              "          [-1.0539e-01,  5.4372e-02, -9.3557e-02],\n",
              "          [-2.3659e-02,  1.7833e-01, -5.9122e-02]]],\n",
              "\n",
              "\n",
              "        [[[ 5.8111e-02,  1.2392e-01, -1.5378e-01],\n",
              "          [-9.8643e-03,  1.6460e-01,  4.6094e-02],\n",
              "          [-4.1165e-02, -7.9933e-02,  1.2361e-01]],\n",
              "\n",
              "         [[-9.4299e-02, -5.6796e-02, -9.6843e-02],\n",
              "          [ 1.5443e-01,  1.0551e-01, -1.2970e-01],\n",
              "          [-6.8913e-02, -1.8086e-01,  3.8319e-02]],\n",
              "\n",
              "         [[ 2.3538e-02,  1.7668e-01,  1.6701e-01],\n",
              "          [-8.1817e-02, -1.8101e-02, -1.2344e-01],\n",
              "          [ 3.7531e-02,  1.3823e-01,  8.9921e-02]]],\n",
              "\n",
              "\n",
              "        [[[ 1.8629e-01,  1.0086e-02,  1.5948e-02],\n",
              "          [-5.4252e-02,  3.4405e-02, -4.0503e-02],\n",
              "          [-1.1004e-01, -1.7539e-02, -9.1759e-02]],\n",
              "\n",
              "         [[ 5.6471e-02,  5.2879e-02,  4.6940e-02],\n",
              "          [ 1.6432e-01, -1.2782e-01,  8.7223e-02],\n",
              "          [ 1.4965e-01,  2.6719e-02,  1.2975e-01]],\n",
              "\n",
              "         [[ 1.4524e-01, -5.2260e-02,  8.3553e-02],\n",
              "          [-4.5893e-02,  7.5249e-05,  3.7214e-02],\n",
              "          [-1.0916e-01, -9.3558e-02,  1.6216e-01]]],\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "        [[[-5.0360e-02, -1.0091e-01, -1.3989e-01],\n",
              "          [-2.3886e-02, -1.0310e-01,  1.5216e-01],\n",
              "          [ 1.8302e-01, -1.4883e-01, -3.8753e-02]],\n",
              "\n",
              "         [[ 8.1115e-03,  1.2052e-02,  1.6082e-01],\n",
              "          [ 1.7090e-01,  8.1719e-02,  6.1602e-02],\n",
              "          [-1.1682e-01,  6.0915e-02,  1.6584e-01]],\n",
              "\n",
              "         [[-1.3601e-01,  2.6996e-02,  9.7761e-02],\n",
              "          [-3.4323e-02,  5.6999e-02,  1.6071e-01],\n",
              "          [-1.6614e-01,  2.8728e-03, -1.4690e-02]]],\n",
              "\n",
              "\n",
              "        [[[ 1.5961e-01,  1.9241e-01, -9.7117e-02],\n",
              "          [ 6.4183e-02,  1.7203e-01,  1.2531e-01],\n",
              "          [ 1.2804e-01,  1.2463e-01,  2.7679e-02]],\n",
              "\n",
              "         [[ 7.4301e-02,  3.0653e-04,  1.7253e-01],\n",
              "          [-1.5410e-01, -1.6842e-01, -1.9046e-01],\n",
              "          [ 5.8921e-02, -4.8850e-03,  1.6892e-01]],\n",
              "\n",
              "         [[-3.2244e-02, -8.9767e-02, -1.8330e-01],\n",
              "          [ 1.6410e-02,  6.0630e-02,  1.0509e-02],\n",
              "          [ 1.8865e-01,  1.4779e-01, -1.5674e-01]]],\n",
              "\n",
              "\n",
              "        [[[ 4.7032e-02,  3.8626e-02,  4.8792e-02],\n",
              "          [ 1.7731e-01, -9.3070e-02, -1.7046e-01],\n",
              "          [-1.5299e-01,  1.4744e-01,  1.3680e-01]],\n",
              "\n",
              "         [[-5.2710e-02,  7.1119e-02,  5.5679e-02],\n",
              "          [-2.7893e-02,  9.0514e-02, -5.5448e-02],\n",
              "          [ 8.0949e-02,  1.1803e-01, -1.2192e-01]],\n",
              "\n",
              "         [[ 1.4326e-02, -6.8382e-03,  7.7612e-03],\n",
              "          [-9.2268e-02, -3.3531e-02,  6.7799e-02],\n",
              "          [-1.7701e-01,  1.4952e-02, -1.7578e-01]]]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if name[0:2]=='N1':\n",
        "      paramName = \"N2\"+name[2:]\n",
        "      param.data = model.state_dict().get(paramName).data\n",
        "      param.requires_grad = False\n",
        "    print(name, param)"
      ],
      "metadata": {
        "id": "TZhztZWGSWip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9705be2-bc8d-4dc2-97cd-e329fe0414db"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N1_C1.weight Parameter containing:\n",
            "tensor([[[[ 0.1655, -0.1023, -0.0654],\n",
            "          [ 0.1320,  0.1459, -0.0293],\n",
            "          [ 0.1307, -0.0038, -0.1443]],\n",
            "\n",
            "         [[ 0.1255, -0.1350,  0.0066],\n",
            "          [-0.0885,  0.1231, -0.1065],\n",
            "          [-0.0700, -0.0737,  0.1228]],\n",
            "\n",
            "         [[ 0.1619,  0.1924,  0.1041],\n",
            "          [ 0.1877,  0.1497,  0.0791],\n",
            "          [-0.0548,  0.0440, -0.0788]]],\n",
            "\n",
            "\n",
            "        [[[-0.1344, -0.0201,  0.0196],\n",
            "          [-0.0519,  0.0757, -0.0161],\n",
            "          [-0.1060, -0.0630, -0.0215]],\n",
            "\n",
            "         [[ 0.0048, -0.1831,  0.1577],\n",
            "          [-0.1467,  0.1101,  0.1224],\n",
            "          [-0.0428,  0.1472, -0.1545]],\n",
            "\n",
            "         [[ 0.1256,  0.1142,  0.1272],\n",
            "          [-0.0127, -0.0286,  0.1315],\n",
            "          [ 0.1065,  0.0171,  0.1037]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0077, -0.0407, -0.0247],\n",
            "          [ 0.0596,  0.0894, -0.1481],\n",
            "          [ 0.1123, -0.1364, -0.1826]],\n",
            "\n",
            "         [[ 0.0936, -0.0706, -0.1452],\n",
            "          [ 0.1321, -0.0645,  0.0175],\n",
            "          [-0.0288,  0.1139,  0.0435]],\n",
            "\n",
            "         [[-0.1189, -0.1822,  0.1268],\n",
            "          [ 0.0598,  0.1577, -0.0791],\n",
            "          [-0.0528, -0.0397,  0.1527]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0128, -0.0542, -0.1227],\n",
            "          [ 0.0888,  0.0861, -0.1001],\n",
            "          [-0.0162, -0.1255, -0.1477]],\n",
            "\n",
            "         [[ 0.0792, -0.1393,  0.1429],\n",
            "          [ 0.0856, -0.1878,  0.1810],\n",
            "          [ 0.0429, -0.0350, -0.0249]],\n",
            "\n",
            "         [[ 0.1902,  0.1473, -0.1349],\n",
            "          [-0.1016,  0.0181,  0.0599],\n",
            "          [-0.0560, -0.0279, -0.1527]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0145, -0.1641,  0.1091],\n",
            "          [ 0.0806, -0.1125,  0.0487],\n",
            "          [ 0.0226,  0.0997, -0.0657]],\n",
            "\n",
            "         [[ 0.1541, -0.0840,  0.1102],\n",
            "          [-0.1436, -0.0688, -0.0991],\n",
            "          [-0.0961,  0.1541,  0.0594]],\n",
            "\n",
            "         [[ 0.0446,  0.1434,  0.0999],\n",
            "          [-0.0178,  0.1110, -0.0866],\n",
            "          [ 0.0818,  0.1547, -0.0135]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0570,  0.0230,  0.1441],\n",
            "          [ 0.1004,  0.1370, -0.1546],\n",
            "          [-0.1582,  0.1786,  0.1786]],\n",
            "\n",
            "         [[-0.0714, -0.1461, -0.0242],\n",
            "          [-0.1655,  0.0348,  0.0625],\n",
            "          [-0.0380, -0.0692,  0.1028]],\n",
            "\n",
            "         [[-0.0100,  0.1184,  0.0539],\n",
            "          [ 0.0535,  0.0673, -0.0018],\n",
            "          [-0.0840,  0.0528, -0.0486]]]], device='cuda:0')\n",
            "N1_C1.bias Parameter containing:\n",
            "tensor([ 0.0149,  0.1442,  0.0307,  0.1423,  0.0271,  0.0737, -0.0683,  0.0026,\n",
            "         0.1769,  0.0853, -0.0475,  0.0976,  0.1593, -0.0339,  0.0075, -0.1508,\n",
            "         0.1101, -0.0543, -0.0705,  0.1912,  0.0731,  0.1866, -0.0827,  0.1098,\n",
            "        -0.0151, -0.0588, -0.0326, -0.1420, -0.0818, -0.1791, -0.1114,  0.1399,\n",
            "        -0.0034,  0.0182,  0.1790,  0.0359,  0.0357,  0.0303,  0.0446,  0.0245,\n",
            "        -0.0075, -0.0333, -0.1813,  0.0199, -0.1550,  0.1054, -0.0339,  0.1648,\n",
            "        -0.1775,  0.0882,  0.0131,  0.1332, -0.1767,  0.1763, -0.0458, -0.0123,\n",
            "         0.0622, -0.0902, -0.0139,  0.0092, -0.1347, -0.1719,  0.1017, -0.1107],\n",
            "       device='cuda:0')\n",
            "N1_C2.weight Parameter containing:\n",
            "tensor([[[[-0.0389,  0.0170, -0.0208],\n",
            "          [-0.0350,  0.0032, -0.0136],\n",
            "          [ 0.0393,  0.0085,  0.0415]],\n",
            "\n",
            "         [[-0.0110,  0.0319, -0.0036],\n",
            "          [ 0.0129, -0.0233,  0.0194],\n",
            "          [-0.0002, -0.0313, -0.0224]],\n",
            "\n",
            "         [[ 0.0323, -0.0017,  0.0073],\n",
            "          [-0.0393, -0.0385,  0.0015],\n",
            "          [ 0.0160,  0.0142, -0.0150]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0226, -0.0023, -0.0325],\n",
            "          [-0.0067, -0.0313,  0.0050],\n",
            "          [-0.0100, -0.0309,  0.0306]],\n",
            "\n",
            "         [[ 0.0046, -0.0414,  0.0237],\n",
            "          [ 0.0085, -0.0149, -0.0062],\n",
            "          [ 0.0201, -0.0125, -0.0394]],\n",
            "\n",
            "         [[ 0.0250, -0.0098, -0.0378],\n",
            "          [-0.0013,  0.0068,  0.0200],\n",
            "          [-0.0110, -0.0193,  0.0377]]],\n",
            "\n",
            "\n",
            "        [[[-0.0164,  0.0298, -0.0226],\n",
            "          [-0.0053,  0.0150, -0.0170],\n",
            "          [ 0.0113, -0.0204,  0.0118]],\n",
            "\n",
            "         [[ 0.0299,  0.0228,  0.0225],\n",
            "          [ 0.0184,  0.0150,  0.0334],\n",
            "          [ 0.0132,  0.0409, -0.0052]],\n",
            "\n",
            "         [[-0.0277,  0.0085, -0.0128],\n",
            "          [ 0.0103,  0.0330,  0.0330],\n",
            "          [ 0.0278, -0.0142, -0.0263]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0029, -0.0032, -0.0070],\n",
            "          [-0.0194, -0.0160,  0.0381],\n",
            "          [ 0.0061, -0.0302,  0.0266]],\n",
            "\n",
            "         [[-0.0111,  0.0157, -0.0345],\n",
            "          [ 0.0276, -0.0384,  0.0203],\n",
            "          [ 0.0272, -0.0292, -0.0107]],\n",
            "\n",
            "         [[ 0.0149,  0.0013,  0.0218],\n",
            "          [ 0.0063,  0.0019, -0.0257],\n",
            "          [ 0.0194, -0.0030, -0.0356]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0145, -0.0224,  0.0394],\n",
            "          [-0.0002,  0.0065,  0.0179],\n",
            "          [-0.0270,  0.0035,  0.0280]],\n",
            "\n",
            "         [[-0.0197, -0.0115,  0.0117],\n",
            "          [-0.0013, -0.0271, -0.0352],\n",
            "          [ 0.0191, -0.0184,  0.0320]],\n",
            "\n",
            "         [[ 0.0158,  0.0251, -0.0025],\n",
            "          [-0.0406, -0.0224, -0.0075],\n",
            "          [-0.0168, -0.0329, -0.0249]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0058,  0.0263,  0.0394],\n",
            "          [ 0.0162,  0.0064, -0.0033],\n",
            "          [-0.0091,  0.0195, -0.0409]],\n",
            "\n",
            "         [[-0.0392, -0.0037,  0.0083],\n",
            "          [-0.0383,  0.0139,  0.0160],\n",
            "          [-0.0271, -0.0305, -0.0244]],\n",
            "\n",
            "         [[ 0.0059, -0.0162, -0.0148],\n",
            "          [-0.0089, -0.0217,  0.0171],\n",
            "          [-0.0296, -0.0173,  0.0333]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.0287, -0.0300, -0.0012],\n",
            "          [-0.0388, -0.0136, -0.0271],\n",
            "          [-0.0228, -0.0080,  0.0064]],\n",
            "\n",
            "         [[-0.0050,  0.0318, -0.0335],\n",
            "          [ 0.0124,  0.0028,  0.0271],\n",
            "          [ 0.0005, -0.0091, -0.0405]],\n",
            "\n",
            "         [[ 0.0098, -0.0109,  0.0025],\n",
            "          [ 0.0098, -0.0340,  0.0115],\n",
            "          [ 0.0413,  0.0371, -0.0307]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0064,  0.0322,  0.0144],\n",
            "          [ 0.0067, -0.0266,  0.0162],\n",
            "          [-0.0416,  0.0024, -0.0378]],\n",
            "\n",
            "         [[ 0.0230,  0.0106, -0.0379],\n",
            "          [ 0.0189,  0.0054,  0.0410],\n",
            "          [ 0.0075,  0.0299, -0.0270]],\n",
            "\n",
            "         [[-0.0022,  0.0031, -0.0366],\n",
            "          [-0.0252,  0.0054,  0.0013],\n",
            "          [ 0.0150, -0.0282, -0.0341]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0172, -0.0369, -0.0054],\n",
            "          [ 0.0355,  0.0364, -0.0211],\n",
            "          [-0.0103,  0.0209, -0.0041]],\n",
            "\n",
            "         [[-0.0023,  0.0336, -0.0006],\n",
            "          [ 0.0347,  0.0007,  0.0207],\n",
            "          [ 0.0409, -0.0385,  0.0415]],\n",
            "\n",
            "         [[ 0.0261,  0.0291,  0.0019],\n",
            "          [ 0.0415,  0.0307,  0.0290],\n",
            "          [ 0.0298, -0.0058, -0.0374]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0367, -0.0008, -0.0172],\n",
            "          [-0.0336,  0.0351, -0.0410],\n",
            "          [-0.0164, -0.0321,  0.0103]],\n",
            "\n",
            "         [[ 0.0354,  0.0046,  0.0350],\n",
            "          [ 0.0161, -0.0114,  0.0070],\n",
            "          [-0.0070,  0.0252, -0.0335]],\n",
            "\n",
            "         [[ 0.0043,  0.0020,  0.0344],\n",
            "          [-0.0318,  0.0090,  0.0079],\n",
            "          [-0.0332, -0.0240,  0.0127]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0213,  0.0002,  0.0230],\n",
            "          [ 0.0220, -0.0171, -0.0318],\n",
            "          [-0.0267,  0.0074, -0.0368]],\n",
            "\n",
            "         [[ 0.0336, -0.0401,  0.0207],\n",
            "          [ 0.0137, -0.0208, -0.0291],\n",
            "          [-0.0146,  0.0050,  0.0302]],\n",
            "\n",
            "         [[ 0.0125,  0.0210,  0.0008],\n",
            "          [ 0.0089, -0.0084, -0.0107],\n",
            "          [ 0.0090,  0.0283,  0.0221]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0366,  0.0021, -0.0137],\n",
            "          [-0.0019,  0.0200, -0.0047],\n",
            "          [ 0.0043,  0.0349,  0.0014]],\n",
            "\n",
            "         [[ 0.0228, -0.0173,  0.0271],\n",
            "          [-0.0259,  0.0283, -0.0281],\n",
            "          [ 0.0036, -0.0072,  0.0374]],\n",
            "\n",
            "         [[-0.0078,  0.0276, -0.0326],\n",
            "          [-0.0349,  0.0222,  0.0149],\n",
            "          [-0.0135,  0.0273,  0.0118]]]], device='cuda:0')\n",
            "N1_C2.bias Parameter containing:\n",
            "tensor([-0.0362, -0.0107,  0.0200, -0.0066,  0.0123, -0.0369,  0.0246, -0.0410,\n",
            "        -0.0131,  0.0193, -0.0004, -0.0232, -0.0260,  0.0127, -0.0149, -0.0141,\n",
            "         0.0182, -0.0347, -0.0321, -0.0086, -0.0108, -0.0146, -0.0298,  0.0286,\n",
            "        -0.0156,  0.0028,  0.0255, -0.0264,  0.0237,  0.0093,  0.0390, -0.0025,\n",
            "         0.0050,  0.0258, -0.0268,  0.0300,  0.0208,  0.0199,  0.0124, -0.0149,\n",
            "         0.0063,  0.0361,  0.0230,  0.0362,  0.0387,  0.0081,  0.0069, -0.0302,\n",
            "         0.0285,  0.0226,  0.0228,  0.0167, -0.0019, -0.0169,  0.0065, -0.0353,\n",
            "        -0.0096,  0.0106,  0.0151,  0.0279,  0.0012, -0.0284, -0.0333,  0.0218],\n",
            "       device='cuda:0')\n",
            "N1_C3.weight Parameter containing:\n",
            "tensor([[[[ 1.6478e-02,  3.6358e-02, -2.4660e-03],\n",
            "          [ 1.4462e-02,  7.8154e-03, -8.7056e-03],\n",
            "          [ 2.8477e-02,  8.8503e-03, -3.6299e-02]],\n",
            "\n",
            "         [[ 3.2571e-02,  1.0216e-03, -3.5548e-02],\n",
            "          [ 2.6030e-03,  7.9593e-03,  2.7901e-02],\n",
            "          [ 3.2491e-02,  2.8091e-04, -2.6749e-02]],\n",
            "\n",
            "         [[-3.7332e-02,  2.2772e-02, -2.1687e-02],\n",
            "          [ 8.6482e-03, -2.7712e-02, -1.8260e-02],\n",
            "          [-1.8952e-02,  4.1075e-02,  3.1533e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.3446e-02,  6.7809e-04,  1.0093e-02],\n",
            "          [-5.1189e-03, -4.0486e-02,  3.2720e-02],\n",
            "          [-1.2961e-02,  5.4246e-03,  3.0900e-02]],\n",
            "\n",
            "         [[ 8.2329e-03, -3.8339e-02, -2.4085e-02],\n",
            "          [ 1.4021e-02, -3.5651e-02,  3.3019e-02],\n",
            "          [ 2.1793e-02, -3.8337e-02,  3.4147e-02]],\n",
            "\n",
            "         [[-2.4871e-03, -3.0027e-02, -5.4283e-03],\n",
            "          [-2.2348e-02, -1.2380e-02,  1.6490e-02],\n",
            "          [-2.1183e-02,  1.9155e-02,  1.4722e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.3230e-02,  2.0484e-02, -2.0927e-02],\n",
            "          [-1.5153e-02,  3.0153e-02, -2.5913e-02],\n",
            "          [ 3.3377e-02,  2.9313e-02,  1.6354e-03]],\n",
            "\n",
            "         [[-2.4203e-02,  3.5340e-02,  1.8368e-02],\n",
            "          [-3.4976e-02, -2.2532e-02, -2.5597e-02],\n",
            "          [-1.5514e-02, -1.7802e-02, -2.9605e-02]],\n",
            "\n",
            "         [[ 3.6059e-02, -3.1973e-02, -4.0246e-02],\n",
            "          [ 1.3481e-04, -1.2641e-02, -2.5085e-02],\n",
            "          [ 1.0859e-02, -6.5656e-03,  3.9958e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.4300e-02, -3.3946e-02, -1.2755e-02],\n",
            "          [-5.5192e-03,  3.1808e-02, -4.6633e-03],\n",
            "          [-2.5712e-02,  4.7626e-03, -2.6625e-03]],\n",
            "\n",
            "         [[ 3.0555e-02, -2.1988e-02,  2.3051e-03],\n",
            "          [-2.6687e-02,  9.9558e-03,  3.4667e-02],\n",
            "          [ 4.1517e-02,  3.9132e-02, -1.3778e-02]],\n",
            "\n",
            "         [[-3.6121e-03, -8.3959e-03, -3.0312e-02],\n",
            "          [-7.9862e-03,  2.0249e-02, -3.5809e-02],\n",
            "          [ 9.6957e-04, -3.6163e-03,  2.3563e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.1184e-02,  3.8366e-02,  2.2504e-02],\n",
            "          [ 8.3281e-03, -4.1128e-02,  1.8214e-02],\n",
            "          [-2.6699e-02, -3.7198e-02,  7.4583e-03]],\n",
            "\n",
            "         [[-1.2468e-02, -3.3490e-02,  3.5394e-02],\n",
            "          [-3.2529e-02,  3.8864e-02, -3.7815e-02],\n",
            "          [-4.1045e-02, -2.5406e-02, -3.3510e-02]],\n",
            "\n",
            "         [[ 2.5755e-02,  7.6556e-03,  1.3890e-02],\n",
            "          [ 4.3359e-03,  3.8102e-02,  1.3977e-02],\n",
            "          [-1.1940e-02, -3.4510e-02,  3.2808e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.6105e-04, -3.8821e-02, -3.7192e-03],\n",
            "          [ 5.8216e-03, -3.1990e-02,  2.8387e-02],\n",
            "          [ 3.4090e-02, -2.2243e-03,  1.0163e-02]],\n",
            "\n",
            "         [[ 2.2208e-02,  4.9898e-03,  5.8362e-03],\n",
            "          [ 3.8052e-02,  2.7821e-02, -4.1032e-02],\n",
            "          [ 8.1057e-03, -3.0102e-02,  2.3647e-02]],\n",
            "\n",
            "         [[-1.7566e-02, -2.0587e-02,  2.4119e-02],\n",
            "          [ 3.1486e-02, -1.3044e-02,  2.0060e-02],\n",
            "          [-3.5179e-02, -2.5455e-02,  1.6286e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 2.6174e-02,  3.9409e-02,  9.9017e-03],\n",
            "          [-3.1703e-02, -1.9336e-02,  1.6662e-02],\n",
            "          [-2.1843e-02, -2.0828e-02,  3.2698e-02]],\n",
            "\n",
            "         [[ 2.1283e-02,  1.3489e-02,  3.7753e-04],\n",
            "          [-3.2695e-02,  2.1647e-02, -7.6396e-03],\n",
            "          [-1.2643e-02, -3.3591e-02, -2.2501e-02]],\n",
            "\n",
            "         [[ 2.7703e-03,  1.0814e-02, -1.3784e-02],\n",
            "          [ 3.1234e-02,  9.4862e-03, -2.3197e-02],\n",
            "          [-1.7959e-02, -3.9530e-02, -3.3960e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.2728e-02, -3.2200e-02, -4.0592e-02],\n",
            "          [-2.6847e-02, -1.0617e-02,  1.9950e-02],\n",
            "          [-4.0580e-02, -1.9289e-02, -1.7786e-02]],\n",
            "\n",
            "         [[ 3.3329e-02,  3.6435e-02,  3.0664e-02],\n",
            "          [ 3.8426e-02,  4.8271e-03,  3.7567e-02],\n",
            "          [ 2.0650e-02,  1.5700e-02,  3.4344e-02]],\n",
            "\n",
            "         [[-1.4099e-02,  3.1830e-02,  2.0700e-02],\n",
            "          [ 1.4313e-02, -3.0187e-02,  3.7834e-02],\n",
            "          [ 1.4758e-02,  2.8110e-02,  2.1674e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.1996e-02, -2.3614e-02, -2.8718e-02],\n",
            "          [ 1.1023e-02,  3.1554e-02, -1.8433e-02],\n",
            "          [ 9.4037e-03, -2.7395e-03, -3.8592e-02]],\n",
            "\n",
            "         [[-3.9353e-02,  1.6781e-02,  1.9607e-02],\n",
            "          [ 1.1022e-02,  2.5836e-02, -2.6190e-02],\n",
            "          [ 7.3778e-04, -4.3795e-03, -4.0791e-02]],\n",
            "\n",
            "         [[ 2.8989e-03,  2.5238e-02, -1.9186e-02],\n",
            "          [ 4.0651e-02,  2.2085e-02,  1.8422e-02],\n",
            "          [-3.6972e-03, -1.8189e-02,  7.2231e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.9173e-02,  3.4628e-02, -7.4682e-03],\n",
            "          [ 2.3930e-02, -1.2837e-02, -1.0575e-02],\n",
            "          [-2.0783e-02,  5.8234e-03, -2.4170e-02]],\n",
            "\n",
            "         [[-4.0537e-03, -7.8612e-03, -2.1826e-02],\n",
            "          [ 9.6693e-03, -4.0850e-02, -2.9677e-02],\n",
            "          [-5.2926e-03,  3.6731e-02,  3.3701e-02]],\n",
            "\n",
            "         [[ 1.4566e-02, -1.3287e-03, -2.6809e-02],\n",
            "          [-3.1466e-02, -9.1707e-03,  1.6904e-02],\n",
            "          [ 2.0809e-02, -2.7789e-03, -2.5178e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.5481e-02,  5.9936e-03, -2.0217e-02],\n",
            "          [ 2.8088e-02,  3.6297e-02,  2.0679e-02],\n",
            "          [-9.1550e-03, -3.4913e-02,  3.6179e-04]],\n",
            "\n",
            "         [[-1.0259e-02, -9.1384e-03,  6.0563e-03],\n",
            "          [-9.1801e-04, -3.6729e-03, -1.1379e-02],\n",
            "          [ 1.0903e-02, -4.5910e-03, -3.8480e-02]],\n",
            "\n",
            "         [[-3.3089e-02, -1.8743e-02,  1.4444e-02],\n",
            "          [ 2.3021e-02,  2.4650e-02,  4.9097e-03],\n",
            "          [ 1.9988e-02, -1.7949e-02,  1.9873e-05]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.4252e-02,  8.8538e-03, -2.3385e-02],\n",
            "          [ 3.6371e-03,  1.7729e-02, -1.2878e-02],\n",
            "          [-1.1424e-03, -2.1906e-02,  1.4050e-02]],\n",
            "\n",
            "         [[ 4.0424e-02, -2.6381e-02, -1.2884e-02],\n",
            "          [ 2.9701e-02,  2.2360e-02,  3.2802e-02],\n",
            "          [ 1.6789e-02,  2.3137e-02,  3.4536e-03]],\n",
            "\n",
            "         [[-2.1963e-02, -3.2222e-02,  1.4629e-03],\n",
            "          [-3.1903e-02,  3.0015e-02,  2.9056e-02],\n",
            "          [-2.5362e-02,  7.8096e-03,  4.0350e-02]]]], device='cuda:0')\n",
            "N1_C3.bias Parameter containing:\n",
            "tensor([ 3.7400e-02,  2.2330e-02,  7.5857e-03,  3.4346e-02,  3.8973e-02,\n",
            "        -4.0468e-02,  2.8967e-02, -3.3560e-02, -5.2724e-03,  3.2445e-02,\n",
            "        -1.2989e-02,  3.6399e-03,  2.5010e-03,  3.8952e-02,  3.1423e-02,\n",
            "        -1.9982e-03, -4.0373e-02,  2.9264e-02,  2.2828e-03, -2.1672e-02,\n",
            "         3.4702e-02, -3.7287e-02,  3.0474e-02,  8.0284e-03, -3.0461e-02,\n",
            "         2.8217e-02, -3.7426e-02, -2.5867e-02,  3.4013e-02, -3.7819e-02,\n",
            "         8.1956e-06,  3.6788e-02,  1.0747e-02,  3.6385e-02,  7.9151e-03,\n",
            "         3.2359e-02,  3.4644e-02,  2.2197e-02,  2.1669e-03, -9.2013e-03,\n",
            "         1.3430e-02,  3.1674e-02,  2.3730e-02, -2.9902e-03, -2.1184e-02,\n",
            "         5.2862e-03, -1.5511e-02,  1.8235e-02,  1.6588e-02, -3.0538e-02,\n",
            "        -3.0428e-02,  4.7907e-04,  4.0648e-03, -1.9085e-02, -1.6336e-02,\n",
            "         2.6100e-02,  3.4864e-02,  2.9851e-02,  2.7967e-02,  3.1120e-02,\n",
            "         1.9592e-02, -2.1125e-03, -3.7226e-02, -3.2400e-02, -3.9092e-02,\n",
            "        -5.3194e-03, -9.4866e-03, -2.2082e-02, -6.7455e-03,  2.3775e-02,\n",
            "        -4.1426e-02,  6.2402e-03, -6.6530e-03, -4.1000e-02,  3.7361e-02,\n",
            "         3.1407e-02, -2.5361e-02, -3.0789e-03,  2.1527e-02,  5.3238e-03,\n",
            "         8.0929e-03,  3.6402e-02, -8.3920e-04, -1.0485e-02, -9.5819e-03,\n",
            "         2.1498e-03,  3.9272e-02, -3.3492e-02, -1.7910e-02, -4.0119e-02,\n",
            "        -2.9607e-03,  3.2599e-02, -4.0779e-03, -9.3085e-03, -1.0358e-02,\n",
            "         7.1023e-03, -2.9775e-02,  3.9179e-02,  2.4351e-02, -2.2705e-02,\n",
            "        -3.9021e-02, -1.9963e-02,  9.8012e-03,  3.0689e-02, -3.3135e-04,\n",
            "         3.4110e-02, -3.9835e-02,  1.4346e-02,  1.1599e-02,  9.7044e-03,\n",
            "        -1.7031e-02, -1.9734e-02, -2.0406e-02, -7.3752e-03, -1.2504e-02,\n",
            "        -3.0380e-02, -2.9908e-02, -1.9683e-02, -5.5387e-04, -4.0003e-02,\n",
            "         2.9565e-02,  2.2016e-02, -3.8433e-03,  9.5005e-03,  1.2060e-02,\n",
            "         3.8807e-02, -2.4303e-02,  3.9190e-02], device='cuda:0')\n",
            "N1_C4.weight Parameter containing:\n",
            "tensor([[[[-0.0123, -0.0256, -0.0132],\n",
            "          [ 0.0006,  0.0058, -0.0254],\n",
            "          [ 0.0134,  0.0024, -0.0269]],\n",
            "\n",
            "         [[ 0.0112, -0.0080, -0.0101],\n",
            "          [-0.0286, -0.0269, -0.0004],\n",
            "          [-0.0086, -0.0230,  0.0020]],\n",
            "\n",
            "         [[ 0.0177,  0.0237,  0.0150],\n",
            "          [-0.0108, -0.0179, -0.0261],\n",
            "          [ 0.0251,  0.0068, -0.0251]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0028, -0.0267, -0.0092],\n",
            "          [ 0.0204,  0.0116, -0.0184],\n",
            "          [-0.0154, -0.0079,  0.0020]],\n",
            "\n",
            "         [[-0.0196,  0.0285,  0.0185],\n",
            "          [ 0.0281, -0.0182,  0.0262],\n",
            "          [-0.0250, -0.0072,  0.0190]],\n",
            "\n",
            "         [[-0.0112, -0.0157,  0.0097],\n",
            "          [-0.0165, -0.0288, -0.0025],\n",
            "          [ 0.0106,  0.0208, -0.0142]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0132,  0.0248,  0.0228],\n",
            "          [-0.0069,  0.0004,  0.0280],\n",
            "          [ 0.0016, -0.0046, -0.0027]],\n",
            "\n",
            "         [[-0.0086,  0.0073,  0.0138],\n",
            "          [ 0.0044, -0.0074,  0.0090],\n",
            "          [ 0.0038, -0.0275, -0.0195]],\n",
            "\n",
            "         [[-0.0029, -0.0217, -0.0283],\n",
            "          [-0.0005, -0.0090,  0.0093],\n",
            "          [ 0.0159, -0.0130,  0.0213]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0212,  0.0048,  0.0194],\n",
            "          [ 0.0169, -0.0176, -0.0132],\n",
            "          [ 0.0110, -0.0283,  0.0065]],\n",
            "\n",
            "         [[ 0.0101, -0.0212, -0.0145],\n",
            "          [-0.0089, -0.0030,  0.0262],\n",
            "          [-0.0269,  0.0275, -0.0160]],\n",
            "\n",
            "         [[-0.0055, -0.0276,  0.0283],\n",
            "          [-0.0260,  0.0154,  0.0182],\n",
            "          [ 0.0159, -0.0104, -0.0220]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0291,  0.0051,  0.0119],\n",
            "          [-0.0150,  0.0261,  0.0294],\n",
            "          [ 0.0093, -0.0274, -0.0275]],\n",
            "\n",
            "         [[ 0.0067,  0.0043,  0.0049],\n",
            "          [-0.0109, -0.0052, -0.0043],\n",
            "          [ 0.0064, -0.0229, -0.0236]],\n",
            "\n",
            "         [[-0.0173, -0.0189, -0.0208],\n",
            "          [ 0.0218,  0.0163, -0.0288],\n",
            "          [ 0.0234,  0.0290,  0.0110]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0220, -0.0007,  0.0010],\n",
            "          [ 0.0251,  0.0193, -0.0206],\n",
            "          [-0.0208, -0.0013,  0.0214]],\n",
            "\n",
            "         [[ 0.0069,  0.0165,  0.0123],\n",
            "          [-0.0003,  0.0126,  0.0037],\n",
            "          [ 0.0244, -0.0042, -0.0063]],\n",
            "\n",
            "         [[-0.0141, -0.0149,  0.0205],\n",
            "          [ 0.0007,  0.0048,  0.0216],\n",
            "          [-0.0199, -0.0218,  0.0045]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0254,  0.0215, -0.0143],\n",
            "          [ 0.0222,  0.0107, -0.0261],\n",
            "          [-0.0097, -0.0039, -0.0041]],\n",
            "\n",
            "         [[ 0.0241,  0.0148,  0.0264],\n",
            "          [-0.0200, -0.0274, -0.0278],\n",
            "          [-0.0156,  0.0270,  0.0015]],\n",
            "\n",
            "         [[-0.0120, -0.0057, -0.0221],\n",
            "          [ 0.0137,  0.0012,  0.0080],\n",
            "          [-0.0156, -0.0275, -0.0173]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0091, -0.0086,  0.0179],\n",
            "          [ 0.0002,  0.0088,  0.0240],\n",
            "          [-0.0017,  0.0212,  0.0238]],\n",
            "\n",
            "         [[ 0.0087, -0.0252,  0.0164],\n",
            "          [-0.0289,  0.0145,  0.0115],\n",
            "          [-0.0166, -0.0276, -0.0145]],\n",
            "\n",
            "         [[-0.0011, -0.0058, -0.0235],\n",
            "          [-0.0238, -0.0100,  0.0112],\n",
            "          [ 0.0234, -0.0013, -0.0017]]],\n",
            "\n",
            "\n",
            "        [[[-0.0070, -0.0172, -0.0121],\n",
            "          [-0.0146,  0.0003,  0.0068],\n",
            "          [ 0.0131, -0.0281, -0.0255]],\n",
            "\n",
            "         [[-0.0256, -0.0018,  0.0129],\n",
            "          [ 0.0062,  0.0259,  0.0217],\n",
            "          [ 0.0043, -0.0108, -0.0230]],\n",
            "\n",
            "         [[-0.0182,  0.0052,  0.0092],\n",
            "          [-0.0260,  0.0060,  0.0087],\n",
            "          [ 0.0277,  0.0063,  0.0262]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0169,  0.0276,  0.0005],\n",
            "          [-0.0203,  0.0086, -0.0235],\n",
            "          [ 0.0049, -0.0267, -0.0098]],\n",
            "\n",
            "         [[ 0.0248, -0.0215,  0.0198],\n",
            "          [-0.0053,  0.0290, -0.0121],\n",
            "          [ 0.0047,  0.0016,  0.0055]],\n",
            "\n",
            "         [[ 0.0159, -0.0182, -0.0226],\n",
            "          [-0.0151, -0.0118,  0.0248],\n",
            "          [ 0.0136,  0.0263, -0.0187]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0277, -0.0124, -0.0089],\n",
            "          [-0.0212, -0.0148, -0.0264],\n",
            "          [-0.0262, -0.0131,  0.0045]],\n",
            "\n",
            "         [[-0.0113,  0.0064,  0.0107],\n",
            "          [ 0.0240,  0.0064, -0.0186],\n",
            "          [-0.0167, -0.0234,  0.0186]],\n",
            "\n",
            "         [[-0.0188,  0.0277,  0.0084],\n",
            "          [-0.0035,  0.0226, -0.0091],\n",
            "          [ 0.0229,  0.0011,  0.0212]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0144,  0.0088, -0.0265],\n",
            "          [ 0.0235,  0.0219,  0.0281],\n",
            "          [-0.0221,  0.0220, -0.0098]],\n",
            "\n",
            "         [[ 0.0284, -0.0241,  0.0250],\n",
            "          [-0.0256, -0.0168, -0.0146],\n",
            "          [-0.0126, -0.0082,  0.0070]],\n",
            "\n",
            "         [[ 0.0241,  0.0044,  0.0041],\n",
            "          [ 0.0164,  0.0130, -0.0264],\n",
            "          [ 0.0294,  0.0289,  0.0028]]]], device='cuda:0')\n",
            "N1_C4.bias Parameter containing:\n",
            "tensor([ 0.0288,  0.0054, -0.0187, -0.0147, -0.0242,  0.0225, -0.0208,  0.0119,\n",
            "        -0.0012,  0.0060, -0.0022,  0.0130, -0.0086,  0.0030,  0.0022, -0.0225,\n",
            "        -0.0232,  0.0166,  0.0293, -0.0019, -0.0012, -0.0044,  0.0284,  0.0247,\n",
            "        -0.0110, -0.0165,  0.0211, -0.0185,  0.0265,  0.0156, -0.0259,  0.0095,\n",
            "         0.0135,  0.0124, -0.0256,  0.0159,  0.0080, -0.0238, -0.0045,  0.0251,\n",
            "        -0.0210, -0.0006,  0.0073,  0.0241,  0.0091, -0.0026, -0.0205, -0.0191,\n",
            "        -0.0026,  0.0126,  0.0052, -0.0057,  0.0050,  0.0037, -0.0063,  0.0219,\n",
            "        -0.0148,  0.0013,  0.0010,  0.0037, -0.0193,  0.0283, -0.0171,  0.0025,\n",
            "         0.0049,  0.0252,  0.0200,  0.0150, -0.0199,  0.0078,  0.0280,  0.0142,\n",
            "         0.0243,  0.0162,  0.0140,  0.0177, -0.0006,  0.0010,  0.0258,  0.0150,\n",
            "         0.0096, -0.0178, -0.0137, -0.0227,  0.0044, -0.0003, -0.0134,  0.0005,\n",
            "         0.0261, -0.0287,  0.0122, -0.0184, -0.0180,  0.0064,  0.0108,  0.0250,\n",
            "        -0.0089,  0.0047,  0.0245, -0.0049, -0.0156,  0.0221, -0.0214, -0.0277,\n",
            "         0.0281, -0.0082,  0.0172,  0.0211, -0.0116, -0.0221,  0.0011,  0.0271,\n",
            "        -0.0081, -0.0085, -0.0094, -0.0081,  0.0286, -0.0142, -0.0270,  0.0131,\n",
            "        -0.0039,  0.0069,  0.0065, -0.0063, -0.0141,  0.0204,  0.0224, -0.0151],\n",
            "       device='cuda:0')\n",
            "N1_D1.weight Parameter containing:\n",
            "tensor([[ 2.4536e-03,  2.2222e-03,  8.0004e-05,  ..., -1.7189e-03,\n",
            "          3.7672e-04,  6.4686e-04],\n",
            "        [-1.8015e-03,  1.6396e-03,  1.1643e-04,  ..., -2.1851e-03,\n",
            "          2.7415e-03,  2.4166e-03],\n",
            "        [-6.1342e-04,  1.8467e-03,  3.8234e-04,  ..., -2.7104e-03,\n",
            "         -6.6150e-04, -2.2750e-03],\n",
            "        ...,\n",
            "        [-1.3594e-03,  2.2981e-03,  2.3157e-03,  ...,  2.4002e-03,\n",
            "         -1.6155e-03,  1.6008e-03],\n",
            "        [ 1.6782e-03,  2.3875e-03,  2.1182e-03,  ..., -1.0080e-03,\n",
            "          1.4187e-04,  2.0821e-03],\n",
            "        [-9.8886e-04, -4.9304e-04,  2.0161e-03,  ...,  2.7328e-03,\n",
            "          1.9217e-03, -3.7604e-04]], device='cuda:0')\n",
            "N1_D1.bias Parameter containing:\n",
            "tensor([ 2.0141e-03,  1.3051e-03, -6.5517e-04, -1.0302e-03, -2.1088e-03,\n",
            "        -2.6833e-03, -2.6688e-03, -1.7081e-03, -2.2935e-03, -6.1256e-04,\n",
            "        -2.4479e-03, -1.3640e-03,  2.1083e-03,  2.7320e-03, -4.0296e-05,\n",
            "         2.0057e-03,  2.3247e-03,  2.1355e-04, -1.3956e-03, -1.5677e-03,\n",
            "         2.3106e-03, -1.7177e-04, -6.0970e-04, -1.4758e-03,  2.5509e-03,\n",
            "         2.3712e-03, -1.3542e-03,  2.6383e-03, -1.9184e-03,  3.3045e-04,\n",
            "        -2.6471e-03,  2.4566e-03,  1.7707e-03,  1.0866e-03, -1.2713e-03,\n",
            "        -3.4732e-04,  8.8777e-04, -1.7947e-04,  2.6929e-03,  2.4575e-03,\n",
            "        -1.1955e-03, -1.0550e-03, -2.6678e-03, -5.6417e-05,  2.1283e-03,\n",
            "        -2.2932e-03,  2.3626e-03,  1.3360e-03,  6.1900e-04,  5.0774e-04,\n",
            "        -7.3697e-05, -2.3220e-03,  1.5369e-03,  4.5875e-04, -1.8032e-03,\n",
            "        -1.7618e-04,  1.6662e-03, -2.4716e-03,  5.1413e-04, -1.4701e-03,\n",
            "         5.3089e-04, -2.5814e-03,  2.5969e-03,  2.4294e-04, -1.5948e-03,\n",
            "        -4.7022e-05,  2.4837e-03,  1.8827e-03, -2.7229e-03, -1.6333e-03,\n",
            "        -1.2172e-03, -8.1095e-04,  2.1233e-03, -1.7493e-03, -1.0726e-04,\n",
            "         1.8228e-03,  1.1752e-03, -2.1465e-03, -1.4527e-03,  6.9293e-04,\n",
            "         2.2636e-03,  6.6714e-04,  2.2816e-03,  2.7560e-03,  1.0259e-03,\n",
            "         2.7445e-03, -6.8191e-04, -1.3280e-03,  3.5246e-04,  8.7752e-04,\n",
            "         4.1938e-04, -4.3437e-04, -2.5514e-03, -9.4640e-04,  9.1219e-04,\n",
            "         4.6513e-04,  2.4664e-03,  1.7291e-03, -2.4677e-03, -2.9188e-05,\n",
            "         2.2228e-03,  2.6860e-03,  2.1353e-04,  7.6689e-04,  6.5679e-05,\n",
            "         1.4407e-03,  2.6844e-03, -2.5244e-03,  2.7194e-03,  4.9753e-04,\n",
            "        -1.1694e-03,  1.0155e-03, -1.2513e-03,  4.3793e-04, -2.3984e-04,\n",
            "         1.2339e-03,  1.1677e-03, -2.5666e-03,  7.3597e-05, -4.0814e-04,\n",
            "        -7.5760e-04,  9.2105e-04, -3.0272e-04,  3.1720e-04,  1.1164e-03,\n",
            "         7.4154e-04,  1.1039e-03,  2.2778e-04, -5.0626e-04,  9.9434e-04,\n",
            "         4.2273e-04, -2.2524e-04,  1.5244e-04,  1.4243e-03, -2.2416e-03,\n",
            "         2.3328e-03,  1.7140e-03,  2.2082e-03, -2.6811e-03, -9.6654e-04,\n",
            "         1.7576e-03, -2.7598e-04,  1.0345e-03,  1.4120e-03, -5.5726e-04,\n",
            "        -8.9680e-04,  8.2907e-04,  1.8753e-03, -8.0066e-04,  1.0474e-03,\n",
            "        -1.0118e-03,  1.5999e-03, -6.0197e-04, -2.0183e-03, -2.7921e-04,\n",
            "        -6.0939e-04, -1.4124e-04,  2.3940e-03,  6.3232e-04, -1.7456e-03,\n",
            "         2.6540e-03,  4.2652e-04, -1.8505e-03,  1.6481e-03, -2.2308e-03,\n",
            "        -2.3387e-03, -4.8033e-05, -2.2836e-03,  1.8273e-03, -2.2781e-04,\n",
            "         1.8837e-03, -6.0818e-04,  5.6497e-04,  1.4428e-03,  1.2082e-03,\n",
            "        -8.8641e-04, -2.6025e-03,  3.5849e-04, -6.0860e-04, -1.0280e-03,\n",
            "        -6.3791e-04, -2.6645e-04,  1.5479e-03,  1.4573e-03,  1.8856e-03,\n",
            "        -1.4610e-03, -1.4303e-03,  2.2273e-04, -4.9616e-04, -1.5390e-03,\n",
            "         1.3128e-03,  2.3298e-03,  2.6536e-04, -2.0736e-03,  1.7052e-03,\n",
            "        -2.1047e-03,  1.5212e-03,  1.8618e-03, -6.9653e-04, -1.2382e-03,\n",
            "        -7.0889e-04,  2.6492e-03,  6.3019e-04,  7.2117e-04, -1.0767e-03,\n",
            "         1.7747e-03,  1.9380e-03, -6.6208e-04,  1.7717e-03,  2.1400e-03,\n",
            "        -7.5901e-04,  2.0075e-03, -2.0101e-03, -1.0262e-03,  1.3106e-03,\n",
            "         7.1158e-04,  1.9428e-03,  1.6448e-03, -1.1298e-03,  3.9570e-05,\n",
            "         2.0196e-04,  8.3856e-04,  2.0556e-03, -1.1921e-04, -1.1479e-03,\n",
            "         1.1449e-03,  1.3362e-03, -4.6807e-04,  1.1662e-03, -7.9076e-04,\n",
            "         7.0761e-04, -1.5940e-03,  2.5693e-03, -1.2726e-03,  1.9806e-03,\n",
            "         1.3790e-03, -9.9193e-04,  7.5753e-04,  1.9252e-03, -8.0542e-04,\n",
            "        -8.8500e-04,  1.4393e-03, -1.2260e-03,  4.9930e-04, -1.3808e-03,\n",
            "         1.7437e-03,  1.2999e-03,  1.9781e-03,  7.0358e-04,  2.4743e-03,\n",
            "        -1.8291e-04,  2.5422e-03,  8.2705e-05, -6.1309e-05, -2.7169e-04,\n",
            "         1.4140e-03], device='cuda:0')\n",
            "N1_D2.weight Parameter containing:\n",
            "tensor([[ 0.0560,  0.0379, -0.0072,  ...,  0.0560, -0.0076, -0.0552],\n",
            "        [ 0.0157,  0.0608,  0.0539,  ..., -0.0485,  0.0403, -0.0390],\n",
            "        [-0.0145, -0.0561,  0.0016,  ..., -0.0375, -0.0612, -0.0181],\n",
            "        ...,\n",
            "        [ 0.0303, -0.0399, -0.0010,  ...,  0.0076, -0.0044,  0.0545],\n",
            "        [-0.0398, -0.0180,  0.0341,  ..., -0.0530,  0.0032, -0.0227],\n",
            "        [-0.0377,  0.0466, -0.0588,  ...,  0.0597,  0.0032,  0.0589]],\n",
            "       device='cuda:0')\n",
            "N1_D2.bias Parameter containing:\n",
            "tensor([-0.0480, -0.0466, -0.0068, -0.0081,  0.0177, -0.0155, -0.0186, -0.0124,\n",
            "         0.0274,  0.0535,  0.0375,  0.0354,  0.0338, -0.0126, -0.0199, -0.0527,\n",
            "         0.0417, -0.0169,  0.0048, -0.0329,  0.0395, -0.0418, -0.0484, -0.0260,\n",
            "         0.0508, -0.0153, -0.0352,  0.0026,  0.0024, -0.0037, -0.0121,  0.0247,\n",
            "        -0.0287, -0.0563,  0.0319, -0.0407, -0.0492, -0.0599, -0.0420, -0.0560,\n",
            "         0.0260, -0.0597,  0.0106, -0.0206, -0.0307,  0.0299,  0.0609,  0.0479,\n",
            "         0.0096,  0.0403, -0.0473, -0.0423, -0.0247, -0.0045,  0.0174,  0.0200,\n",
            "         0.0508, -0.0587, -0.0041, -0.0142,  0.0328,  0.0448,  0.0136,  0.0136,\n",
            "         0.0528,  0.0278, -0.0288,  0.0610, -0.0486, -0.0120, -0.0565,  0.0028,\n",
            "         0.0286,  0.0247, -0.0421, -0.0078, -0.0262,  0.0070,  0.0170,  0.0115,\n",
            "         0.0598,  0.0228, -0.0368, -0.0302, -0.0014, -0.0526, -0.0446,  0.0242,\n",
            "        -0.0610, -0.0543, -0.0038,  0.0490, -0.0093, -0.0592, -0.0224,  0.0225,\n",
            "         0.0416,  0.0604, -0.0389, -0.0388, -0.0053, -0.0120, -0.0625,  0.0142,\n",
            "         0.0366,  0.0186, -0.0034, -0.0454,  0.0576, -0.0376,  0.0443, -0.0338,\n",
            "        -0.0111,  0.0219,  0.0078,  0.0119, -0.0419, -0.0233,  0.0107,  0.0178,\n",
            "         0.0353,  0.0570,  0.0486, -0.0541, -0.0051, -0.0580,  0.0294,  0.0004,\n",
            "         0.0140,  0.0449, -0.0152,  0.0232,  0.0114,  0.0028,  0.0020, -0.0545,\n",
            "         0.0606, -0.0344,  0.0500,  0.0211, -0.0226,  0.0260, -0.0107, -0.0443,\n",
            "        -0.0186,  0.0612,  0.0616,  0.0128, -0.0012, -0.0181,  0.0622,  0.0419,\n",
            "        -0.0114, -0.0464,  0.0468,  0.0624,  0.0419, -0.0208,  0.0292,  0.0303,\n",
            "        -0.0524, -0.0519,  0.0203, -0.0521,  0.0292,  0.0022, -0.0380, -0.0024,\n",
            "         0.0298,  0.0314,  0.0469, -0.0004, -0.0355, -0.0447,  0.0613, -0.0205,\n",
            "        -0.0611,  0.0362,  0.0118,  0.0235, -0.0325,  0.0214,  0.0039, -0.0460,\n",
            "         0.0464,  0.0173,  0.0044,  0.0332, -0.0172,  0.0617,  0.0513, -0.0046,\n",
            "         0.0442,  0.0138, -0.0600, -0.0043, -0.0196, -0.0090,  0.0470,  0.0496,\n",
            "        -0.0483,  0.0318, -0.0517,  0.0611,  0.0228,  0.0590, -0.0451,  0.0204,\n",
            "        -0.0561,  0.0028,  0.0416,  0.0016, -0.0334, -0.0067,  0.0430,  0.0122,\n",
            "        -0.0395,  0.0546, -0.0515, -0.0262,  0.0260,  0.0325, -0.0255, -0.0010,\n",
            "        -0.0189,  0.0121,  0.0125, -0.0252,  0.0313, -0.0288, -0.0300,  0.0331,\n",
            "        -0.0205,  0.0189, -0.0592, -0.0590,  0.0432, -0.0043,  0.0465, -0.0028,\n",
            "        -0.0105,  0.0540,  0.0401,  0.0143, -0.0442, -0.0210,  0.0101,  0.0566,\n",
            "         0.0622,  0.0219,  0.0323,  0.0417, -0.0377, -0.0584,  0.0359,  0.0025],\n",
            "       device='cuda:0')\n",
            "N2_C1.weight Parameter containing:\n",
            "tensor([[[[ 0.1655, -0.1023, -0.0654],\n",
            "          [ 0.1320,  0.1459, -0.0293],\n",
            "          [ 0.1307, -0.0038, -0.1443]],\n",
            "\n",
            "         [[ 0.1255, -0.1350,  0.0066],\n",
            "          [-0.0885,  0.1231, -0.1065],\n",
            "          [-0.0700, -0.0737,  0.1228]],\n",
            "\n",
            "         [[ 0.1619,  0.1924,  0.1041],\n",
            "          [ 0.1877,  0.1497,  0.0791],\n",
            "          [-0.0548,  0.0440, -0.0788]]],\n",
            "\n",
            "\n",
            "        [[[-0.1344, -0.0201,  0.0196],\n",
            "          [-0.0519,  0.0757, -0.0161],\n",
            "          [-0.1060, -0.0630, -0.0215]],\n",
            "\n",
            "         [[ 0.0048, -0.1831,  0.1577],\n",
            "          [-0.1467,  0.1101,  0.1224],\n",
            "          [-0.0428,  0.1472, -0.1545]],\n",
            "\n",
            "         [[ 0.1256,  0.1142,  0.1272],\n",
            "          [-0.0127, -0.0286,  0.1315],\n",
            "          [ 0.1065,  0.0171,  0.1037]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0077, -0.0407, -0.0247],\n",
            "          [ 0.0596,  0.0894, -0.1481],\n",
            "          [ 0.1123, -0.1364, -0.1826]],\n",
            "\n",
            "         [[ 0.0936, -0.0706, -0.1452],\n",
            "          [ 0.1321, -0.0645,  0.0175],\n",
            "          [-0.0288,  0.1139,  0.0435]],\n",
            "\n",
            "         [[-0.1189, -0.1822,  0.1268],\n",
            "          [ 0.0598,  0.1577, -0.0791],\n",
            "          [-0.0528, -0.0397,  0.1527]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0128, -0.0542, -0.1227],\n",
            "          [ 0.0888,  0.0861, -0.1001],\n",
            "          [-0.0162, -0.1255, -0.1477]],\n",
            "\n",
            "         [[ 0.0792, -0.1393,  0.1429],\n",
            "          [ 0.0856, -0.1878,  0.1810],\n",
            "          [ 0.0429, -0.0350, -0.0249]],\n",
            "\n",
            "         [[ 0.1902,  0.1473, -0.1349],\n",
            "          [-0.1016,  0.0181,  0.0599],\n",
            "          [-0.0560, -0.0279, -0.1527]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0145, -0.1641,  0.1091],\n",
            "          [ 0.0806, -0.1125,  0.0487],\n",
            "          [ 0.0226,  0.0997, -0.0657]],\n",
            "\n",
            "         [[ 0.1541, -0.0840,  0.1102],\n",
            "          [-0.1436, -0.0688, -0.0991],\n",
            "          [-0.0961,  0.1541,  0.0594]],\n",
            "\n",
            "         [[ 0.0446,  0.1434,  0.0999],\n",
            "          [-0.0178,  0.1110, -0.0866],\n",
            "          [ 0.0818,  0.1547, -0.0135]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0570,  0.0230,  0.1441],\n",
            "          [ 0.1004,  0.1370, -0.1546],\n",
            "          [-0.1582,  0.1786,  0.1786]],\n",
            "\n",
            "         [[-0.0714, -0.1461, -0.0242],\n",
            "          [-0.1655,  0.0348,  0.0625],\n",
            "          [-0.0380, -0.0692,  0.1028]],\n",
            "\n",
            "         [[-0.0100,  0.1184,  0.0539],\n",
            "          [ 0.0535,  0.0673, -0.0018],\n",
            "          [-0.0840,  0.0528, -0.0486]]]], device='cuda:0', requires_grad=True)\n",
            "N2_C1.bias Parameter containing:\n",
            "tensor([ 0.0149,  0.1442,  0.0307,  0.1423,  0.0271,  0.0737, -0.0683,  0.0026,\n",
            "         0.1769,  0.0853, -0.0475,  0.0976,  0.1593, -0.0339,  0.0075, -0.1508,\n",
            "         0.1101, -0.0543, -0.0705,  0.1912,  0.0731,  0.1866, -0.0827,  0.1098,\n",
            "        -0.0151, -0.0588, -0.0326, -0.1420, -0.0818, -0.1791, -0.1114,  0.1399,\n",
            "        -0.0034,  0.0182,  0.1790,  0.0359,  0.0357,  0.0303,  0.0446,  0.0245,\n",
            "        -0.0075, -0.0333, -0.1813,  0.0199, -0.1550,  0.1054, -0.0339,  0.1648,\n",
            "        -0.1775,  0.0882,  0.0131,  0.1332, -0.1767,  0.1763, -0.0458, -0.0123,\n",
            "         0.0622, -0.0902, -0.0139,  0.0092, -0.1347, -0.1719,  0.1017, -0.1107],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "N2_C2.weight Parameter containing:\n",
            "tensor([[[[-0.0389,  0.0170, -0.0208],\n",
            "          [-0.0350,  0.0032, -0.0136],\n",
            "          [ 0.0393,  0.0085,  0.0415]],\n",
            "\n",
            "         [[-0.0110,  0.0319, -0.0036],\n",
            "          [ 0.0129, -0.0233,  0.0194],\n",
            "          [-0.0002, -0.0313, -0.0224]],\n",
            "\n",
            "         [[ 0.0323, -0.0017,  0.0073],\n",
            "          [-0.0393, -0.0385,  0.0015],\n",
            "          [ 0.0160,  0.0142, -0.0150]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0226, -0.0023, -0.0325],\n",
            "          [-0.0067, -0.0313,  0.0050],\n",
            "          [-0.0100, -0.0309,  0.0306]],\n",
            "\n",
            "         [[ 0.0046, -0.0414,  0.0237],\n",
            "          [ 0.0085, -0.0149, -0.0062],\n",
            "          [ 0.0201, -0.0125, -0.0394]],\n",
            "\n",
            "         [[ 0.0250, -0.0098, -0.0378],\n",
            "          [-0.0013,  0.0068,  0.0200],\n",
            "          [-0.0110, -0.0193,  0.0377]]],\n",
            "\n",
            "\n",
            "        [[[-0.0164,  0.0298, -0.0226],\n",
            "          [-0.0053,  0.0150, -0.0170],\n",
            "          [ 0.0113, -0.0204,  0.0118]],\n",
            "\n",
            "         [[ 0.0299,  0.0228,  0.0225],\n",
            "          [ 0.0184,  0.0150,  0.0334],\n",
            "          [ 0.0132,  0.0409, -0.0052]],\n",
            "\n",
            "         [[-0.0277,  0.0085, -0.0128],\n",
            "          [ 0.0103,  0.0330,  0.0330],\n",
            "          [ 0.0278, -0.0142, -0.0263]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0029, -0.0032, -0.0070],\n",
            "          [-0.0194, -0.0160,  0.0381],\n",
            "          [ 0.0061, -0.0302,  0.0266]],\n",
            "\n",
            "         [[-0.0111,  0.0157, -0.0345],\n",
            "          [ 0.0276, -0.0384,  0.0203],\n",
            "          [ 0.0272, -0.0292, -0.0107]],\n",
            "\n",
            "         [[ 0.0149,  0.0013,  0.0218],\n",
            "          [ 0.0063,  0.0019, -0.0257],\n",
            "          [ 0.0194, -0.0030, -0.0356]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0145, -0.0224,  0.0394],\n",
            "          [-0.0002,  0.0065,  0.0179],\n",
            "          [-0.0270,  0.0035,  0.0280]],\n",
            "\n",
            "         [[-0.0197, -0.0115,  0.0117],\n",
            "          [-0.0013, -0.0271, -0.0352],\n",
            "          [ 0.0191, -0.0184,  0.0320]],\n",
            "\n",
            "         [[ 0.0158,  0.0251, -0.0025],\n",
            "          [-0.0406, -0.0224, -0.0075],\n",
            "          [-0.0168, -0.0329, -0.0249]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0058,  0.0263,  0.0394],\n",
            "          [ 0.0162,  0.0064, -0.0033],\n",
            "          [-0.0091,  0.0195, -0.0409]],\n",
            "\n",
            "         [[-0.0392, -0.0037,  0.0083],\n",
            "          [-0.0383,  0.0139,  0.0160],\n",
            "          [-0.0271, -0.0305, -0.0244]],\n",
            "\n",
            "         [[ 0.0059, -0.0162, -0.0148],\n",
            "          [-0.0089, -0.0217,  0.0171],\n",
            "          [-0.0296, -0.0173,  0.0333]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.0287, -0.0300, -0.0012],\n",
            "          [-0.0388, -0.0136, -0.0271],\n",
            "          [-0.0228, -0.0080,  0.0064]],\n",
            "\n",
            "         [[-0.0050,  0.0318, -0.0335],\n",
            "          [ 0.0124,  0.0028,  0.0271],\n",
            "          [ 0.0005, -0.0091, -0.0405]],\n",
            "\n",
            "         [[ 0.0098, -0.0109,  0.0025],\n",
            "          [ 0.0098, -0.0340,  0.0115],\n",
            "          [ 0.0413,  0.0371, -0.0307]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0064,  0.0322,  0.0144],\n",
            "          [ 0.0067, -0.0266,  0.0162],\n",
            "          [-0.0416,  0.0024, -0.0378]],\n",
            "\n",
            "         [[ 0.0230,  0.0106, -0.0379],\n",
            "          [ 0.0189,  0.0054,  0.0410],\n",
            "          [ 0.0075,  0.0299, -0.0270]],\n",
            "\n",
            "         [[-0.0022,  0.0031, -0.0366],\n",
            "          [-0.0252,  0.0054,  0.0013],\n",
            "          [ 0.0150, -0.0282, -0.0341]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0172, -0.0369, -0.0054],\n",
            "          [ 0.0355,  0.0364, -0.0211],\n",
            "          [-0.0103,  0.0209, -0.0041]],\n",
            "\n",
            "         [[-0.0023,  0.0336, -0.0006],\n",
            "          [ 0.0347,  0.0007,  0.0207],\n",
            "          [ 0.0409, -0.0385,  0.0415]],\n",
            "\n",
            "         [[ 0.0261,  0.0291,  0.0019],\n",
            "          [ 0.0415,  0.0307,  0.0290],\n",
            "          [ 0.0298, -0.0058, -0.0374]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0367, -0.0008, -0.0172],\n",
            "          [-0.0336,  0.0351, -0.0410],\n",
            "          [-0.0164, -0.0321,  0.0103]],\n",
            "\n",
            "         [[ 0.0354,  0.0046,  0.0350],\n",
            "          [ 0.0161, -0.0114,  0.0070],\n",
            "          [-0.0070,  0.0252, -0.0335]],\n",
            "\n",
            "         [[ 0.0043,  0.0020,  0.0344],\n",
            "          [-0.0318,  0.0090,  0.0079],\n",
            "          [-0.0332, -0.0240,  0.0127]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0213,  0.0002,  0.0230],\n",
            "          [ 0.0220, -0.0171, -0.0318],\n",
            "          [-0.0267,  0.0074, -0.0368]],\n",
            "\n",
            "         [[ 0.0336, -0.0401,  0.0207],\n",
            "          [ 0.0137, -0.0208, -0.0291],\n",
            "          [-0.0146,  0.0050,  0.0302]],\n",
            "\n",
            "         [[ 0.0125,  0.0210,  0.0008],\n",
            "          [ 0.0089, -0.0084, -0.0107],\n",
            "          [ 0.0090,  0.0283,  0.0221]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0366,  0.0021, -0.0137],\n",
            "          [-0.0019,  0.0200, -0.0047],\n",
            "          [ 0.0043,  0.0349,  0.0014]],\n",
            "\n",
            "         [[ 0.0228, -0.0173,  0.0271],\n",
            "          [-0.0259,  0.0283, -0.0281],\n",
            "          [ 0.0036, -0.0072,  0.0374]],\n",
            "\n",
            "         [[-0.0078,  0.0276, -0.0326],\n",
            "          [-0.0349,  0.0222,  0.0149],\n",
            "          [-0.0135,  0.0273,  0.0118]]]], device='cuda:0', requires_grad=True)\n",
            "N2_C2.bias Parameter containing:\n",
            "tensor([-0.0362, -0.0107,  0.0200, -0.0066,  0.0123, -0.0369,  0.0246, -0.0410,\n",
            "        -0.0131,  0.0193, -0.0004, -0.0232, -0.0260,  0.0127, -0.0149, -0.0141,\n",
            "         0.0182, -0.0347, -0.0321, -0.0086, -0.0108, -0.0146, -0.0298,  0.0286,\n",
            "        -0.0156,  0.0028,  0.0255, -0.0264,  0.0237,  0.0093,  0.0390, -0.0025,\n",
            "         0.0050,  0.0258, -0.0268,  0.0300,  0.0208,  0.0199,  0.0124, -0.0149,\n",
            "         0.0063,  0.0361,  0.0230,  0.0362,  0.0387,  0.0081,  0.0069, -0.0302,\n",
            "         0.0285,  0.0226,  0.0228,  0.0167, -0.0019, -0.0169,  0.0065, -0.0353,\n",
            "        -0.0096,  0.0106,  0.0151,  0.0279,  0.0012, -0.0284, -0.0333,  0.0218],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "N2_C3.weight Parameter containing:\n",
            "tensor([[[[ 1.6478e-02,  3.6358e-02, -2.4660e-03],\n",
            "          [ 1.4462e-02,  7.8154e-03, -8.7056e-03],\n",
            "          [ 2.8477e-02,  8.8503e-03, -3.6299e-02]],\n",
            "\n",
            "         [[ 3.2571e-02,  1.0216e-03, -3.5548e-02],\n",
            "          [ 2.6030e-03,  7.9593e-03,  2.7901e-02],\n",
            "          [ 3.2491e-02,  2.8091e-04, -2.6749e-02]],\n",
            "\n",
            "         [[-3.7332e-02,  2.2772e-02, -2.1687e-02],\n",
            "          [ 8.6482e-03, -2.7712e-02, -1.8260e-02],\n",
            "          [-1.8952e-02,  4.1075e-02,  3.1533e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.3446e-02,  6.7809e-04,  1.0093e-02],\n",
            "          [-5.1189e-03, -4.0486e-02,  3.2720e-02],\n",
            "          [-1.2961e-02,  5.4246e-03,  3.0900e-02]],\n",
            "\n",
            "         [[ 8.2329e-03, -3.8339e-02, -2.4085e-02],\n",
            "          [ 1.4021e-02, -3.5651e-02,  3.3019e-02],\n",
            "          [ 2.1793e-02, -3.8337e-02,  3.4147e-02]],\n",
            "\n",
            "         [[-2.4871e-03, -3.0027e-02, -5.4283e-03],\n",
            "          [-2.2348e-02, -1.2380e-02,  1.6490e-02],\n",
            "          [-2.1183e-02,  1.9155e-02,  1.4722e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.3230e-02,  2.0484e-02, -2.0927e-02],\n",
            "          [-1.5153e-02,  3.0153e-02, -2.5913e-02],\n",
            "          [ 3.3377e-02,  2.9313e-02,  1.6354e-03]],\n",
            "\n",
            "         [[-2.4203e-02,  3.5340e-02,  1.8368e-02],\n",
            "          [-3.4976e-02, -2.2532e-02, -2.5597e-02],\n",
            "          [-1.5514e-02, -1.7802e-02, -2.9605e-02]],\n",
            "\n",
            "         [[ 3.6059e-02, -3.1973e-02, -4.0246e-02],\n",
            "          [ 1.3481e-04, -1.2641e-02, -2.5085e-02],\n",
            "          [ 1.0859e-02, -6.5656e-03,  3.9958e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.4300e-02, -3.3946e-02, -1.2755e-02],\n",
            "          [-5.5192e-03,  3.1808e-02, -4.6633e-03],\n",
            "          [-2.5712e-02,  4.7626e-03, -2.6625e-03]],\n",
            "\n",
            "         [[ 3.0555e-02, -2.1988e-02,  2.3051e-03],\n",
            "          [-2.6687e-02,  9.9558e-03,  3.4667e-02],\n",
            "          [ 4.1517e-02,  3.9132e-02, -1.3778e-02]],\n",
            "\n",
            "         [[-3.6121e-03, -8.3959e-03, -3.0312e-02],\n",
            "          [-7.9862e-03,  2.0249e-02, -3.5809e-02],\n",
            "          [ 9.6957e-04, -3.6163e-03,  2.3563e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.1184e-02,  3.8366e-02,  2.2504e-02],\n",
            "          [ 8.3281e-03, -4.1128e-02,  1.8214e-02],\n",
            "          [-2.6699e-02, -3.7198e-02,  7.4583e-03]],\n",
            "\n",
            "         [[-1.2468e-02, -3.3490e-02,  3.5394e-02],\n",
            "          [-3.2529e-02,  3.8864e-02, -3.7815e-02],\n",
            "          [-4.1045e-02, -2.5406e-02, -3.3510e-02]],\n",
            "\n",
            "         [[ 2.5755e-02,  7.6556e-03,  1.3890e-02],\n",
            "          [ 4.3359e-03,  3.8102e-02,  1.3977e-02],\n",
            "          [-1.1940e-02, -3.4510e-02,  3.2808e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.6105e-04, -3.8821e-02, -3.7192e-03],\n",
            "          [ 5.8216e-03, -3.1990e-02,  2.8387e-02],\n",
            "          [ 3.4090e-02, -2.2243e-03,  1.0163e-02]],\n",
            "\n",
            "         [[ 2.2208e-02,  4.9898e-03,  5.8362e-03],\n",
            "          [ 3.8052e-02,  2.7821e-02, -4.1032e-02],\n",
            "          [ 8.1057e-03, -3.0102e-02,  2.3647e-02]],\n",
            "\n",
            "         [[-1.7566e-02, -2.0587e-02,  2.4119e-02],\n",
            "          [ 3.1486e-02, -1.3044e-02,  2.0060e-02],\n",
            "          [-3.5179e-02, -2.5455e-02,  1.6286e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 2.6174e-02,  3.9409e-02,  9.9017e-03],\n",
            "          [-3.1703e-02, -1.9336e-02,  1.6662e-02],\n",
            "          [-2.1843e-02, -2.0828e-02,  3.2698e-02]],\n",
            "\n",
            "         [[ 2.1283e-02,  1.3489e-02,  3.7753e-04],\n",
            "          [-3.2695e-02,  2.1647e-02, -7.6396e-03],\n",
            "          [-1.2643e-02, -3.3591e-02, -2.2501e-02]],\n",
            "\n",
            "         [[ 2.7703e-03,  1.0814e-02, -1.3784e-02],\n",
            "          [ 3.1234e-02,  9.4862e-03, -2.3197e-02],\n",
            "          [-1.7959e-02, -3.9530e-02, -3.3960e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.2728e-02, -3.2200e-02, -4.0592e-02],\n",
            "          [-2.6847e-02, -1.0617e-02,  1.9950e-02],\n",
            "          [-4.0580e-02, -1.9289e-02, -1.7786e-02]],\n",
            "\n",
            "         [[ 3.3329e-02,  3.6435e-02,  3.0664e-02],\n",
            "          [ 3.8426e-02,  4.8271e-03,  3.7567e-02],\n",
            "          [ 2.0650e-02,  1.5700e-02,  3.4344e-02]],\n",
            "\n",
            "         [[-1.4099e-02,  3.1830e-02,  2.0700e-02],\n",
            "          [ 1.4313e-02, -3.0187e-02,  3.7834e-02],\n",
            "          [ 1.4758e-02,  2.8110e-02,  2.1674e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.1996e-02, -2.3614e-02, -2.8718e-02],\n",
            "          [ 1.1023e-02,  3.1554e-02, -1.8433e-02],\n",
            "          [ 9.4037e-03, -2.7395e-03, -3.8592e-02]],\n",
            "\n",
            "         [[-3.9353e-02,  1.6781e-02,  1.9607e-02],\n",
            "          [ 1.1022e-02,  2.5836e-02, -2.6190e-02],\n",
            "          [ 7.3778e-04, -4.3795e-03, -4.0791e-02]],\n",
            "\n",
            "         [[ 2.8989e-03,  2.5238e-02, -1.9186e-02],\n",
            "          [ 4.0651e-02,  2.2085e-02,  1.8422e-02],\n",
            "          [-3.6972e-03, -1.8189e-02,  7.2231e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.9173e-02,  3.4628e-02, -7.4682e-03],\n",
            "          [ 2.3930e-02, -1.2837e-02, -1.0575e-02],\n",
            "          [-2.0783e-02,  5.8234e-03, -2.4170e-02]],\n",
            "\n",
            "         [[-4.0537e-03, -7.8612e-03, -2.1826e-02],\n",
            "          [ 9.6693e-03, -4.0850e-02, -2.9677e-02],\n",
            "          [-5.2926e-03,  3.6731e-02,  3.3701e-02]],\n",
            "\n",
            "         [[ 1.4566e-02, -1.3287e-03, -2.6809e-02],\n",
            "          [-3.1466e-02, -9.1707e-03,  1.6904e-02],\n",
            "          [ 2.0809e-02, -2.7789e-03, -2.5178e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.5481e-02,  5.9936e-03, -2.0217e-02],\n",
            "          [ 2.8088e-02,  3.6297e-02,  2.0679e-02],\n",
            "          [-9.1550e-03, -3.4913e-02,  3.6179e-04]],\n",
            "\n",
            "         [[-1.0259e-02, -9.1384e-03,  6.0563e-03],\n",
            "          [-9.1801e-04, -3.6729e-03, -1.1379e-02],\n",
            "          [ 1.0903e-02, -4.5910e-03, -3.8480e-02]],\n",
            "\n",
            "         [[-3.3089e-02, -1.8743e-02,  1.4444e-02],\n",
            "          [ 2.3021e-02,  2.4650e-02,  4.9097e-03],\n",
            "          [ 1.9988e-02, -1.7949e-02,  1.9873e-05]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.4252e-02,  8.8538e-03, -2.3385e-02],\n",
            "          [ 3.6371e-03,  1.7729e-02, -1.2878e-02],\n",
            "          [-1.1424e-03, -2.1906e-02,  1.4050e-02]],\n",
            "\n",
            "         [[ 4.0424e-02, -2.6381e-02, -1.2884e-02],\n",
            "          [ 2.9701e-02,  2.2360e-02,  3.2802e-02],\n",
            "          [ 1.6789e-02,  2.3137e-02,  3.4536e-03]],\n",
            "\n",
            "         [[-2.1963e-02, -3.2222e-02,  1.4629e-03],\n",
            "          [-3.1903e-02,  3.0015e-02,  2.9056e-02],\n",
            "          [-2.5362e-02,  7.8096e-03,  4.0350e-02]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "N2_C3.bias Parameter containing:\n",
            "tensor([ 3.7400e-02,  2.2330e-02,  7.5857e-03,  3.4346e-02,  3.8973e-02,\n",
            "        -4.0468e-02,  2.8967e-02, -3.3560e-02, -5.2724e-03,  3.2445e-02,\n",
            "        -1.2989e-02,  3.6399e-03,  2.5010e-03,  3.8952e-02,  3.1423e-02,\n",
            "        -1.9982e-03, -4.0373e-02,  2.9264e-02,  2.2828e-03, -2.1672e-02,\n",
            "         3.4702e-02, -3.7287e-02,  3.0474e-02,  8.0284e-03, -3.0461e-02,\n",
            "         2.8217e-02, -3.7426e-02, -2.5867e-02,  3.4013e-02, -3.7819e-02,\n",
            "         8.1956e-06,  3.6788e-02,  1.0747e-02,  3.6385e-02,  7.9151e-03,\n",
            "         3.2359e-02,  3.4644e-02,  2.2197e-02,  2.1669e-03, -9.2013e-03,\n",
            "         1.3430e-02,  3.1674e-02,  2.3730e-02, -2.9902e-03, -2.1184e-02,\n",
            "         5.2862e-03, -1.5511e-02,  1.8235e-02,  1.6588e-02, -3.0538e-02,\n",
            "        -3.0428e-02,  4.7907e-04,  4.0648e-03, -1.9085e-02, -1.6336e-02,\n",
            "         2.6100e-02,  3.4864e-02,  2.9851e-02,  2.7967e-02,  3.1120e-02,\n",
            "         1.9592e-02, -2.1125e-03, -3.7226e-02, -3.2400e-02, -3.9092e-02,\n",
            "        -5.3194e-03, -9.4866e-03, -2.2082e-02, -6.7455e-03,  2.3775e-02,\n",
            "        -4.1426e-02,  6.2402e-03, -6.6530e-03, -4.1000e-02,  3.7361e-02,\n",
            "         3.1407e-02, -2.5361e-02, -3.0789e-03,  2.1527e-02,  5.3238e-03,\n",
            "         8.0929e-03,  3.6402e-02, -8.3920e-04, -1.0485e-02, -9.5819e-03,\n",
            "         2.1498e-03,  3.9272e-02, -3.3492e-02, -1.7910e-02, -4.0119e-02,\n",
            "        -2.9607e-03,  3.2599e-02, -4.0779e-03, -9.3085e-03, -1.0358e-02,\n",
            "         7.1023e-03, -2.9775e-02,  3.9179e-02,  2.4351e-02, -2.2705e-02,\n",
            "        -3.9021e-02, -1.9963e-02,  9.8012e-03,  3.0689e-02, -3.3135e-04,\n",
            "         3.4110e-02, -3.9835e-02,  1.4346e-02,  1.1599e-02,  9.7044e-03,\n",
            "        -1.7031e-02, -1.9734e-02, -2.0406e-02, -7.3752e-03, -1.2504e-02,\n",
            "        -3.0380e-02, -2.9908e-02, -1.9683e-02, -5.5387e-04, -4.0003e-02,\n",
            "         2.9565e-02,  2.2016e-02, -3.8433e-03,  9.5005e-03,  1.2060e-02,\n",
            "         3.8807e-02, -2.4303e-02,  3.9190e-02], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "N2_C4.weight Parameter containing:\n",
            "tensor([[[[-0.0123, -0.0256, -0.0132],\n",
            "          [ 0.0006,  0.0058, -0.0254],\n",
            "          [ 0.0134,  0.0024, -0.0269]],\n",
            "\n",
            "         [[ 0.0112, -0.0080, -0.0101],\n",
            "          [-0.0286, -0.0269, -0.0004],\n",
            "          [-0.0086, -0.0230,  0.0020]],\n",
            "\n",
            "         [[ 0.0177,  0.0237,  0.0150],\n",
            "          [-0.0108, -0.0179, -0.0261],\n",
            "          [ 0.0251,  0.0068, -0.0251]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0028, -0.0267, -0.0092],\n",
            "          [ 0.0204,  0.0116, -0.0184],\n",
            "          [-0.0154, -0.0079,  0.0020]],\n",
            "\n",
            "         [[-0.0196,  0.0285,  0.0185],\n",
            "          [ 0.0281, -0.0182,  0.0262],\n",
            "          [-0.0250, -0.0072,  0.0190]],\n",
            "\n",
            "         [[-0.0112, -0.0157,  0.0097],\n",
            "          [-0.0165, -0.0288, -0.0025],\n",
            "          [ 0.0106,  0.0208, -0.0142]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0132,  0.0248,  0.0228],\n",
            "          [-0.0069,  0.0004,  0.0280],\n",
            "          [ 0.0016, -0.0046, -0.0027]],\n",
            "\n",
            "         [[-0.0086,  0.0073,  0.0138],\n",
            "          [ 0.0044, -0.0074,  0.0090],\n",
            "          [ 0.0038, -0.0275, -0.0195]],\n",
            "\n",
            "         [[-0.0029, -0.0217, -0.0283],\n",
            "          [-0.0005, -0.0090,  0.0093],\n",
            "          [ 0.0159, -0.0130,  0.0213]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0212,  0.0048,  0.0194],\n",
            "          [ 0.0169, -0.0176, -0.0132],\n",
            "          [ 0.0110, -0.0283,  0.0065]],\n",
            "\n",
            "         [[ 0.0101, -0.0212, -0.0145],\n",
            "          [-0.0089, -0.0030,  0.0262],\n",
            "          [-0.0269,  0.0275, -0.0160]],\n",
            "\n",
            "         [[-0.0055, -0.0276,  0.0283],\n",
            "          [-0.0260,  0.0154,  0.0182],\n",
            "          [ 0.0159, -0.0104, -0.0220]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0291,  0.0051,  0.0119],\n",
            "          [-0.0150,  0.0261,  0.0294],\n",
            "          [ 0.0093, -0.0274, -0.0275]],\n",
            "\n",
            "         [[ 0.0067,  0.0043,  0.0049],\n",
            "          [-0.0109, -0.0052, -0.0043],\n",
            "          [ 0.0064, -0.0229, -0.0236]],\n",
            "\n",
            "         [[-0.0173, -0.0189, -0.0208],\n",
            "          [ 0.0218,  0.0163, -0.0288],\n",
            "          [ 0.0234,  0.0290,  0.0110]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0220, -0.0007,  0.0010],\n",
            "          [ 0.0251,  0.0193, -0.0206],\n",
            "          [-0.0208, -0.0013,  0.0214]],\n",
            "\n",
            "         [[ 0.0069,  0.0165,  0.0123],\n",
            "          [-0.0003,  0.0126,  0.0037],\n",
            "          [ 0.0244, -0.0042, -0.0063]],\n",
            "\n",
            "         [[-0.0141, -0.0149,  0.0205],\n",
            "          [ 0.0007,  0.0048,  0.0216],\n",
            "          [-0.0199, -0.0218,  0.0045]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0254,  0.0215, -0.0143],\n",
            "          [ 0.0222,  0.0107, -0.0261],\n",
            "          [-0.0097, -0.0039, -0.0041]],\n",
            "\n",
            "         [[ 0.0241,  0.0148,  0.0264],\n",
            "          [-0.0200, -0.0274, -0.0278],\n",
            "          [-0.0156,  0.0270,  0.0015]],\n",
            "\n",
            "         [[-0.0120, -0.0057, -0.0221],\n",
            "          [ 0.0137,  0.0012,  0.0080],\n",
            "          [-0.0156, -0.0275, -0.0173]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0091, -0.0086,  0.0179],\n",
            "          [ 0.0002,  0.0088,  0.0240],\n",
            "          [-0.0017,  0.0212,  0.0238]],\n",
            "\n",
            "         [[ 0.0087, -0.0252,  0.0164],\n",
            "          [-0.0289,  0.0145,  0.0115],\n",
            "          [-0.0166, -0.0276, -0.0145]],\n",
            "\n",
            "         [[-0.0011, -0.0058, -0.0235],\n",
            "          [-0.0238, -0.0100,  0.0112],\n",
            "          [ 0.0234, -0.0013, -0.0017]]],\n",
            "\n",
            "\n",
            "        [[[-0.0070, -0.0172, -0.0121],\n",
            "          [-0.0146,  0.0003,  0.0068],\n",
            "          [ 0.0131, -0.0281, -0.0255]],\n",
            "\n",
            "         [[-0.0256, -0.0018,  0.0129],\n",
            "          [ 0.0062,  0.0259,  0.0217],\n",
            "          [ 0.0043, -0.0108, -0.0230]],\n",
            "\n",
            "         [[-0.0182,  0.0052,  0.0092],\n",
            "          [-0.0260,  0.0060,  0.0087],\n",
            "          [ 0.0277,  0.0063,  0.0262]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0169,  0.0276,  0.0005],\n",
            "          [-0.0203,  0.0086, -0.0235],\n",
            "          [ 0.0049, -0.0267, -0.0098]],\n",
            "\n",
            "         [[ 0.0248, -0.0215,  0.0198],\n",
            "          [-0.0053,  0.0290, -0.0121],\n",
            "          [ 0.0047,  0.0016,  0.0055]],\n",
            "\n",
            "         [[ 0.0159, -0.0182, -0.0226],\n",
            "          [-0.0151, -0.0118,  0.0248],\n",
            "          [ 0.0136,  0.0263, -0.0187]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0277, -0.0124, -0.0089],\n",
            "          [-0.0212, -0.0148, -0.0264],\n",
            "          [-0.0262, -0.0131,  0.0045]],\n",
            "\n",
            "         [[-0.0113,  0.0064,  0.0107],\n",
            "          [ 0.0240,  0.0064, -0.0186],\n",
            "          [-0.0167, -0.0234,  0.0186]],\n",
            "\n",
            "         [[-0.0188,  0.0277,  0.0084],\n",
            "          [-0.0035,  0.0226, -0.0091],\n",
            "          [ 0.0229,  0.0011,  0.0212]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0144,  0.0088, -0.0265],\n",
            "          [ 0.0235,  0.0219,  0.0281],\n",
            "          [-0.0221,  0.0220, -0.0098]],\n",
            "\n",
            "         [[ 0.0284, -0.0241,  0.0250],\n",
            "          [-0.0256, -0.0168, -0.0146],\n",
            "          [-0.0126, -0.0082,  0.0070]],\n",
            "\n",
            "         [[ 0.0241,  0.0044,  0.0041],\n",
            "          [ 0.0164,  0.0130, -0.0264],\n",
            "          [ 0.0294,  0.0289,  0.0028]]]], device='cuda:0', requires_grad=True)\n",
            "N2_C4.bias Parameter containing:\n",
            "tensor([ 0.0288,  0.0054, -0.0187, -0.0147, -0.0242,  0.0225, -0.0208,  0.0119,\n",
            "        -0.0012,  0.0060, -0.0022,  0.0130, -0.0086,  0.0030,  0.0022, -0.0225,\n",
            "        -0.0232,  0.0166,  0.0293, -0.0019, -0.0012, -0.0044,  0.0284,  0.0247,\n",
            "        -0.0110, -0.0165,  0.0211, -0.0185,  0.0265,  0.0156, -0.0259,  0.0095,\n",
            "         0.0135,  0.0124, -0.0256,  0.0159,  0.0080, -0.0238, -0.0045,  0.0251,\n",
            "        -0.0210, -0.0006,  0.0073,  0.0241,  0.0091, -0.0026, -0.0205, -0.0191,\n",
            "        -0.0026,  0.0126,  0.0052, -0.0057,  0.0050,  0.0037, -0.0063,  0.0219,\n",
            "        -0.0148,  0.0013,  0.0010,  0.0037, -0.0193,  0.0283, -0.0171,  0.0025,\n",
            "         0.0049,  0.0252,  0.0200,  0.0150, -0.0199,  0.0078,  0.0280,  0.0142,\n",
            "         0.0243,  0.0162,  0.0140,  0.0177, -0.0006,  0.0010,  0.0258,  0.0150,\n",
            "         0.0096, -0.0178, -0.0137, -0.0227,  0.0044, -0.0003, -0.0134,  0.0005,\n",
            "         0.0261, -0.0287,  0.0122, -0.0184, -0.0180,  0.0064,  0.0108,  0.0250,\n",
            "        -0.0089,  0.0047,  0.0245, -0.0049, -0.0156,  0.0221, -0.0214, -0.0277,\n",
            "         0.0281, -0.0082,  0.0172,  0.0211, -0.0116, -0.0221,  0.0011,  0.0271,\n",
            "        -0.0081, -0.0085, -0.0094, -0.0081,  0.0286, -0.0142, -0.0270,  0.0131,\n",
            "        -0.0039,  0.0069,  0.0065, -0.0063, -0.0141,  0.0204,  0.0224, -0.0151],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "N2_D1.weight Parameter containing:\n",
            "tensor([[ 2.4536e-03,  2.2222e-03,  8.0004e-05,  ..., -1.7189e-03,\n",
            "          3.7672e-04,  6.4686e-04],\n",
            "        [-1.8015e-03,  1.6396e-03,  1.1643e-04,  ..., -2.1851e-03,\n",
            "          2.7415e-03,  2.4166e-03],\n",
            "        [-6.1342e-04,  1.8467e-03,  3.8234e-04,  ..., -2.7104e-03,\n",
            "         -6.6150e-04, -2.2750e-03],\n",
            "        ...,\n",
            "        [-1.3594e-03,  2.2981e-03,  2.3157e-03,  ...,  2.4002e-03,\n",
            "         -1.6155e-03,  1.6008e-03],\n",
            "        [ 1.6782e-03,  2.3875e-03,  2.1182e-03,  ..., -1.0080e-03,\n",
            "          1.4187e-04,  2.0821e-03],\n",
            "        [-9.8886e-04, -4.9304e-04,  2.0161e-03,  ...,  2.7328e-03,\n",
            "          1.9217e-03, -3.7604e-04]], device='cuda:0', requires_grad=True)\n",
            "N2_D1.bias Parameter containing:\n",
            "tensor([ 2.0141e-03,  1.3051e-03, -6.5517e-04, -1.0302e-03, -2.1088e-03,\n",
            "        -2.6833e-03, -2.6688e-03, -1.7081e-03, -2.2935e-03, -6.1256e-04,\n",
            "        -2.4479e-03, -1.3640e-03,  2.1083e-03,  2.7320e-03, -4.0296e-05,\n",
            "         2.0057e-03,  2.3247e-03,  2.1355e-04, -1.3956e-03, -1.5677e-03,\n",
            "         2.3106e-03, -1.7177e-04, -6.0970e-04, -1.4758e-03,  2.5509e-03,\n",
            "         2.3712e-03, -1.3542e-03,  2.6383e-03, -1.9184e-03,  3.3045e-04,\n",
            "        -2.6471e-03,  2.4566e-03,  1.7707e-03,  1.0866e-03, -1.2713e-03,\n",
            "        -3.4732e-04,  8.8777e-04, -1.7947e-04,  2.6929e-03,  2.4575e-03,\n",
            "        -1.1955e-03, -1.0550e-03, -2.6678e-03, -5.6417e-05,  2.1283e-03,\n",
            "        -2.2932e-03,  2.3626e-03,  1.3360e-03,  6.1900e-04,  5.0774e-04,\n",
            "        -7.3697e-05, -2.3220e-03,  1.5369e-03,  4.5875e-04, -1.8032e-03,\n",
            "        -1.7618e-04,  1.6662e-03, -2.4716e-03,  5.1413e-04, -1.4701e-03,\n",
            "         5.3089e-04, -2.5814e-03,  2.5969e-03,  2.4294e-04, -1.5948e-03,\n",
            "        -4.7022e-05,  2.4837e-03,  1.8827e-03, -2.7229e-03, -1.6333e-03,\n",
            "        -1.2172e-03, -8.1095e-04,  2.1233e-03, -1.7493e-03, -1.0726e-04,\n",
            "         1.8228e-03,  1.1752e-03, -2.1465e-03, -1.4527e-03,  6.9293e-04,\n",
            "         2.2636e-03,  6.6714e-04,  2.2816e-03,  2.7560e-03,  1.0259e-03,\n",
            "         2.7445e-03, -6.8191e-04, -1.3280e-03,  3.5246e-04,  8.7752e-04,\n",
            "         4.1938e-04, -4.3437e-04, -2.5514e-03, -9.4640e-04,  9.1219e-04,\n",
            "         4.6513e-04,  2.4664e-03,  1.7291e-03, -2.4677e-03, -2.9188e-05,\n",
            "         2.2228e-03,  2.6860e-03,  2.1353e-04,  7.6689e-04,  6.5679e-05,\n",
            "         1.4407e-03,  2.6844e-03, -2.5244e-03,  2.7194e-03,  4.9753e-04,\n",
            "        -1.1694e-03,  1.0155e-03, -1.2513e-03,  4.3793e-04, -2.3984e-04,\n",
            "         1.2339e-03,  1.1677e-03, -2.5666e-03,  7.3597e-05, -4.0814e-04,\n",
            "        -7.5760e-04,  9.2105e-04, -3.0272e-04,  3.1720e-04,  1.1164e-03,\n",
            "         7.4154e-04,  1.1039e-03,  2.2778e-04, -5.0626e-04,  9.9434e-04,\n",
            "         4.2273e-04, -2.2524e-04,  1.5244e-04,  1.4243e-03, -2.2416e-03,\n",
            "         2.3328e-03,  1.7140e-03,  2.2082e-03, -2.6811e-03, -9.6654e-04,\n",
            "         1.7576e-03, -2.7598e-04,  1.0345e-03,  1.4120e-03, -5.5726e-04,\n",
            "        -8.9680e-04,  8.2907e-04,  1.8753e-03, -8.0066e-04,  1.0474e-03,\n",
            "        -1.0118e-03,  1.5999e-03, -6.0197e-04, -2.0183e-03, -2.7921e-04,\n",
            "        -6.0939e-04, -1.4124e-04,  2.3940e-03,  6.3232e-04, -1.7456e-03,\n",
            "         2.6540e-03,  4.2652e-04, -1.8505e-03,  1.6481e-03, -2.2308e-03,\n",
            "        -2.3387e-03, -4.8033e-05, -2.2836e-03,  1.8273e-03, -2.2781e-04,\n",
            "         1.8837e-03, -6.0818e-04,  5.6497e-04,  1.4428e-03,  1.2082e-03,\n",
            "        -8.8641e-04, -2.6025e-03,  3.5849e-04, -6.0860e-04, -1.0280e-03,\n",
            "        -6.3791e-04, -2.6645e-04,  1.5479e-03,  1.4573e-03,  1.8856e-03,\n",
            "        -1.4610e-03, -1.4303e-03,  2.2273e-04, -4.9616e-04, -1.5390e-03,\n",
            "         1.3128e-03,  2.3298e-03,  2.6536e-04, -2.0736e-03,  1.7052e-03,\n",
            "        -2.1047e-03,  1.5212e-03,  1.8618e-03, -6.9653e-04, -1.2382e-03,\n",
            "        -7.0889e-04,  2.6492e-03,  6.3019e-04,  7.2117e-04, -1.0767e-03,\n",
            "         1.7747e-03,  1.9380e-03, -6.6208e-04,  1.7717e-03,  2.1400e-03,\n",
            "        -7.5901e-04,  2.0075e-03, -2.0101e-03, -1.0262e-03,  1.3106e-03,\n",
            "         7.1158e-04,  1.9428e-03,  1.6448e-03, -1.1298e-03,  3.9570e-05,\n",
            "         2.0196e-04,  8.3856e-04,  2.0556e-03, -1.1921e-04, -1.1479e-03,\n",
            "         1.1449e-03,  1.3362e-03, -4.6807e-04,  1.1662e-03, -7.9076e-04,\n",
            "         7.0761e-04, -1.5940e-03,  2.5693e-03, -1.2726e-03,  1.9806e-03,\n",
            "         1.3790e-03, -9.9193e-04,  7.5753e-04,  1.9252e-03, -8.0542e-04,\n",
            "        -8.8500e-04,  1.4393e-03, -1.2260e-03,  4.9930e-04, -1.3808e-03,\n",
            "         1.7437e-03,  1.2999e-03,  1.9781e-03,  7.0358e-04,  2.4743e-03,\n",
            "        -1.8291e-04,  2.5422e-03,  8.2705e-05, -6.1309e-05, -2.7169e-04,\n",
            "         1.4140e-03], device='cuda:0', requires_grad=True)\n",
            "N2_D2.weight Parameter containing:\n",
            "tensor([[ 0.0560,  0.0379, -0.0072,  ...,  0.0560, -0.0076, -0.0552],\n",
            "        [ 0.0157,  0.0608,  0.0539,  ..., -0.0485,  0.0403, -0.0390],\n",
            "        [-0.0145, -0.0561,  0.0016,  ..., -0.0375, -0.0612, -0.0181],\n",
            "        ...,\n",
            "        [ 0.0303, -0.0399, -0.0010,  ...,  0.0076, -0.0044,  0.0545],\n",
            "        [-0.0398, -0.0180,  0.0341,  ..., -0.0530,  0.0032, -0.0227],\n",
            "        [-0.0377,  0.0466, -0.0588,  ...,  0.0597,  0.0032,  0.0589]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "N2_D2.bias Parameter containing:\n",
            "tensor([-0.0480, -0.0466, -0.0068, -0.0081,  0.0177, -0.0155, -0.0186, -0.0124,\n",
            "         0.0274,  0.0535,  0.0375,  0.0354,  0.0338, -0.0126, -0.0199, -0.0527,\n",
            "         0.0417, -0.0169,  0.0048, -0.0329,  0.0395, -0.0418, -0.0484, -0.0260,\n",
            "         0.0508, -0.0153, -0.0352,  0.0026,  0.0024, -0.0037, -0.0121,  0.0247,\n",
            "        -0.0287, -0.0563,  0.0319, -0.0407, -0.0492, -0.0599, -0.0420, -0.0560,\n",
            "         0.0260, -0.0597,  0.0106, -0.0206, -0.0307,  0.0299,  0.0609,  0.0479,\n",
            "         0.0096,  0.0403, -0.0473, -0.0423, -0.0247, -0.0045,  0.0174,  0.0200,\n",
            "         0.0508, -0.0587, -0.0041, -0.0142,  0.0328,  0.0448,  0.0136,  0.0136,\n",
            "         0.0528,  0.0278, -0.0288,  0.0610, -0.0486, -0.0120, -0.0565,  0.0028,\n",
            "         0.0286,  0.0247, -0.0421, -0.0078, -0.0262,  0.0070,  0.0170,  0.0115,\n",
            "         0.0598,  0.0228, -0.0368, -0.0302, -0.0014, -0.0526, -0.0446,  0.0242,\n",
            "        -0.0610, -0.0543, -0.0038,  0.0490, -0.0093, -0.0592, -0.0224,  0.0225,\n",
            "         0.0416,  0.0604, -0.0389, -0.0388, -0.0053, -0.0120, -0.0625,  0.0142,\n",
            "         0.0366,  0.0186, -0.0034, -0.0454,  0.0576, -0.0376,  0.0443, -0.0338,\n",
            "        -0.0111,  0.0219,  0.0078,  0.0119, -0.0419, -0.0233,  0.0107,  0.0178,\n",
            "         0.0353,  0.0570,  0.0486, -0.0541, -0.0051, -0.0580,  0.0294,  0.0004,\n",
            "         0.0140,  0.0449, -0.0152,  0.0232,  0.0114,  0.0028,  0.0020, -0.0545,\n",
            "         0.0606, -0.0344,  0.0500,  0.0211, -0.0226,  0.0260, -0.0107, -0.0443,\n",
            "        -0.0186,  0.0612,  0.0616,  0.0128, -0.0012, -0.0181,  0.0622,  0.0419,\n",
            "        -0.0114, -0.0464,  0.0468,  0.0624,  0.0419, -0.0208,  0.0292,  0.0303,\n",
            "        -0.0524, -0.0519,  0.0203, -0.0521,  0.0292,  0.0022, -0.0380, -0.0024,\n",
            "         0.0298,  0.0314,  0.0469, -0.0004, -0.0355, -0.0447,  0.0613, -0.0205,\n",
            "        -0.0611,  0.0362,  0.0118,  0.0235, -0.0325,  0.0214,  0.0039, -0.0460,\n",
            "         0.0464,  0.0173,  0.0044,  0.0332, -0.0172,  0.0617,  0.0513, -0.0046,\n",
            "         0.0442,  0.0138, -0.0600, -0.0043, -0.0196, -0.0090,  0.0470,  0.0496,\n",
            "        -0.0483,  0.0318, -0.0517,  0.0611,  0.0228,  0.0590, -0.0451,  0.0204,\n",
            "        -0.0561,  0.0028,  0.0416,  0.0016, -0.0334, -0.0067,  0.0430,  0.0122,\n",
            "        -0.0395,  0.0546, -0.0515, -0.0262,  0.0260,  0.0325, -0.0255, -0.0010,\n",
            "        -0.0189,  0.0121,  0.0125, -0.0252,  0.0313, -0.0288, -0.0300,  0.0331,\n",
            "        -0.0205,  0.0189, -0.0592, -0.0590,  0.0432, -0.0043,  0.0465, -0.0028,\n",
            "        -0.0105,  0.0540,  0.0401,  0.0143, -0.0442, -0.0210,  0.0101,  0.0566,\n",
            "         0.0622,  0.0219,  0.0323,  0.0417, -0.0377, -0.0584,  0.0359,  0.0025],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "outputs.weight Parameter containing:\n",
            "tensor([[-0.0611, -0.0595,  0.0130,  ...,  0.0593,  0.0199, -0.0611],\n",
            "        [-0.0308, -0.0124, -0.0070,  ...,  0.0618, -0.0509, -0.0332],\n",
            "        [-0.0554,  0.0586,  0.0396,  ...,  0.0471,  0.0182, -0.0520],\n",
            "        ...,\n",
            "        [-0.0335, -0.0299,  0.0330,  ..., -0.0391,  0.0591,  0.0079],\n",
            "        [ 0.0488,  0.0556, -0.0508,  ...,  0.0161,  0.0378, -0.0610],\n",
            "        [ 0.0403, -0.0405,  0.0587,  ..., -0.0385, -0.0493,  0.0620]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "outputs.bias Parameter containing:\n",
            "tensor([ 0.0305,  0.0343, -0.0474,  0.0407, -0.0427, -0.0057,  0.0512, -0.0527,\n",
            "        -0.0526,  0.0618], device='cuda:0', requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "      print(name)"
      ],
      "metadata": {
        "id": "DpVBDSUgSeFh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8edcf5ad-06f0-4284-b814-ba103d54ab56"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N2_C1.weight\n",
            "N2_C1.bias\n",
            "N2_C2.weight\n",
            "N2_C2.bias\n",
            "N2_C3.weight\n",
            "N2_C3.bias\n",
            "N2_C4.weight\n",
            "N2_C4.bias\n",
            "N2_D1.weight\n",
            "N2_D1.bias\n",
            "N2_D2.weight\n",
            "N2_D2.bias\n",
            "outputs.weight\n",
            "outputs.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "for epoch in range(1, 81):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "id": "jyxWSo8cSiA7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdfe94e0-3443-43ad-f6cd-7f7520226dda"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [3124/50000 (100%)]\tLoss: 3078.861254\n",
            "\n",
            "Test set: Average loss: 518.1368, Accuracy: 4045/10000 (40%)\n",
            "\n",
            "Train Epoch: 2 [3124/50000 (100%)]\tLoss: 2395.505561\n",
            "\n",
            "Test set: Average loss: 443.6495, Accuracy: 4906/10000 (49%)\n",
            "\n",
            "Train Epoch: 3 [3124/50000 (100%)]\tLoss: 2107.911940\n",
            "\n",
            "Test set: Average loss: 403.6222, Accuracy: 5375/10000 (54%)\n",
            "\n",
            "Train Epoch: 4 [3124/50000 (100%)]\tLoss: 1888.841750\n",
            "\n",
            "Test set: Average loss: 370.3296, Accuracy: 5779/10000 (58%)\n",
            "\n",
            "Train Epoch: 5 [3124/50000 (100%)]\tLoss: 1687.698318\n",
            "\n",
            "Test set: Average loss: 353.6449, Accuracy: 5992/10000 (60%)\n",
            "\n",
            "Train Epoch: 6 [3124/50000 (100%)]\tLoss: 1492.584705\n",
            "\n",
            "Test set: Average loss: 345.0158, Accuracy: 5978/10000 (60%)\n",
            "\n",
            "Train Epoch: 7 [3124/50000 (100%)]\tLoss: 1303.555784\n",
            "\n",
            "Test set: Average loss: 315.3890, Accuracy: 6509/10000 (65%)\n",
            "\n",
            "Train Epoch: 8 [3124/50000 (100%)]\tLoss: 1106.454641\n",
            "\n",
            "Test set: Average loss: 340.1308, Accuracy: 6380/10000 (64%)\n",
            "\n",
            "Train Epoch: 9 [3124/50000 (100%)]\tLoss: 889.413151\n",
            "\n",
            "Test set: Average loss: 342.7491, Accuracy: 6480/10000 (65%)\n",
            "\n",
            "Train Epoch: 10 [3124/50000 (100%)]\tLoss: 670.861526\n",
            "\n",
            "Test set: Average loss: 406.1424, Accuracy: 6180/10000 (62%)\n",
            "\n",
            "Train Epoch: 11 [3124/50000 (100%)]\tLoss: 466.093332\n",
            "\n",
            "Test set: Average loss: 398.5691, Accuracy: 6371/10000 (64%)\n",
            "\n",
            "Train Epoch: 12 [3124/50000 (100%)]\tLoss: 305.838949\n",
            "\n",
            "Test set: Average loss: 521.2490, Accuracy: 6362/10000 (64%)\n",
            "\n",
            "Train Epoch: 13 [3124/50000 (100%)]\tLoss: 212.663187\n",
            "\n",
            "Test set: Average loss: 514.5104, Accuracy: 6489/10000 (65%)\n",
            "\n",
            "Train Epoch: 14 [3124/50000 (100%)]\tLoss: 154.010729\n",
            "\n",
            "Test set: Average loss: 539.0383, Accuracy: 6544/10000 (65%)\n",
            "\n",
            "Train Epoch: 15 [3124/50000 (100%)]\tLoss: 109.146928\n",
            "\n",
            "Test set: Average loss: 626.8196, Accuracy: 6518/10000 (65%)\n",
            "\n",
            "Train Epoch: 16 [3124/50000 (100%)]\tLoss: 81.518784\n",
            "\n",
            "Test set: Average loss: 644.7286, Accuracy: 6579/10000 (66%)\n",
            "\n",
            "Train Epoch: 17 [3124/50000 (100%)]\tLoss: 65.211284\n",
            "\n",
            "Test set: Average loss: 703.1748, Accuracy: 6493/10000 (65%)\n",
            "\n",
            "Train Epoch: 18 [3124/50000 (100%)]\tLoss: 55.861641\n",
            "\n",
            "Test set: Average loss: 691.9532, Accuracy: 6647/10000 (66%)\n",
            "\n",
            "Train Epoch: 19 [3124/50000 (100%)]\tLoss: 42.622738\n",
            "\n",
            "Test set: Average loss: 743.8734, Accuracy: 6489/10000 (65%)\n",
            "\n",
            "Train Epoch: 20 [3124/50000 (100%)]\tLoss: 32.998879\n",
            "\n",
            "Test set: Average loss: 765.2744, Accuracy: 6557/10000 (66%)\n",
            "\n",
            "Train Epoch: 21 [3124/50000 (100%)]\tLoss: 34.016603\n",
            "\n",
            "Test set: Average loss: 797.5605, Accuracy: 6582/10000 (66%)\n",
            "\n",
            "Train Epoch: 22 [3124/50000 (100%)]\tLoss: 28.217220\n",
            "\n",
            "Test set: Average loss: 834.6671, Accuracy: 6524/10000 (65%)\n",
            "\n",
            "Train Epoch: 23 [3124/50000 (100%)]\tLoss: 31.369297\n",
            "\n",
            "Test set: Average loss: 822.7095, Accuracy: 6660/10000 (67%)\n",
            "\n",
            "Train Epoch: 24 [3124/50000 (100%)]\tLoss: 11.254739\n",
            "\n",
            "Test set: Average loss: 837.0827, Accuracy: 6621/10000 (66%)\n",
            "\n",
            "Train Epoch: 25 [3124/50000 (100%)]\tLoss: 12.350353\n",
            "\n",
            "Test set: Average loss: 869.6308, Accuracy: 6630/10000 (66%)\n",
            "\n",
            "Train Epoch: 26 [3124/50000 (100%)]\tLoss: 6.457505\n",
            "\n",
            "Test set: Average loss: 901.3768, Accuracy: 6670/10000 (67%)\n",
            "\n",
            "Train Epoch: 27 [3124/50000 (100%)]\tLoss: 17.268641\n",
            "\n",
            "Test set: Average loss: 892.7358, Accuracy: 6513/10000 (65%)\n",
            "\n",
            "Train Epoch: 28 [3124/50000 (100%)]\tLoss: 26.232041\n",
            "\n",
            "Test set: Average loss: 823.3445, Accuracy: 6485/10000 (65%)\n",
            "\n",
            "Train Epoch: 29 [3124/50000 (100%)]\tLoss: 25.182871\n",
            "\n",
            "Test set: Average loss: 832.3903, Accuracy: 6534/10000 (65%)\n",
            "\n",
            "Train Epoch: 30 [3124/50000 (100%)]\tLoss: 17.271391\n",
            "\n",
            "Test set: Average loss: 853.7899, Accuracy: 6678/10000 (67%)\n",
            "\n",
            "Train Epoch: 31 [3124/50000 (100%)]\tLoss: 7.197364\n",
            "\n",
            "Test set: Average loss: 915.7567, Accuracy: 6610/10000 (66%)\n",
            "\n",
            "Train Epoch: 32 [3124/50000 (100%)]\tLoss: 15.132840\n",
            "\n",
            "Test set: Average loss: 891.9252, Accuracy: 6667/10000 (67%)\n",
            "\n",
            "Train Epoch: 33 [3124/50000 (100%)]\tLoss: 9.349711\n",
            "\n",
            "Test set: Average loss: 931.4025, Accuracy: 6631/10000 (66%)\n",
            "\n",
            "Train Epoch: 34 [3124/50000 (100%)]\tLoss: 1.080635\n",
            "\n",
            "Test set: Average loss: 934.7982, Accuracy: 6691/10000 (67%)\n",
            "\n",
            "Train Epoch: 35 [3124/50000 (100%)]\tLoss: 0.474119\n",
            "\n",
            "Test set: Average loss: 944.9737, Accuracy: 6692/10000 (67%)\n",
            "\n",
            "Train Epoch: 36 [3124/50000 (100%)]\tLoss: 0.124805\n",
            "\n",
            "Test set: Average loss: 962.3826, Accuracy: 6695/10000 (67%)\n",
            "\n",
            "Train Epoch: 37 [3124/50000 (100%)]\tLoss: 0.089669\n",
            "\n",
            "Test set: Average loss: 975.8499, Accuracy: 6701/10000 (67%)\n",
            "\n",
            "Train Epoch: 38 [3124/50000 (100%)]\tLoss: 0.073884\n",
            "\n",
            "Test set: Average loss: 986.9651, Accuracy: 6701/10000 (67%)\n",
            "\n",
            "Train Epoch: 39 [3124/50000 (100%)]\tLoss: 0.063515\n",
            "\n",
            "Test set: Average loss: 996.9457, Accuracy: 6701/10000 (67%)\n",
            "\n",
            "Train Epoch: 40 [3124/50000 (100%)]\tLoss: 0.056012\n",
            "\n",
            "Test set: Average loss: 1005.3535, Accuracy: 6702/10000 (67%)\n",
            "\n",
            "Train Epoch: 41 [3124/50000 (100%)]\tLoss: 0.050326\n",
            "\n",
            "Test set: Average loss: 1013.0283, Accuracy: 6701/10000 (67%)\n",
            "\n",
            "Train Epoch: 42 [3124/50000 (100%)]\tLoss: 0.045766\n",
            "\n",
            "Test set: Average loss: 1019.8278, Accuracy: 6697/10000 (67%)\n",
            "\n",
            "Train Epoch: 43 [3124/50000 (100%)]\tLoss: 0.041982\n",
            "\n",
            "Test set: Average loss: 1026.1675, Accuracy: 6701/10000 (67%)\n",
            "\n",
            "Train Epoch: 44 [3124/50000 (100%)]\tLoss: 0.038828\n",
            "\n",
            "Test set: Average loss: 1031.9537, Accuracy: 6698/10000 (67%)\n",
            "\n",
            "Train Epoch: 45 [3124/50000 (100%)]\tLoss: 0.036129\n",
            "\n",
            "Test set: Average loss: 1037.2380, Accuracy: 6703/10000 (67%)\n",
            "\n",
            "Train Epoch: 46 [3124/50000 (100%)]\tLoss: 0.033803\n",
            "\n",
            "Test set: Average loss: 1042.3958, Accuracy: 6703/10000 (67%)\n",
            "\n",
            "Train Epoch: 47 [3124/50000 (100%)]\tLoss: 0.031766\n",
            "\n",
            "Test set: Average loss: 1047.2834, Accuracy: 6707/10000 (67%)\n",
            "\n",
            "Train Epoch: 48 [3124/50000 (100%)]\tLoss: 0.029959\n",
            "\n",
            "Test set: Average loss: 1051.8779, Accuracy: 6711/10000 (67%)\n",
            "\n",
            "Train Epoch: 49 [3124/50000 (100%)]\tLoss: 0.028374\n",
            "\n",
            "Test set: Average loss: 1056.1029, Accuracy: 6712/10000 (67%)\n",
            "\n",
            "Train Epoch: 50 [3124/50000 (100%)]\tLoss: 0.026928\n",
            "\n",
            "Test set: Average loss: 1060.2367, Accuracy: 6713/10000 (67%)\n",
            "\n",
            "Train Epoch: 51 [3124/50000 (100%)]\tLoss: 0.025657\n",
            "\n",
            "Test set: Average loss: 1064.0452, Accuracy: 6720/10000 (67%)\n",
            "\n",
            "Train Epoch: 52 [3124/50000 (100%)]\tLoss: 0.024507\n",
            "\n",
            "Test set: Average loss: 1067.6855, Accuracy: 6721/10000 (67%)\n",
            "\n",
            "Train Epoch: 53 [3124/50000 (100%)]\tLoss: 0.023438\n",
            "\n",
            "Test set: Average loss: 1071.1965, Accuracy: 6721/10000 (67%)\n",
            "\n",
            "Train Epoch: 54 [3124/50000 (100%)]\tLoss: 0.022446\n",
            "\n",
            "Test set: Average loss: 1074.6888, Accuracy: 6720/10000 (67%)\n",
            "\n",
            "Train Epoch: 55 [3124/50000 (100%)]\tLoss: 0.021558\n",
            "\n",
            "Test set: Average loss: 1078.0336, Accuracy: 6722/10000 (67%)\n",
            "\n",
            "Train Epoch: 56 [3124/50000 (100%)]\tLoss: 0.020716\n",
            "\n",
            "Test set: Average loss: 1081.1927, Accuracy: 6723/10000 (67%)\n",
            "\n",
            "Train Epoch: 57 [3124/50000 (100%)]\tLoss: 0.019953\n",
            "\n",
            "Test set: Average loss: 1084.2560, Accuracy: 6723/10000 (67%)\n",
            "\n",
            "Train Epoch: 58 [3124/50000 (100%)]\tLoss: 0.019246\n",
            "\n",
            "Test set: Average loss: 1087.2076, Accuracy: 6726/10000 (67%)\n",
            "\n",
            "Train Epoch: 59 [3124/50000 (100%)]\tLoss: 0.018593\n",
            "\n",
            "Test set: Average loss: 1090.0480, Accuracy: 6725/10000 (67%)\n",
            "\n",
            "Train Epoch: 60 [3124/50000 (100%)]\tLoss: 0.017973\n",
            "\n",
            "Test set: Average loss: 1092.8262, Accuracy: 6726/10000 (67%)\n",
            "\n",
            "Train Epoch: 61 [3124/50000 (100%)]\tLoss: 0.017394\n",
            "\n",
            "Test set: Average loss: 1095.5195, Accuracy: 6725/10000 (67%)\n",
            "\n",
            "Train Epoch: 62 [3124/50000 (100%)]\tLoss: 0.016847\n",
            "\n",
            "Test set: Average loss: 1098.1080, Accuracy: 6727/10000 (67%)\n",
            "\n",
            "Train Epoch: 63 [3124/50000 (100%)]\tLoss: 0.016347\n",
            "\n",
            "Test set: Average loss: 1100.6110, Accuracy: 6728/10000 (67%)\n",
            "\n",
            "Train Epoch: 64 [3124/50000 (100%)]\tLoss: 0.015857\n",
            "\n",
            "Test set: Average loss: 1103.1230, Accuracy: 6728/10000 (67%)\n",
            "\n",
            "Train Epoch: 65 [3124/50000 (100%)]\tLoss: 0.015412\n",
            "\n",
            "Test set: Average loss: 1105.4902, Accuracy: 6728/10000 (67%)\n",
            "\n",
            "Train Epoch: 66 [3124/50000 (100%)]\tLoss: 0.014992\n",
            "\n",
            "Test set: Average loss: 1107.8158, Accuracy: 6728/10000 (67%)\n",
            "\n",
            "Train Epoch: 67 [3124/50000 (100%)]\tLoss: 0.014587\n",
            "\n",
            "Test set: Average loss: 1110.0693, Accuracy: 6729/10000 (67%)\n",
            "\n",
            "Train Epoch: 68 [3124/50000 (100%)]\tLoss: 0.014208\n",
            "\n",
            "Test set: Average loss: 1112.3019, Accuracy: 6728/10000 (67%)\n",
            "\n",
            "Train Epoch: 69 [3124/50000 (100%)]\tLoss: 0.013838\n",
            "\n",
            "Test set: Average loss: 1114.4735, Accuracy: 6729/10000 (67%)\n",
            "\n",
            "Train Epoch: 70 [3124/50000 (100%)]\tLoss: 0.013498\n",
            "\n",
            "Test set: Average loss: 1116.5836, Accuracy: 6729/10000 (67%)\n",
            "\n",
            "Train Epoch: 71 [3124/50000 (100%)]\tLoss: 0.013174\n",
            "\n",
            "Test set: Average loss: 1118.6410, Accuracy: 6729/10000 (67%)\n",
            "\n",
            "Train Epoch: 72 [3124/50000 (100%)]\tLoss: 0.012853\n",
            "\n",
            "Test set: Average loss: 1120.6345, Accuracy: 6727/10000 (67%)\n",
            "\n",
            "Train Epoch: 73 [3124/50000 (100%)]\tLoss: 0.012554\n",
            "\n",
            "Test set: Average loss: 1122.5621, Accuracy: 6727/10000 (67%)\n",
            "\n",
            "Train Epoch: 74 [3124/50000 (100%)]\tLoss: 0.012277\n",
            "\n",
            "Test set: Average loss: 1124.4915, Accuracy: 6726/10000 (67%)\n",
            "\n",
            "Train Epoch: 75 [3124/50000 (100%)]\tLoss: 0.012001\n",
            "\n",
            "Test set: Average loss: 1126.3695, Accuracy: 6732/10000 (67%)\n",
            "\n",
            "Train Epoch: 76 [3124/50000 (100%)]\tLoss: 0.011740\n",
            "\n",
            "Test set: Average loss: 1128.2214, Accuracy: 6732/10000 (67%)\n",
            "\n",
            "Train Epoch: 77 [3124/50000 (100%)]\tLoss: 0.011494\n",
            "\n",
            "Test set: Average loss: 1130.0219, Accuracy: 6732/10000 (67%)\n",
            "\n",
            "Train Epoch: 78 [3124/50000 (100%)]\tLoss: 0.011249\n",
            "\n",
            "Test set: Average loss: 1131.7938, Accuracy: 6732/10000 (67%)\n",
            "\n",
            "Train Epoch: 79 [3124/50000 (100%)]\tLoss: 0.011022\n",
            "\n",
            "Test set: Average loss: 1133.5228, Accuracy: 6732/10000 (67%)\n",
            "\n",
            "Train Epoch: 80 [3124/50000 (100%)]\tLoss: 0.010802\n",
            "\n",
            "Test set: Average loss: 1135.2095, Accuracy: 6732/10000 (67%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FLNPF"
      ],
      "metadata": {
        "id": "13yfVAAcSnXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Conv4Relu()\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "\n",
        "for epoch in range(1, 81):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "id": "zqXeJHemSoO6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6252ee59-fb39-45db-e592-7fe6b0dafe58"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [3124/50000 (100%)]\tLoss: 3086.428401\n",
            "\n",
            "Test set: Average loss: 511.7622, Accuracy: 4050/10000 (40%)\n",
            "\n",
            "Train Epoch: 2 [3124/50000 (100%)]\tLoss: 2339.563638\n",
            "\n",
            "Test set: Average loss: 439.9502, Accuracy: 5020/10000 (50%)\n",
            "\n",
            "Train Epoch: 3 [3124/50000 (100%)]\tLoss: 2062.114802\n",
            "\n",
            "Test set: Average loss: 412.6857, Accuracy: 5248/10000 (52%)\n",
            "\n",
            "Train Epoch: 4 [3124/50000 (100%)]\tLoss: 1841.932782\n",
            "\n",
            "Test set: Average loss: 368.9293, Accuracy: 5791/10000 (58%)\n",
            "\n",
            "Train Epoch: 5 [3124/50000 (100%)]\tLoss: 1645.002154\n",
            "\n",
            "Test set: Average loss: 350.4777, Accuracy: 5939/10000 (59%)\n",
            "\n",
            "Train Epoch: 6 [3124/50000 (100%)]\tLoss: 1452.582220\n",
            "\n",
            "Test set: Average loss: 338.5309, Accuracy: 6207/10000 (62%)\n",
            "\n",
            "Train Epoch: 7 [3124/50000 (100%)]\tLoss: 1260.276534\n",
            "\n",
            "Test set: Average loss: 334.2257, Accuracy: 6369/10000 (64%)\n",
            "\n",
            "Train Epoch: 8 [3124/50000 (100%)]\tLoss: 1057.363025\n",
            "\n",
            "Test set: Average loss: 341.2044, Accuracy: 6378/10000 (64%)\n",
            "\n",
            "Train Epoch: 9 [3124/50000 (100%)]\tLoss: 838.496314\n",
            "\n",
            "Test set: Average loss: 365.6265, Accuracy: 6372/10000 (64%)\n",
            "\n",
            "Train Epoch: 10 [3124/50000 (100%)]\tLoss: 619.543868\n",
            "\n",
            "Test set: Average loss: 435.1526, Accuracy: 6205/10000 (62%)\n",
            "\n",
            "Train Epoch: 11 [3124/50000 (100%)]\tLoss: 422.162632\n",
            "\n",
            "Test set: Average loss: 429.8204, Accuracy: 6471/10000 (65%)\n",
            "\n",
            "Train Epoch: 12 [3124/50000 (100%)]\tLoss: 274.644968\n",
            "\n",
            "Test set: Average loss: 499.3895, Accuracy: 6372/10000 (64%)\n",
            "\n",
            "Train Epoch: 13 [3124/50000 (100%)]\tLoss: 197.003471\n",
            "\n",
            "Test set: Average loss: 533.0242, Accuracy: 6369/10000 (64%)\n",
            "\n",
            "Train Epoch: 14 [3124/50000 (100%)]\tLoss: 142.523386\n",
            "\n",
            "Test set: Average loss: 557.1494, Accuracy: 6659/10000 (67%)\n",
            "\n",
            "Train Epoch: 15 [3124/50000 (100%)]\tLoss: 100.321773\n",
            "\n",
            "Test set: Average loss: 768.2534, Accuracy: 6206/10000 (62%)\n",
            "\n",
            "Train Epoch: 16 [3124/50000 (100%)]\tLoss: 76.665007\n",
            "\n",
            "Test set: Average loss: 633.0942, Accuracy: 6647/10000 (66%)\n",
            "\n",
            "Train Epoch: 17 [3124/50000 (100%)]\tLoss: 63.071020\n",
            "\n",
            "Test set: Average loss: 674.3345, Accuracy: 6655/10000 (67%)\n",
            "\n",
            "Train Epoch: 18 [3124/50000 (100%)]\tLoss: 53.800168\n",
            "\n",
            "Test set: Average loss: 682.7591, Accuracy: 6641/10000 (66%)\n",
            "\n",
            "Train Epoch: 19 [3124/50000 (100%)]\tLoss: 34.111810\n",
            "\n",
            "Test set: Average loss: 732.9059, Accuracy: 6664/10000 (67%)\n",
            "\n",
            "Train Epoch: 20 [3124/50000 (100%)]\tLoss: 32.992953\n",
            "\n",
            "Test set: Average loss: 762.0076, Accuracy: 6607/10000 (66%)\n",
            "\n",
            "Train Epoch: 21 [3124/50000 (100%)]\tLoss: 32.942299\n",
            "\n",
            "Test set: Average loss: 702.6964, Accuracy: 6618/10000 (66%)\n",
            "\n",
            "Train Epoch: 22 [3124/50000 (100%)]\tLoss: 21.302264\n",
            "\n",
            "Test set: Average loss: 785.3149, Accuracy: 6699/10000 (67%)\n",
            "\n",
            "Train Epoch: 23 [3124/50000 (100%)]\tLoss: 7.631237\n",
            "\n",
            "Test set: Average loss: 938.8556, Accuracy: 6534/10000 (65%)\n",
            "\n",
            "Train Epoch: 24 [3124/50000 (100%)]\tLoss: 11.137115\n",
            "\n",
            "Test set: Average loss: 888.6257, Accuracy: 6660/10000 (67%)\n",
            "\n",
            "Train Epoch: 25 [3124/50000 (100%)]\tLoss: 11.817475\n",
            "\n",
            "Test set: Average loss: 867.4735, Accuracy: 6564/10000 (66%)\n",
            "\n",
            "Train Epoch: 26 [3124/50000 (100%)]\tLoss: 23.927186\n",
            "\n",
            "Test set: Average loss: 822.6674, Accuracy: 6726/10000 (67%)\n",
            "\n",
            "Train Epoch: 27 [3124/50000 (100%)]\tLoss: 18.093959\n",
            "\n",
            "Test set: Average loss: 852.0474, Accuracy: 6697/10000 (67%)\n",
            "\n",
            "Train Epoch: 28 [3124/50000 (100%)]\tLoss: 19.985963\n",
            "\n",
            "Test set: Average loss: 878.9527, Accuracy: 6620/10000 (66%)\n",
            "\n",
            "Train Epoch: 29 [3124/50000 (100%)]\tLoss: 9.333648\n",
            "\n",
            "Test set: Average loss: 898.9158, Accuracy: 6755/10000 (68%)\n",
            "\n",
            "Train Epoch: 30 [3124/50000 (100%)]\tLoss: 2.532150\n",
            "\n",
            "Test set: Average loss: 930.6689, Accuracy: 6736/10000 (67%)\n",
            "\n",
            "Train Epoch: 31 [3124/50000 (100%)]\tLoss: 0.545034\n",
            "\n",
            "Test set: Average loss: 946.4507, Accuracy: 6801/10000 (68%)\n",
            "\n",
            "Train Epoch: 32 [3124/50000 (100%)]\tLoss: 0.213719\n",
            "\n",
            "Test set: Average loss: 970.5881, Accuracy: 6826/10000 (68%)\n",
            "\n",
            "Train Epoch: 33 [3124/50000 (100%)]\tLoss: 0.120023\n",
            "\n",
            "Test set: Average loss: 976.8779, Accuracy: 6839/10000 (68%)\n",
            "\n",
            "Train Epoch: 34 [3124/50000 (100%)]\tLoss: 0.082470\n",
            "\n",
            "Test set: Average loss: 988.1895, Accuracy: 6841/10000 (68%)\n",
            "\n",
            "Train Epoch: 35 [3124/50000 (100%)]\tLoss: 0.069812\n",
            "\n",
            "Test set: Average loss: 998.1517, Accuracy: 6840/10000 (68%)\n",
            "\n",
            "Train Epoch: 36 [3124/50000 (100%)]\tLoss: 0.061384\n",
            "\n",
            "Test set: Average loss: 1006.4158, Accuracy: 6841/10000 (68%)\n",
            "\n",
            "Train Epoch: 37 [3124/50000 (100%)]\tLoss: 0.054990\n",
            "\n",
            "Test set: Average loss: 1014.2398, Accuracy: 6841/10000 (68%)\n",
            "\n",
            "Train Epoch: 38 [3124/50000 (100%)]\tLoss: 0.049949\n",
            "\n",
            "Test set: Average loss: 1021.1642, Accuracy: 6839/10000 (68%)\n",
            "\n",
            "Train Epoch: 39 [3124/50000 (100%)]\tLoss: 0.045879\n",
            "\n",
            "Test set: Average loss: 1027.6376, Accuracy: 6841/10000 (68%)\n",
            "\n",
            "Train Epoch: 40 [3124/50000 (100%)]\tLoss: 0.042426\n",
            "\n",
            "Test set: Average loss: 1033.5331, Accuracy: 6841/10000 (68%)\n",
            "\n",
            "Train Epoch: 41 [3124/50000 (100%)]\tLoss: 0.039458\n",
            "\n",
            "Test set: Average loss: 1039.0991, Accuracy: 6843/10000 (68%)\n",
            "\n",
            "Train Epoch: 42 [3124/50000 (100%)]\tLoss: 0.036933\n",
            "\n",
            "Test set: Average loss: 1044.1195, Accuracy: 6844/10000 (68%)\n",
            "\n",
            "Train Epoch: 43 [3124/50000 (100%)]\tLoss: 0.034714\n",
            "\n",
            "Test set: Average loss: 1049.1628, Accuracy: 6846/10000 (68%)\n",
            "\n",
            "Train Epoch: 44 [3124/50000 (100%)]\tLoss: 0.032738\n",
            "\n",
            "Test set: Average loss: 1053.9265, Accuracy: 6847/10000 (68%)\n",
            "\n",
            "Train Epoch: 45 [3124/50000 (100%)]\tLoss: 0.031013\n",
            "\n",
            "Test set: Average loss: 1058.3835, Accuracy: 6847/10000 (68%)\n",
            "\n",
            "Train Epoch: 46 [3124/50000 (100%)]\tLoss: 0.029461\n",
            "\n",
            "Test set: Average loss: 1062.5070, Accuracy: 6849/10000 (68%)\n",
            "\n",
            "Train Epoch: 47 [3124/50000 (100%)]\tLoss: 0.028036\n",
            "\n",
            "Test set: Average loss: 1066.4415, Accuracy: 6849/10000 (68%)\n",
            "\n",
            "Train Epoch: 48 [3124/50000 (100%)]\tLoss: 0.026776\n",
            "\n",
            "Test set: Average loss: 1070.1816, Accuracy: 6854/10000 (69%)\n",
            "\n",
            "Train Epoch: 49 [3124/50000 (100%)]\tLoss: 0.025615\n",
            "\n",
            "Test set: Average loss: 1073.8656, Accuracy: 6852/10000 (69%)\n",
            "\n",
            "Train Epoch: 50 [3124/50000 (100%)]\tLoss: 0.024553\n",
            "\n",
            "Test set: Average loss: 1077.3625, Accuracy: 6851/10000 (69%)\n",
            "\n",
            "Train Epoch: 51 [3124/50000 (100%)]\tLoss: 0.023573\n",
            "\n",
            "Test set: Average loss: 1080.6201, Accuracy: 6852/10000 (69%)\n",
            "\n",
            "Train Epoch: 52 [3124/50000 (100%)]\tLoss: 0.022648\n",
            "\n",
            "Test set: Average loss: 1084.0151, Accuracy: 6852/10000 (69%)\n",
            "\n",
            "Train Epoch: 53 [3124/50000 (100%)]\tLoss: 0.021857\n",
            "\n",
            "Test set: Average loss: 1087.0640, Accuracy: 6852/10000 (69%)\n",
            "\n",
            "Train Epoch: 54 [3124/50000 (100%)]\tLoss: 0.021063\n",
            "\n",
            "Test set: Average loss: 1090.0382, Accuracy: 6852/10000 (69%)\n",
            "\n",
            "Train Epoch: 55 [3124/50000 (100%)]\tLoss: 0.020374\n",
            "\n",
            "Test set: Average loss: 1093.0402, Accuracy: 6854/10000 (69%)\n",
            "\n",
            "Train Epoch: 56 [3124/50000 (100%)]\tLoss: 0.019691\n",
            "\n",
            "Test set: Average loss: 1095.8604, Accuracy: 6855/10000 (69%)\n",
            "\n",
            "Train Epoch: 57 [3124/50000 (100%)]\tLoss: 0.019095\n",
            "\n",
            "Test set: Average loss: 1098.6746, Accuracy: 6854/10000 (69%)\n",
            "\n",
            "Train Epoch: 58 [3124/50000 (100%)]\tLoss: 0.018480\n",
            "\n",
            "Test set: Average loss: 1101.2817, Accuracy: 6852/10000 (69%)\n",
            "\n",
            "Train Epoch: 59 [3124/50000 (100%)]\tLoss: 0.017916\n",
            "\n",
            "Test set: Average loss: 1103.9020, Accuracy: 6852/10000 (69%)\n",
            "\n",
            "Train Epoch: 60 [3124/50000 (100%)]\tLoss: 0.017410\n",
            "\n",
            "Test set: Average loss: 1106.3756, Accuracy: 6855/10000 (69%)\n",
            "\n",
            "Train Epoch: 61 [3124/50000 (100%)]\tLoss: 0.016908\n",
            "\n",
            "Test set: Average loss: 1108.8173, Accuracy: 6854/10000 (69%)\n",
            "\n",
            "Train Epoch: 62 [3124/50000 (100%)]\tLoss: 0.016449\n",
            "\n",
            "Test set: Average loss: 1111.1985, Accuracy: 6854/10000 (69%)\n",
            "\n",
            "Train Epoch: 63 [3124/50000 (100%)]\tLoss: 0.016014\n",
            "\n",
            "Test set: Average loss: 1113.4490, Accuracy: 6853/10000 (69%)\n",
            "\n",
            "Train Epoch: 64 [3124/50000 (100%)]\tLoss: 0.015605\n",
            "\n",
            "Test set: Average loss: 1115.7562, Accuracy: 6854/10000 (69%)\n",
            "\n",
            "Train Epoch: 65 [3124/50000 (100%)]\tLoss: 0.015190\n",
            "\n",
            "Test set: Average loss: 1118.0719, Accuracy: 6854/10000 (69%)\n",
            "\n",
            "Train Epoch: 66 [3124/50000 (100%)]\tLoss: 0.014827\n",
            "\n",
            "Test set: Average loss: 1120.1833, Accuracy: 6853/10000 (69%)\n",
            "\n",
            "Train Epoch: 67 [3124/50000 (100%)]\tLoss: 0.014467\n",
            "\n",
            "Test set: Average loss: 1122.2249, Accuracy: 6853/10000 (69%)\n",
            "\n",
            "Train Epoch: 68 [3124/50000 (100%)]\tLoss: 0.014133\n",
            "\n",
            "Test set: Average loss: 1124.2917, Accuracy: 6853/10000 (69%)\n",
            "\n",
            "Train Epoch: 69 [3124/50000 (100%)]\tLoss: 0.013806\n",
            "\n",
            "Test set: Average loss: 1126.2819, Accuracy: 6855/10000 (69%)\n",
            "\n",
            "Train Epoch: 70 [3124/50000 (100%)]\tLoss: 0.013498\n",
            "\n",
            "Test set: Average loss: 1128.2358, Accuracy: 6855/10000 (69%)\n",
            "\n",
            "Train Epoch: 71 [3124/50000 (100%)]\tLoss: 0.013200\n",
            "\n",
            "Test set: Average loss: 1130.1980, Accuracy: 6856/10000 (69%)\n",
            "\n",
            "Train Epoch: 72 [3124/50000 (100%)]\tLoss: 0.012910\n",
            "\n",
            "Test set: Average loss: 1132.0977, Accuracy: 6856/10000 (69%)\n",
            "\n",
            "Train Epoch: 73 [3124/50000 (100%)]\tLoss: 0.012651\n",
            "\n",
            "Test set: Average loss: 1133.8937, Accuracy: 6854/10000 (69%)\n",
            "\n",
            "Train Epoch: 74 [3124/50000 (100%)]\tLoss: 0.012388\n",
            "\n",
            "Test set: Average loss: 1135.7032, Accuracy: 6853/10000 (69%)\n",
            "\n",
            "Train Epoch: 75 [3124/50000 (100%)]\tLoss: 0.012139\n",
            "\n",
            "Test set: Average loss: 1137.4783, Accuracy: 6854/10000 (69%)\n",
            "\n",
            "Train Epoch: 76 [3124/50000 (100%)]\tLoss: 0.011903\n",
            "\n",
            "Test set: Average loss: 1139.1947, Accuracy: 6855/10000 (69%)\n",
            "\n",
            "Train Epoch: 77 [3124/50000 (100%)]\tLoss: 0.011668\n",
            "\n",
            "Test set: Average loss: 1140.8806, Accuracy: 6855/10000 (69%)\n",
            "\n",
            "Train Epoch: 78 [3124/50000 (100%)]\tLoss: 0.011447\n",
            "\n",
            "Test set: Average loss: 1142.5874, Accuracy: 6856/10000 (69%)\n",
            "\n",
            "Train Epoch: 79 [3124/50000 (100%)]\tLoss: 0.011228\n",
            "\n",
            "Test set: Average loss: 1144.2518, Accuracy: 6856/10000 (69%)\n",
            "\n",
            "Train Epoch: 80 [3124/50000 (100%)]\tLoss: 0.011055\n",
            "\n",
            "Test set: Average loss: 1145.8660, Accuracy: 6855/10000 (69%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relu_model = model\n",
        "for p in model.parameters():\n",
        "  print(p)"
      ],
      "metadata": {
        "id": "ivpd_pE0Stwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34253cdb-e1ad-4334-f833-a31062dc8f0f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[[[ 0.1850,  0.1115,  0.1831],\n",
            "          [-0.1391,  0.0578,  0.0505],\n",
            "          [-0.2225,  0.0643,  0.0665]],\n",
            "\n",
            "         [[ 0.2318,  0.0711,  0.2237],\n",
            "          [-0.2885, -0.1129, -0.1827],\n",
            "          [-0.2874, -0.0576,  0.1046]],\n",
            "\n",
            "         [[ 0.2614,  0.3017,  0.1691],\n",
            "          [-0.2203, -0.1606, -0.1812],\n",
            "          [-0.2189, -0.2360,  0.1888]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2074,  0.0269,  0.1690],\n",
            "          [ 0.0721, -0.0900,  0.0665],\n",
            "          [-0.1260, -0.3127, -0.1733]],\n",
            "\n",
            "         [[-0.1441,  0.2017, -0.0143],\n",
            "          [ 0.0981,  0.2157,  0.0939],\n",
            "          [-0.0933, -0.1735,  0.0905]],\n",
            "\n",
            "         [[-0.2129,  0.0109, -0.1249],\n",
            "          [ 0.2147,  0.2667,  0.0806],\n",
            "          [ 0.0045, -0.2233, -0.1043]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3584,  0.4198,  0.0082],\n",
            "          [-0.0666, -0.0327, -0.2260],\n",
            "          [-0.1696, -0.3413,  0.0915]],\n",
            "\n",
            "         [[ 0.3407,  0.2133,  0.2846],\n",
            "          [-0.2246, -0.2679, -0.2333],\n",
            "          [ 0.0398, -0.1506, -0.0129]],\n",
            "\n",
            "         [[-0.0582,  0.3015,  0.1911],\n",
            "          [-0.2736,  0.0127, -0.0768],\n",
            "          [ 0.0971, -0.2242, -0.0113]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.0516,  0.0951,  0.0048],\n",
            "          [-0.1990, -0.0611,  0.0823],\n",
            "          [ 0.0975,  0.1456, -0.1776]],\n",
            "\n",
            "         [[-0.0306,  0.0234, -0.1482],\n",
            "          [ 0.1720,  0.0797,  0.0585],\n",
            "          [ 0.1518,  0.0231,  0.0545]],\n",
            "\n",
            "         [[ 0.0991, -0.1346,  0.0729],\n",
            "          [ 0.0754,  0.0973,  0.1120],\n",
            "          [ 0.1782,  0.1899, -0.1961]]],\n",
            "\n",
            "\n",
            "        [[[-0.0476,  0.0185, -0.1424],\n",
            "          [-0.0975,  0.0724, -0.1751],\n",
            "          [-0.2256, -0.1652, -0.2199]],\n",
            "\n",
            "         [[ 0.0870,  0.2384,  0.1185],\n",
            "          [-0.0255,  0.0211, -0.1359],\n",
            "          [-0.1383,  0.0971, -0.0132]],\n",
            "\n",
            "         [[-0.1139,  0.2112,  0.1243],\n",
            "          [-0.0934,  0.1213,  0.0802],\n",
            "          [-0.1705,  0.1090,  0.0710]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0803,  0.0805, -0.2928],\n",
            "          [ 0.2636,  0.1788, -0.3373],\n",
            "          [ 0.2163,  0.0973, -0.0495]],\n",
            "\n",
            "         [[ 0.2925, -0.1340,  0.0446],\n",
            "          [ 0.3892, -0.1039, -0.1950],\n",
            "          [ 0.1041,  0.1922, -0.1485]],\n",
            "\n",
            "         [[-0.0089, -0.0341, -0.1952],\n",
            "          [-0.0444, -0.0433, -0.1957],\n",
            "          [ 0.0422, -0.1094, -0.3230]]]], device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0301,  0.0105, -0.0469,  0.0473, -0.1475, -0.0746,  0.0011, -0.1366,\n",
            "         0.1881,  0.2837,  0.2384, -0.1048, -0.2568, -0.1633, -0.1894,  0.0096,\n",
            "        -0.0616,  0.1114, -0.0298,  0.0116,  0.0063, -0.0164,  0.0137,  0.2229,\n",
            "         0.1321,  0.0428, -0.0016, -0.2139, -0.0366, -0.0172,  0.1737, -0.0777,\n",
            "        -0.0188,  0.0567, -0.0620,  0.2354, -0.0137,  0.1631, -0.2529, -0.0372,\n",
            "         0.2649, -0.0347,  0.1135, -0.0412, -0.1295,  0.3023,  0.3410, -0.0666,\n",
            "        -0.1154, -0.0153, -0.0106,  0.1522, -0.2352,  0.2969, -0.0158,  0.1157,\n",
            "         0.2923,  0.0673, -0.0261,  0.0478,  0.1350, -0.1102, -0.0135,  0.1241],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 0.0517, -0.0175,  0.0301],\n",
            "          [ 0.0410,  0.0211, -0.0031],\n",
            "          [-0.0120, -0.0078, -0.0242]],\n",
            "\n",
            "         [[-0.0473, -0.0400,  0.0240],\n",
            "          [-0.0406, -0.0158, -0.0263],\n",
            "          [-0.0443, -0.0169,  0.0073]],\n",
            "\n",
            "         [[ 0.0126,  0.0021,  0.0048],\n",
            "          [-0.0199,  0.0094, -0.0055],\n",
            "          [-0.0348,  0.0245,  0.0062]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0384, -0.0219, -0.0393],\n",
            "          [-0.0270, -0.0406,  0.0213],\n",
            "          [-0.0244,  0.0025,  0.0373]],\n",
            "\n",
            "         [[ 0.0030,  0.0274, -0.0135],\n",
            "          [-0.0163, -0.0280, -0.0392],\n",
            "          [ 0.0035,  0.0325, -0.0009]],\n",
            "\n",
            "         [[-0.0321,  0.0352,  0.0327],\n",
            "          [-0.0218, -0.0307, -0.0209],\n",
            "          [-0.0034,  0.0301,  0.0322]]],\n",
            "\n",
            "\n",
            "        [[[-0.0041, -0.0312,  0.0066],\n",
            "          [-0.0284, -0.0228, -0.0140],\n",
            "          [ 0.0339, -0.0066,  0.0448]],\n",
            "\n",
            "         [[ 0.0318,  0.0613,  0.0134],\n",
            "          [-0.0115,  0.0432, -0.0102],\n",
            "          [ 0.0410,  0.0606,  0.0254]],\n",
            "\n",
            "         [[ 0.0069, -0.0255,  0.0289],\n",
            "          [ 0.0318,  0.0241,  0.0486],\n",
            "          [-0.0134,  0.0059,  0.0003]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0159,  0.0702,  0.0082],\n",
            "          [ 0.0173,  0.0706,  0.0253],\n",
            "          [ 0.0378, -0.0198, -0.0128]],\n",
            "\n",
            "         [[ 0.0465,  0.0094,  0.0002],\n",
            "          [-0.0096, -0.0053, -0.0224],\n",
            "          [ 0.0239,  0.0210,  0.0116]],\n",
            "\n",
            "         [[-0.0166,  0.0187,  0.0458],\n",
            "          [-0.0601,  0.0207, -0.0077],\n",
            "          [-0.0318, -0.0477,  0.0100]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0211, -0.0230,  0.0062],\n",
            "          [ 0.0077,  0.0069,  0.0242],\n",
            "          [-0.0189, -0.0497,  0.0003]],\n",
            "\n",
            "         [[-0.0461, -0.0285, -0.0100],\n",
            "          [-0.0086, -0.0188,  0.0064],\n",
            "          [-0.0088,  0.0189,  0.0463]],\n",
            "\n",
            "         [[ 0.0065,  0.0183,  0.0470],\n",
            "          [-0.0145,  0.0021, -0.0291],\n",
            "          [-0.0226, -0.0405,  0.0024]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0298, -0.0093, -0.0619],\n",
            "          [-0.0346, -0.0093, -0.0432],\n",
            "          [ 0.0079,  0.0290,  0.0072]],\n",
            "\n",
            "         [[ 0.0020, -0.0165, -0.0352],\n",
            "          [-0.0498, -0.0562, -0.0060],\n",
            "          [-0.0540, -0.0387,  0.0220]],\n",
            "\n",
            "         [[ 0.0433,  0.0111,  0.0301],\n",
            "          [ 0.0059,  0.0024,  0.0338],\n",
            "          [ 0.0398,  0.0386,  0.0063]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.0070,  0.0297,  0.0078],\n",
            "          [ 0.0861,  0.0304,  0.0515],\n",
            "          [ 0.0961,  0.1021,  0.0971]],\n",
            "\n",
            "         [[ 0.0086,  0.0037,  0.0385],\n",
            "          [-0.0127,  0.0728,  0.0033],\n",
            "          [ 0.0226,  0.0570, -0.0080]],\n",
            "\n",
            "         [[-0.0258,  0.0186,  0.0215],\n",
            "          [ 0.0445,  0.0656,  0.0960],\n",
            "          [ 0.0476,  0.0679,  0.0492]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0337, -0.0151, -0.0058],\n",
            "          [-0.0046,  0.0419,  0.0447],\n",
            "          [ 0.0176, -0.0272,  0.0019]],\n",
            "\n",
            "         [[ 0.0066,  0.0070, -0.0473],\n",
            "          [ 0.0118, -0.0091, -0.0412],\n",
            "          [-0.0407,  0.0283,  0.0054]],\n",
            "\n",
            "         [[ 0.0122,  0.0912,  0.0985],\n",
            "          [-0.0473,  0.0255,  0.0532],\n",
            "          [-0.0160,  0.0789,  0.0642]]],\n",
            "\n",
            "\n",
            "        [[[-0.0192,  0.0661,  0.0825],\n",
            "          [ 0.0183,  0.0747,  0.1003],\n",
            "          [-0.0411,  0.0143,  0.0455]],\n",
            "\n",
            "         [[ 0.0247,  0.0662,  0.0477],\n",
            "          [ 0.0388,  0.0039,  0.0345],\n",
            "          [-0.0346, -0.0253, -0.0095]],\n",
            "\n",
            "         [[ 0.0077,  0.0445,  0.0702],\n",
            "          [ 0.1063,  0.1381,  0.1006],\n",
            "          [ 0.0378,  0.0638,  0.0868]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0010,  0.0022, -0.0057],\n",
            "          [-0.0076, -0.0238,  0.0255],\n",
            "          [-0.0218,  0.0120,  0.0401]],\n",
            "\n",
            "         [[ 0.0385, -0.0122,  0.0280],\n",
            "          [ 0.0018,  0.0332,  0.0152],\n",
            "          [ 0.0163, -0.0044,  0.0228]],\n",
            "\n",
            "         [[ 0.0052, -0.0076,  0.0292],\n",
            "          [ 0.0746,  0.0253, -0.0985],\n",
            "          [ 0.0497, -0.0400, -0.0489]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0373,  0.0035,  0.0446],\n",
            "          [ 0.0274, -0.0323, -0.0182],\n",
            "          [-0.0155, -0.0265, -0.0306]],\n",
            "\n",
            "         [[ 0.0407,  0.0498,  0.0052],\n",
            "          [ 0.0241,  0.0234, -0.0035],\n",
            "          [ 0.0353,  0.0282,  0.0583]],\n",
            "\n",
            "         [[ 0.0176,  0.0383,  0.0609],\n",
            "          [ 0.0279, -0.0042,  0.0029],\n",
            "          [ 0.0231,  0.0176,  0.0419]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0023, -0.0079, -0.0117],\n",
            "          [ 0.0146,  0.0181, -0.0081],\n",
            "          [ 0.0091,  0.0005, -0.0155]],\n",
            "\n",
            "         [[ 0.0392, -0.0071, -0.0016],\n",
            "          [-0.0456,  0.0241, -0.0138],\n",
            "          [-0.0150, -0.0310, -0.0087]],\n",
            "\n",
            "         [[ 0.0136, -0.0134, -0.0168],\n",
            "          [ 0.0579,  0.0019,  0.0587],\n",
            "          [-0.0201,  0.0152,  0.0520]]]], device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 2.5223e-02, -6.7030e-02,  2.5103e-01, -4.7806e-02, -2.6351e-02,\n",
            "         2.5393e-01,  5.4950e-02,  6.1424e-02,  8.5633e-06, -1.4331e-01,\n",
            "        -4.3093e-04,  1.0272e-01, -2.2347e-02,  4.0590e-01, -5.4142e-02,\n",
            "         3.3019e-03, -1.7162e-02,  1.3540e-01, -2.2758e-02,  2.9230e-01,\n",
            "         2.2749e-02, -7.8563e-02,  2.1746e-01, -3.3498e-02,  4.0971e-01,\n",
            "        -2.6998e-02,  1.5206e-01,  1.1368e-02,  3.5717e-02, -2.3405e-02,\n",
            "         7.5202e-02, -3.1353e-02, -7.0346e-05,  2.0347e-01, -2.9659e-02,\n",
            "        -5.1213e-02,  6.7883e-03, -2.5231e-02, -1.6708e-01, -6.4754e-02,\n",
            "         2.5218e-01,  1.4438e-01, -2.5866e-02,  5.2550e-03, -6.4121e-02,\n",
            "         6.3966e-02,  4.2533e-02,  1.1383e-01, -5.9676e-02,  7.1821e-03,\n",
            "        -6.4877e-02, -1.4902e-01,  1.4964e-02,  1.1508e-01, -5.8099e-02,\n",
            "        -4.9545e-02,  2.9462e-01,  1.2203e-01,  3.7885e-02,  2.5941e-02,\n",
            "         1.0078e-02, -1.2026e-02, -8.5287e-02, -1.3418e-02], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 3.4626e-02, -4.6595e-03,  1.4499e-02],\n",
            "          [ 1.4076e-02,  1.8905e-02, -1.8034e-03],\n",
            "          [ 5.1125e-03, -4.5117e-02,  2.4908e-02]],\n",
            "\n",
            "         [[-3.0814e-02,  3.0113e-02, -1.7621e-02],\n",
            "          [-1.0952e-02,  1.8643e-02,  5.2669e-02],\n",
            "          [-4.8650e-04,  2.8552e-02, -1.6121e-02]],\n",
            "\n",
            "         [[-1.4106e-02,  1.5565e-02,  4.2248e-02],\n",
            "          [ 1.9850e-02, -3.2039e-04, -2.8494e-02],\n",
            "          [-2.1500e-02, -2.2698e-02,  3.4731e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.2941e-02, -8.5006e-04, -2.3639e-02],\n",
            "          [-5.3040e-02,  1.5819e-02, -3.3155e-02],\n",
            "          [-2.0948e-03, -2.7850e-02, -2.7639e-02]],\n",
            "\n",
            "         [[-5.6677e-03, -3.3344e-02, -5.4079e-02],\n",
            "          [-4.9271e-02, -3.0883e-02,  9.6468e-03],\n",
            "          [ 8.9350e-03, -5.2298e-02, -1.9076e-02]],\n",
            "\n",
            "         [[-4.6774e-02,  2.6037e-03,  1.1574e-02],\n",
            "          [ 3.7149e-02, -2.1667e-02, -2.7277e-02],\n",
            "          [ 1.8603e-02, -1.8990e-02,  2.2000e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.2096e-02,  1.0393e-02,  1.9613e-02],\n",
            "          [-4.9768e-03,  2.6734e-04, -6.6517e-05],\n",
            "          [-1.7393e-02,  3.1685e-02, -2.2002e-02]],\n",
            "\n",
            "         [[-1.6925e-02,  2.6994e-02,  9.3463e-03],\n",
            "          [ 1.5674e-02, -1.1799e-02,  4.7955e-02],\n",
            "          [ 2.2364e-02,  3.7042e-02, -2.3655e-02]],\n",
            "\n",
            "         [[-2.6231e-02,  4.0988e-02, -3.4184e-02],\n",
            "          [-8.0955e-03,  4.5178e-02,  3.6090e-02],\n",
            "          [ 1.6891e-02, -2.6632e-02, -2.4496e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.3421e-03, -1.9242e-02,  1.8943e-02],\n",
            "          [ 2.8786e-03, -2.6919e-02, -1.8842e-02],\n",
            "          [-1.4960e-02, -3.0355e-02,  3.8281e-02]],\n",
            "\n",
            "         [[-3.9029e-02, -5.6352e-02, -6.3206e-02],\n",
            "          [ 1.8576e-02,  5.0691e-03, -3.6689e-02],\n",
            "          [-4.7043e-02,  1.1519e-02, -4.2204e-02]],\n",
            "\n",
            "         [[-7.4504e-03, -1.8083e-02,  2.5816e-02],\n",
            "          [ 2.2835e-02, -2.6359e-02, -5.7481e-04],\n",
            "          [-1.0854e-02, -1.3317e-02, -1.6285e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 2.0154e-02,  3.3552e-02,  3.9417e-03],\n",
            "          [-2.6881e-02,  1.1239e-03,  1.6238e-02],\n",
            "          [-1.4401e-02,  2.3622e-02,  3.9293e-02]],\n",
            "\n",
            "         [[-1.2789e-02,  1.5748e-03, -4.4629e-02],\n",
            "          [-4.4432e-02, -4.9217e-03, -5.0669e-02],\n",
            "          [-3.9893e-02,  2.0248e-02, -4.6002e-02]],\n",
            "\n",
            "         [[-2.0310e-02,  3.9012e-02,  1.9612e-02],\n",
            "          [ 2.3234e-02,  1.8031e-02, -2.8533e-02],\n",
            "          [-4.0584e-02,  2.7775e-02,  3.1457e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.8961e-02, -4.3207e-03, -3.7205e-02],\n",
            "          [-6.1907e-02, -2.1038e-02, -1.3063e-05],\n",
            "          [-2.2119e-02, -4.4255e-02,  7.4601e-03]],\n",
            "\n",
            "         [[ 1.1462e-02,  3.8933e-02,  1.3110e-02],\n",
            "          [ 4.5013e-02,  9.2387e-03, -6.1911e-04],\n",
            "          [-2.8198e-02, -3.6940e-02, -3.1382e-02]],\n",
            "\n",
            "         [[-1.9991e-03, -4.5632e-02, -7.3957e-05],\n",
            "          [-1.7897e-02,  6.5416e-03,  3.2468e-02],\n",
            "          [-2.8409e-03,  3.4759e-02, -3.6076e-03]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 3.0741e-02, -5.6853e-03,  2.2380e-02],\n",
            "          [ 5.6538e-03,  3.7641e-02,  1.3322e-02],\n",
            "          [-6.2035e-03,  4.0728e-02,  4.5531e-03]],\n",
            "\n",
            "         [[-3.8736e-02, -2.8141e-02, -7.1004e-03],\n",
            "          [ 3.0544e-02,  2.2781e-02,  1.0454e-02],\n",
            "          [-2.7979e-02, -3.8326e-02, -1.7178e-02]],\n",
            "\n",
            "         [[-1.0098e-02, -2.9304e-02, -2.0211e-02],\n",
            "          [-1.1979e-02,  5.9223e-03, -1.3749e-02],\n",
            "          [ 1.8142e-02, -1.9166e-02, -1.9517e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.7246e-02,  7.7194e-02,  4.3242e-02],\n",
            "          [ 1.8233e-02,  4.7718e-03,  4.2086e-02],\n",
            "          [-1.5213e-02, -3.9621e-02,  2.7031e-02]],\n",
            "\n",
            "         [[ 1.4855e-02, -1.8786e-02,  3.2044e-02],\n",
            "          [ 6.9414e-02,  8.4829e-02,  3.2204e-02],\n",
            "          [ 1.2784e-02,  5.9205e-02,  4.2990e-02]],\n",
            "\n",
            "         [[-4.4375e-02, -2.9822e-02,  2.8202e-02],\n",
            "          [-3.1405e-02, -7.6557e-03,  1.6898e-02],\n",
            "          [ 2.1664e-02, -2.6214e-02, -1.9131e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.9200e-02,  2.0480e-02, -8.6929e-03],\n",
            "          [ 1.9217e-02,  3.8905e-02, -1.2561e-02],\n",
            "          [-3.9574e-02,  1.4454e-02,  2.3903e-03]],\n",
            "\n",
            "         [[ 2.9691e-02, -3.4991e-02,  6.4891e-03],\n",
            "          [ 5.6950e-02, -1.7217e-02, -3.9653e-02],\n",
            "          [ 1.4340e-02, -5.7258e-02, -4.4735e-02]],\n",
            "\n",
            "         [[-5.4550e-03,  1.7119e-02, -3.0946e-02],\n",
            "          [ 1.2787e-02,  2.6508e-03, -2.7191e-02],\n",
            "          [-1.2514e-02, -5.3887e-03,  1.1168e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.0976e-02, -2.8717e-02,  5.3186e-03],\n",
            "          [ 5.1734e-03,  1.0014e-02, -5.1204e-02],\n",
            "          [ 1.2705e-02, -6.3588e-02, -5.1596e-02]],\n",
            "\n",
            "         [[-4.4269e-02,  9.5589e-03,  7.4079e-02],\n",
            "          [ 2.5183e-02,  6.5123e-02,  1.6874e-02],\n",
            "          [ 1.5227e-02, -6.5580e-03,  2.1850e-02]],\n",
            "\n",
            "         [[-2.5134e-02, -4.1426e-02,  9.7919e-03],\n",
            "          [ 1.7157e-02, -2.8346e-02, -2.8219e-02],\n",
            "          [ 4.4441e-02, -6.8879e-03,  2.2491e-02]]],\n",
            "\n",
            "\n",
            "        [[[-3.9323e-03, -1.1406e-03,  4.0809e-02],\n",
            "          [ 1.4711e-02,  1.8420e-02,  2.5556e-02],\n",
            "          [ 4.4488e-02,  9.0756e-03, -6.2122e-03]],\n",
            "\n",
            "         [[-3.8576e-02,  1.3468e-02, -6.1298e-03],\n",
            "          [-4.8631e-02, -2.3761e-03, -1.1938e-02],\n",
            "          [-1.4787e-03,  3.0961e-02, -1.5593e-02]],\n",
            "\n",
            "         [[-4.7041e-02,  3.3454e-03,  1.0405e-02],\n",
            "          [-3.3627e-02, -2.0960e-02,  2.5503e-02],\n",
            "          [ 1.8840e-03,  1.5590e-02,  2.3061e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.6847e-02, -1.5207e-02,  5.4271e-02],\n",
            "          [ 6.8698e-02,  6.1393e-02,  3.2795e-02],\n",
            "          [ 2.0875e-02,  7.6348e-02,  5.9368e-02]],\n",
            "\n",
            "         [[-9.6898e-03, -1.1035e-02, -3.9809e-02],\n",
            "          [ 2.2599e-02, -4.7233e-03,  2.7048e-02],\n",
            "          [ 3.6063e-02, -5.4300e-03, -2.1131e-02]],\n",
            "\n",
            "         [[ 1.7216e-02, -8.1123e-03,  2.0497e-03],\n",
            "          [ 3.5932e-02,  6.5652e-03, -9.0931e-03],\n",
            "          [ 3.2150e-02,  2.8699e-02,  1.5057e-02]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-2.3715e-02,  2.3861e-02, -7.1090e-02, -7.4776e-03,  1.5370e-01,\n",
            "         8.0764e-02, -4.9749e-02, -3.5784e-02, -2.8966e-02, -4.9894e-02,\n",
            "        -1.0697e-01, -1.7258e-02,  4.3389e-02, -5.0448e-04,  2.3130e-02,\n",
            "         9.2402e-02,  2.6930e-02, -2.6119e-02, -2.6679e-02,  7.0494e-03,\n",
            "        -5.7844e-02,  1.0948e-01,  2.2680e-01,  1.1913e-01, -1.1819e-02,\n",
            "        -1.0205e-02,  1.2527e-03, -2.2699e-03, -8.7090e-03, -8.7350e-03,\n",
            "        -6.7125e-02,  2.0293e-02, -1.8016e-02, -1.6932e-02,  6.7430e-02,\n",
            "        -1.3724e-02,  1.5317e-01, -6.3236e-02,  1.2970e-01, -1.1752e-03,\n",
            "        -6.6569e-02, -9.3255e-02, -2.9840e-02,  3.0209e-02,  3.2914e-02,\n",
            "        -1.1643e-03,  7.0609e-02,  1.7545e-02, -8.7578e-03,  7.9702e-03,\n",
            "         3.1932e-03, -2.4950e-02, -1.2741e-02, -4.5159e-02,  1.3691e-01,\n",
            "         1.1385e-01,  3.4217e-02,  5.8086e-02, -2.4186e-02,  6.4835e-02,\n",
            "        -6.8992e-02,  5.0952e-03, -1.6119e-02, -4.3861e-02,  1.0845e-02,\n",
            "         3.1170e-02, -1.6043e-02, -4.3304e-02, -6.5738e-02,  1.0401e-01,\n",
            "         7.8815e-02,  6.3466e-03,  7.3419e-02,  1.0201e-01,  5.5639e-02,\n",
            "         2.8595e-02,  1.1153e-01,  1.7598e-01, -5.5079e-02,  6.2890e-03,\n",
            "        -8.4849e-02,  2.1370e-02,  1.2054e-02,  1.6771e-02, -3.1937e-02,\n",
            "         1.4997e-01,  3.3789e-02,  5.6407e-02,  1.4560e-02,  5.5517e-02,\n",
            "         6.9355e-02,  3.4218e-02, -4.2589e-02, -2.7509e-02,  1.3539e-02,\n",
            "         8.8167e-03, -1.6076e-03,  2.7595e-04,  1.6985e-02, -3.5088e-02,\n",
            "        -1.1110e-03,  1.0746e-02,  3.5067e-02, -1.4428e-02, -8.9168e-02,\n",
            "         1.8559e-01,  2.7413e-02,  6.5041e-02,  2.4145e-03, -4.8447e-02,\n",
            "         7.0476e-04,  9.7666e-02,  5.8957e-02,  4.2890e-02,  8.9255e-03,\n",
            "         2.9361e-02,  2.3015e-02,  4.1605e-02, -1.8883e-04, -3.9358e-02,\n",
            "         2.5058e-02,  1.3592e-02,  1.0586e-01, -3.9561e-02,  4.1241e-04,\n",
            "        -9.1743e-03,  4.3953e-02,  1.9609e-02], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-3.5233e-02, -2.3770e-02,  5.8925e-03],\n",
            "          [-1.6700e-02,  7.1138e-03,  1.5006e-02],\n",
            "          [-3.9917e-03, -1.7893e-03, -8.0348e-04]],\n",
            "\n",
            "         [[-6.8629e-03, -3.4599e-02, -3.1392e-02],\n",
            "          [ 2.1459e-04, -7.6361e-05,  1.9168e-02],\n",
            "          [-8.7548e-03,  3.7297e-03,  4.2103e-03]],\n",
            "\n",
            "         [[-2.9791e-02, -1.5243e-02,  1.2616e-02],\n",
            "          [ 8.8411e-04,  3.6220e-03,  1.6133e-02],\n",
            "          [ 2.4319e-02,  3.3935e-02, -3.4608e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.1073e-03, -3.6275e-02, -3.3208e-02],\n",
            "          [-2.1561e-02,  7.7902e-03, -3.1871e-02],\n",
            "          [-3.6367e-02, -4.3885e-02, -4.0022e-02]],\n",
            "\n",
            "         [[-8.2886e-03,  2.5857e-02, -1.2933e-02],\n",
            "          [ 3.4558e-02, -1.3836e-02, -1.4900e-02],\n",
            "          [ 1.3430e-02, -3.3071e-02, -1.6434e-02]],\n",
            "\n",
            "         [[ 2.4955e-02, -2.8446e-02, -1.6878e-02],\n",
            "          [ 9.6389e-03,  4.4577e-03, -1.3638e-02],\n",
            "          [ 1.8844e-02, -2.2063e-02, -1.4159e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.4007e-02, -1.8871e-02, -2.1101e-02],\n",
            "          [-1.8805e-02, -1.4945e-03,  1.1506e-02],\n",
            "          [ 7.7183e-03,  8.0212e-03,  2.6266e-02]],\n",
            "\n",
            "         [[ 6.4536e-04,  1.8233e-02, -9.2371e-03],\n",
            "          [-1.0318e-02, -1.6446e-02, -1.5965e-02],\n",
            "          [ 1.1347e-02,  1.7162e-03,  6.8274e-04]],\n",
            "\n",
            "         [[-9.0624e-03,  1.9865e-02, -1.8608e-02],\n",
            "          [-3.3829e-02, -2.5829e-02,  3.4187e-02],\n",
            "          [ 1.6330e-02, -3.1897e-02, -1.4359e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.4868e-02,  3.7079e-02,  2.7095e-02],\n",
            "          [-6.0797e-03, -1.0632e-03, -9.4285e-03],\n",
            "          [ 7.5923e-03, -2.5109e-02, -3.3250e-02]],\n",
            "\n",
            "         [[-2.7092e-03,  2.7985e-02,  4.7927e-03],\n",
            "          [-1.5764e-02, -1.7569e-02,  1.8975e-02],\n",
            "          [ 1.1320e-03, -2.0955e-02, -1.6269e-02]],\n",
            "\n",
            "         [[ 4.1474e-03, -2.0806e-02, -1.8972e-02],\n",
            "          [-7.6547e-03, -8.3379e-03,  3.0023e-03],\n",
            "          [-2.1061e-02,  1.2516e-03,  1.6056e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.6502e-03, -3.4292e-02, -3.5787e-02],\n",
            "          [-1.5410e-02, -2.8428e-02, -2.6417e-02],\n",
            "          [ 8.5857e-03, -1.4984e-02, -1.1492e-02]],\n",
            "\n",
            "         [[ 7.9233e-03, -9.0605e-03,  2.2980e-02],\n",
            "          [-2.4555e-02,  2.9554e-02, -1.5374e-02],\n",
            "          [-4.0894e-03, -2.3321e-02, -2.0423e-02]],\n",
            "\n",
            "         [[-8.8321e-03, -1.4936e-02,  1.2841e-02],\n",
            "          [-3.0133e-03, -1.0199e-02, -3.4364e-02],\n",
            "          [-5.4099e-03,  1.5228e-02, -2.2896e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.6723e-02, -5.9633e-03, -1.0083e-02],\n",
            "          [-3.0935e-02, -5.8135e-03,  4.4064e-03],\n",
            "          [ 6.2743e-03,  1.8525e-02,  4.0319e-03]],\n",
            "\n",
            "         [[-2.2604e-02, -3.3843e-02, -2.5342e-02],\n",
            "          [-1.2649e-03, -8.6352e-03,  9.9989e-03],\n",
            "          [-4.3079e-03, -2.5510e-02, -4.9242e-02]],\n",
            "\n",
            "         [[ 1.7740e-03, -2.8115e-02, -4.4458e-03],\n",
            "          [-2.7375e-02, -1.2030e-02,  2.2782e-03],\n",
            "          [ 2.2603e-02, -5.4265e-03,  3.2432e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 2.4608e-02, -2.5437e-02,  2.1556e-02],\n",
            "          [ 3.0807e-02,  1.2477e-02, -2.4074e-02],\n",
            "          [ 2.7114e-02,  3.1476e-02,  9.6571e-03]],\n",
            "\n",
            "         [[-2.6372e-02, -1.7245e-02, -7.5359e-03],\n",
            "          [ 1.2581e-02,  1.0976e-02, -1.6428e-02],\n",
            "          [ 2.6984e-02, -7.8190e-03, -1.3050e-02]],\n",
            "\n",
            "         [[ 3.2362e-02,  2.8367e-02, -8.1259e-03],\n",
            "          [-1.0390e-02,  9.1102e-04, -2.2192e-02],\n",
            "          [-2.1308e-02,  5.9297e-03,  5.1805e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.1121e-02,  2.2675e-03, -8.6622e-03],\n",
            "          [ 2.0739e-02,  1.1482e-02,  1.5224e-02],\n",
            "          [ 1.7987e-02, -9.3996e-03,  1.7309e-02]],\n",
            "\n",
            "         [[ 2.8557e-02, -1.4422e-02,  2.2673e-02],\n",
            "          [ 5.4916e-03, -7.3762e-03, -1.1017e-02],\n",
            "          [ 8.5843e-03,  1.0738e-02, -1.8040e-02]],\n",
            "\n",
            "         [[ 6.6615e-04, -3.4710e-02, -3.1210e-02],\n",
            "          [-2.9635e-02, -1.3346e-02, -2.0811e-02],\n",
            "          [-6.5415e-03,  1.5305e-02, -3.0771e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.0585e-04, -9.2240e-03,  5.1441e-03],\n",
            "          [ 1.2043e-03, -3.2454e-02, -2.0950e-02],\n",
            "          [-9.2835e-03,  7.7588e-04, -2.8092e-02]],\n",
            "\n",
            "         [[-2.3910e-02, -2.2134e-04, -2.5663e-02],\n",
            "          [-2.2155e-02,  1.3960e-02, -7.5707e-03],\n",
            "          [-2.8894e-02,  3.6426e-03, -3.8127e-02]],\n",
            "\n",
            "         [[ 1.1127e-02, -3.9703e-02,  3.3917e-03],\n",
            "          [ 3.2932e-02,  2.0553e-02,  2.5474e-02],\n",
            "          [-2.3298e-02,  2.4178e-02, -1.3042e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.5560e-02,  6.3740e-04, -2.4987e-02],\n",
            "          [ 3.2255e-02,  3.2896e-02,  2.1262e-02],\n",
            "          [-1.4453e-02, -8.9813e-03,  6.8786e-03]],\n",
            "\n",
            "         [[ 2.6723e-03, -2.1797e-02,  9.3972e-03],\n",
            "          [ 4.2911e-02,  1.4935e-03, -3.2764e-02],\n",
            "          [-3.9824e-03, -6.5525e-03, -3.6285e-02]],\n",
            "\n",
            "         [[ 1.6835e-02,  1.9841e-02, -1.4901e-02],\n",
            "          [-2.5695e-02, -3.5772e-03, -7.9566e-03],\n",
            "          [ 1.5349e-02, -2.8715e-02, -2.7507e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 2.2520e-02, -2.9694e-02, -9.6571e-03],\n",
            "          [-4.4720e-04, -3.1266e-02,  2.4385e-02],\n",
            "          [-2.6584e-02, -2.0007e-02, -3.9698e-03]],\n",
            "\n",
            "         [[-1.5541e-03, -1.3736e-02,  3.1293e-03],\n",
            "          [-1.6671e-02,  1.2878e-02,  1.5099e-04],\n",
            "          [-7.0160e-03, -3.1486e-02,  7.5685e-03]],\n",
            "\n",
            "         [[ 2.6081e-02, -1.6629e-03, -3.6289e-02],\n",
            "          [ 2.6837e-02,  1.9523e-03,  2.9169e-02],\n",
            "          [-2.1998e-02,  3.4883e-02,  4.4808e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.2058e-02,  1.0674e-02, -2.5914e-02],\n",
            "          [-4.9889e-02, -8.5812e-03,  1.9211e-02],\n",
            "          [-4.4872e-02, -4.2417e-02,  7.6130e-03]],\n",
            "\n",
            "         [[ 3.1836e-02,  1.3182e-02, -4.0866e-03],\n",
            "          [ 2.5191e-02,  4.8571e-03, -7.8343e-03],\n",
            "          [-8.7786e-03,  2.2606e-02,  2.3510e-02]],\n",
            "\n",
            "         [[-1.3841e-02, -2.6470e-02,  1.2238e-02],\n",
            "          [-1.9655e-02,  9.5686e-03, -1.0282e-02],\n",
            "          [ 2.0481e-02,  8.4534e-03,  2.7784e-03]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 6.5764e-02, -3.5683e-02, -1.1497e-02, -1.5651e-02, -5.6180e-02,\n",
            "         7.3352e-02,  5.3025e-04, -6.9200e-03,  7.9908e-02, -9.2850e-03,\n",
            "         3.4429e-02, -2.4924e-02,  9.6388e-02, -1.3857e-02,  1.2093e-01,\n",
            "        -7.9739e-03, -3.1918e-02,  1.4954e-02,  6.7956e-02, -4.6734e-02,\n",
            "        -5.6940e-02, -6.8940e-03,  2.9657e-02, -3.7898e-02, -1.1406e-02,\n",
            "        -4.1812e-02, -7.9214e-02,  7.5223e-02, -4.9278e-03, -9.8886e-03,\n",
            "         4.0491e-02, -7.2542e-02, -9.5474e-03,  2.4482e-02,  7.9476e-02,\n",
            "         1.2145e-01, -1.4514e-02,  4.4787e-02,  5.1283e-02, -7.3459e-03,\n",
            "        -1.6087e-02,  2.9156e-02, -1.8771e-02, -3.1097e-02, -7.5149e-02,\n",
            "        -1.7682e-04, -4.3696e-02,  1.1455e-02, -2.8791e-02, -2.6728e-02,\n",
            "        -5.7734e-05, -2.0002e-02, -8.0030e-02, -8.9515e-03,  9.0767e-02,\n",
            "        -2.4848e-03, -3.0636e-02, -5.5087e-02, -2.4100e-02,  3.2193e-02,\n",
            "         1.0211e-02,  1.5776e-02, -1.7421e-02,  1.9532e-02, -2.1658e-02,\n",
            "         7.9575e-02,  9.7399e-02,  1.0131e-01, -5.0308e-02, -3.6936e-02,\n",
            "         1.7715e-02,  7.8757e-02, -4.1378e-02,  8.6555e-02,  2.0258e-02,\n",
            "         1.7104e-01,  2.4518e-02,  3.7341e-02, -1.9803e-02,  3.6117e-02,\n",
            "         4.2466e-02, -2.1380e-02, -9.4927e-02,  7.5221e-02, -5.5481e-04,\n",
            "         1.1948e-01, -4.9553e-03,  5.0898e-02,  5.6577e-02,  1.3049e-01,\n",
            "        -2.5716e-02, -3.3741e-02, -4.6883e-02, -3.4761e-02, -3.4833e-02,\n",
            "         4.5446e-02, -2.1609e-03, -2.8693e-03, -2.1178e-03, -1.1170e-02,\n",
            "         1.9495e-02, -1.7681e-02,  1.0063e-01, -7.9476e-03, -1.9035e-02,\n",
            "        -6.2964e-02,  2.4514e-02, -7.6044e-03, -5.7480e-02, -8.7730e-03,\n",
            "         8.9832e-03,  2.0721e-02, -1.0635e-01,  8.5132e-02,  8.3502e-03,\n",
            "        -2.0282e-02,  8.7324e-02,  3.3919e-02,  7.2621e-02, -3.4860e-02,\n",
            "        -4.0238e-02, -2.3661e-02, -9.4937e-03, -1.7020e-02, -5.5877e-03,\n",
            "         1.7843e-02, -7.4935e-02,  2.4408e-02], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 2.2028e-03,  1.4709e-03,  3.0148e-03,  ..., -1.0119e-03,\n",
            "         -2.1741e-03, -1.5711e-03],\n",
            "        [ 2.1094e-03,  6.5458e-03,  2.9443e-03,  ..., -1.7045e-04,\n",
            "         -4.2024e-03,  1.2213e-03],\n",
            "        [-6.2115e-04,  2.1193e-04, -1.3517e-03,  ..., -8.1871e-04,\n",
            "         -1.2601e-03,  1.5834e-04],\n",
            "        ...,\n",
            "        [ 1.9803e-04, -2.1803e-03, -1.5149e-03,  ..., -6.6710e-03,\n",
            "         -8.1932e-04, -1.3591e-04],\n",
            "        [ 1.5994e-04, -1.4079e-03, -1.5350e-03,  ..., -2.0174e-04,\n",
            "         -3.0950e-03, -4.9047e-03],\n",
            "        [ 1.1106e-03,  3.6411e-03,  4.9585e-05,  ..., -1.8086e-03,\n",
            "          5.2633e-04,  2.8258e-03]], device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 1.9766e-02,  2.7292e-02, -7.4502e-03,  3.7277e-03, -2.4223e-03,\n",
            "         2.1825e-02,  5.5891e-03, -3.7776e-03,  1.5831e-03,  4.8088e-03,\n",
            "        -5.6634e-03, -1.6082e-03,  1.3494e-03, -1.4438e-03, -9.9392e-03,\n",
            "        -8.6492e-03,  4.3441e-03,  2.6132e-03, -2.0935e-03,  1.2728e-02,\n",
            "         7.0502e-03, -1.2317e-03, -1.0740e-02, -3.1601e-04, -2.3839e-03,\n",
            "         8.1220e-04,  7.1384e-03, -1.3064e-03,  2.3592e-03, -1.0845e-02,\n",
            "        -6.9096e-03,  2.5008e-03, -8.0120e-03, -7.5177e-03, -2.9161e-04,\n",
            "         2.7964e-04, -2.8475e-03,  7.9571e-03,  9.2535e-03, -1.4872e-03,\n",
            "        -4.1106e-03,  1.5125e-02, -3.9263e-03,  2.7291e-02, -3.4479e-03,\n",
            "         4.0602e-03, -3.3972e-04, -1.1164e-04, -1.0928e-02, -1.0719e-02,\n",
            "         1.5186e-02, -2.8114e-03,  1.8364e-02,  3.5538e-04,  5.6847e-03,\n",
            "         4.5275e-03,  4.2965e-03,  2.1365e-03, -2.3030e-03,  1.3460e-02,\n",
            "         4.2752e-04,  1.7322e-02,  1.7794e-03,  1.4657e-03,  7.8786e-03,\n",
            "         5.6744e-03,  5.3714e-03, -3.9468e-03, -6.6443e-03,  2.4787e-03,\n",
            "         3.7910e-03,  1.5393e-02,  2.5718e-03,  1.1990e-02,  5.0136e-04,\n",
            "         7.1810e-03,  6.9034e-03,  1.3136e-02, -7.4546e-03, -1.1141e-02,\n",
            "        -1.0818e-02,  1.3442e-02,  4.3910e-03, -7.5722e-03,  1.2223e-04,\n",
            "        -8.3480e-03, -2.0080e-03,  2.4352e-03, -7.1845e-03, -4.1911e-04,\n",
            "        -1.3606e-03,  1.3172e-02,  9.9991e-03,  1.9974e-02,  8.9681e-03,\n",
            "         2.8159e-04, -5.8783e-03,  5.8321e-05,  6.4500e-04,  1.5398e-03,\n",
            "         9.7632e-03,  1.6674e-02, -3.5016e-03, -1.0507e-03,  9.9478e-04,\n",
            "         2.5366e-03,  1.8881e-02,  1.7663e-04,  3.8378e-03,  3.8474e-03,\n",
            "        -1.6868e-03,  1.5137e-02,  1.9823e-02, -1.3378e-03, -1.2007e-04,\n",
            "        -1.5705e-03,  7.3171e-03, -4.2489e-03,  4.3597e-03,  1.3061e-04,\n",
            "        -8.3107e-03, -6.2861e-04, -1.4505e-03,  1.2661e-02,  1.1852e-02,\n",
            "         4.5903e-03,  2.9290e-03, -6.5104e-03, -6.4470e-03, -1.9350e-02,\n",
            "        -7.8451e-03, -4.4800e-03,  4.2584e-03, -3.6150e-03,  1.3845e-03,\n",
            "         8.8203e-03, -1.0199e-02,  1.5953e-03,  1.4247e-03,  5.4326e-03,\n",
            "        -5.6327e-04, -1.4142e-02, -8.3301e-03,  4.2897e-04,  3.1655e-03,\n",
            "         3.1757e-03, -6.7762e-03,  1.6154e-03,  2.3675e-03,  1.2385e-02,\n",
            "         6.8188e-03,  9.9395e-03,  5.7764e-03,  8.1153e-03, -5.7309e-03,\n",
            "         1.4928e-02,  1.9655e-03, -1.6554e-06,  1.4028e-02, -1.1400e-03,\n",
            "         1.7247e-02, -3.0565e-03, -6.6128e-03, -5.0568e-03, -5.1290e-03,\n",
            "         1.2408e-03, -7.5152e-03,  1.3359e-02, -1.4800e-03, -3.8817e-03,\n",
            "        -6.8097e-03,  7.7920e-03, -3.9119e-03,  6.4606e-03,  6.1568e-04,\n",
            "        -2.5631e-03, -2.4155e-03, -2.9801e-04, -6.1919e-05,  1.1271e-02,\n",
            "         1.0207e-02,  1.6587e-02, -2.2878e-03, -3.2110e-03, -1.1574e-03,\n",
            "         3.3590e-03,  8.3084e-03, -5.9526e-03,  8.5053e-03,  1.3647e-03,\n",
            "         5.3848e-03, -2.9929e-04,  1.3828e-02,  2.3769e-03, -5.1293e-03,\n",
            "         5.3650e-03, -8.2381e-04, -6.1756e-03,  2.8239e-03,  4.1366e-03,\n",
            "         8.3145e-03, -4.4412e-03,  1.0083e-02,  8.2946e-03, -1.8662e-04,\n",
            "        -1.4656e-02,  5.4018e-03, -5.2671e-03, -1.2513e-03, -5.4324e-03,\n",
            "         1.6631e-03, -6.5916e-03, -1.2483e-03, -1.6975e-03, -8.5231e-03,\n",
            "        -7.6497e-03, -2.6215e-02,  2.3991e-02, -6.1909e-03,  9.9149e-03,\n",
            "        -2.6174e-03, -6.0589e-03,  8.4265e-03,  2.5438e-03,  6.5575e-04,\n",
            "         1.4049e-02,  1.4439e-02,  3.3497e-03,  1.9149e-03,  2.9212e-02,\n",
            "        -3.9427e-03,  1.7168e-02,  1.7253e-02, -9.6616e-03, -6.0491e-03,\n",
            "        -3.9117e-03, -3.0042e-03, -7.0862e-03,  2.0765e-03, -2.6589e-03,\n",
            "        -1.3381e-03,  5.8153e-03, -4.7640e-03,  2.2388e-03,  1.2325e-03,\n",
            "         6.2998e-03,  3.3719e-03, -1.0703e-02,  1.1885e-03, -8.8944e-03,\n",
            "        -1.1395e-03, -1.0231e-02,  1.7510e-03, -4.2428e-03, -4.9664e-03,\n",
            "         6.3337e-03], device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.0119,  0.0409, -0.0640,  ...,  0.0521,  0.0068,  0.0249],\n",
            "        [-0.0486,  0.0264,  0.0021,  ..., -0.0356,  0.0059, -0.0352],\n",
            "        [ 0.0629,  0.1130, -0.0943,  ..., -0.0686,  0.0840,  0.0553],\n",
            "        ...,\n",
            "        [-0.0308,  0.0149,  0.0382,  ..., -0.0408,  0.0249,  0.0752],\n",
            "        [ 0.0359, -0.0608, -0.0079,  ..., -0.0504, -0.0439, -0.0080],\n",
            "        [-0.0133,  0.0182,  0.0377,  ..., -0.0640,  0.0320, -0.0279]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0533, -0.0270,  0.0779,  0.0269, -0.0186, -0.0030, -0.0234, -0.0164,\n",
            "         0.0537,  0.0234, -0.0159,  0.0382, -0.0434, -0.0020,  0.0358,  0.0175,\n",
            "        -0.0298, -0.0276,  0.0190,  0.0311,  0.0194, -0.0275,  0.0462,  0.0272,\n",
            "         0.0561,  0.0498,  0.0107,  0.0690, -0.0132, -0.0215,  0.0630,  0.0413,\n",
            "         0.0129,  0.0123,  0.0413,  0.0534,  0.0281, -0.0561, -0.0096,  0.0374,\n",
            "         0.0403, -0.0349,  0.0382, -0.0049, -0.0220, -0.0131,  0.0601, -0.0142,\n",
            "        -0.0069,  0.0548, -0.0143, -0.0078, -0.0417,  0.0114,  0.0302,  0.0610,\n",
            "         0.0536, -0.0594, -0.0105, -0.0254, -0.0569,  0.0177,  0.0615,  0.0598,\n",
            "        -0.0557,  0.0245, -0.0076, -0.0571, -0.0269,  0.0416, -0.0015,  0.0410,\n",
            "         0.0073, -0.0539,  0.0057, -0.0541, -0.0250, -0.0086,  0.0328, -0.0610,\n",
            "        -0.0256,  0.0483,  0.0278,  0.0826,  0.0313,  0.0639, -0.0185,  0.0675,\n",
            "         0.0279,  0.0134, -0.0860, -0.0258,  0.0404, -0.0395, -0.0070,  0.0498,\n",
            "         0.0329,  0.0440, -0.0190,  0.0163, -0.0384, -0.0075, -0.0228,  0.0383,\n",
            "         0.0255,  0.0636,  0.0455, -0.0395,  0.0396, -0.0014,  0.0259, -0.0588,\n",
            "        -0.0469, -0.0516, -0.0327,  0.0225,  0.0171, -0.0204, -0.0174, -0.0217,\n",
            "        -0.0561,  0.0068,  0.0081, -0.0215, -0.0289, -0.0168, -0.0134,  0.0695,\n",
            "        -0.0587, -0.0347,  0.0214, -0.0229, -0.0350, -0.0196,  0.0384,  0.0159,\n",
            "         0.0896,  0.0340,  0.0433,  0.0672, -0.0228, -0.0513, -0.0585, -0.0152,\n",
            "         0.0519, -0.0356,  0.0041,  0.0265,  0.0391,  0.0050, -0.0439, -0.0016,\n",
            "         0.0705, -0.0378, -0.0599, -0.0445, -0.0129,  0.0321, -0.0106, -0.0237,\n",
            "        -0.0548, -0.0553,  0.0573,  0.0616, -0.0604,  0.0159,  0.0324, -0.0771,\n",
            "         0.0341,  0.0534, -0.0436,  0.0520, -0.0471,  0.0317,  0.0218, -0.0734,\n",
            "        -0.0369, -0.0347,  0.0260, -0.0579, -0.0380, -0.0288, -0.0508, -0.0207,\n",
            "        -0.0155, -0.0401, -0.0253,  0.0252,  0.0355,  0.0444, -0.0107, -0.0104,\n",
            "         0.0328,  0.0079, -0.0058,  0.0450,  0.0319,  0.0136,  0.0312,  0.0356,\n",
            "        -0.0509,  0.0008,  0.0608,  0.0297,  0.0437,  0.0339,  0.0324, -0.0275,\n",
            "         0.0037, -0.0537, -0.0012, -0.0058,  0.0533, -0.0071,  0.0574, -0.0547,\n",
            "         0.0171,  0.0012,  0.0497,  0.0653,  0.0071,  0.0481,  0.0421, -0.0493,\n",
            "         0.0480, -0.0374,  0.0970, -0.0154, -0.0199,  0.0227, -0.0467,  0.0459,\n",
            "        -0.0487,  0.0758, -0.0020, -0.0508,  0.0674,  0.0200,  0.0476, -0.0609,\n",
            "        -0.0085,  0.0317, -0.0114,  0.0925,  0.0316,  0.0423,  0.0360, -0.0472,\n",
            "         0.0627,  0.0008,  0.0130,  0.0131,  0.0429,  0.0463, -0.0061,  0.0265],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.0919, -0.1529, -0.1582,  ..., -0.1210, -0.0496,  0.0704],\n",
            "        [-0.0338,  0.0061, -0.1440,  ...,  0.0182,  0.0452,  0.0198],\n",
            "        [-0.1698,  0.0602, -0.0125,  ..., -0.0844, -0.0070,  0.2223],\n",
            "        ...,\n",
            "        [-0.0516, -0.1368,  0.0825,  ...,  0.0097,  0.0544, -0.0175],\n",
            "        [ 0.0088,  0.1071, -0.0258,  ...,  0.0820,  0.0160, -0.1237],\n",
            "        [ 0.0706, -0.0481,  0.1850,  ..., -0.0720,  0.0405,  0.0318]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0602, -0.1876, -0.0314, -0.0437,  0.1963, -0.0859,  0.1337, -0.0619,\n",
            "         0.0432, -0.0874], device='cuda:0', requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Conv4Galu()\n",
        "model.to(device)\n",
        "model"
      ],
      "metadata": {
        "id": "BznM_eBXZex5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "075db91c-984f-4fb0-c9e0-725088e41cc2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv4Galu(\n",
              "  (N1_C1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N1_C2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N1_C3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N1_C4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N1_F1): Flatten(start_dim=1, end_dim=-1)\n",
              "  (N1_D1): Linear(in_features=131072, out_features=256, bias=True)\n",
              "  (N1_D2): Linear(in_features=256, out_features=256, bias=True)\n",
              "  (N2_C1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N2_C2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N2_C3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N2_C4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "  (N2_F1): Flatten(start_dim=1, end_dim=-1)\n",
              "  (N2_D1): Linear(in_features=131072, out_features=256, bias=True)\n",
              "  (N2_D2): Linear(in_features=256, out_features=256, bias=True)\n",
              "  (outputs): Linear(in_features=256, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for p in relu_model.parameters():\n",
        "  print(p)"
      ],
      "metadata": {
        "id": "LsZJSC8LZlnS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf522d16-8906-4493-cb51-8794fd7cb345"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[[[ 0.1850,  0.1115,  0.1831],\n",
            "          [-0.1391,  0.0578,  0.0505],\n",
            "          [-0.2225,  0.0643,  0.0665]],\n",
            "\n",
            "         [[ 0.2318,  0.0711,  0.2237],\n",
            "          [-0.2885, -0.1129, -0.1827],\n",
            "          [-0.2874, -0.0576,  0.1046]],\n",
            "\n",
            "         [[ 0.2614,  0.3017,  0.1691],\n",
            "          [-0.2203, -0.1606, -0.1812],\n",
            "          [-0.2189, -0.2360,  0.1888]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2074,  0.0269,  0.1690],\n",
            "          [ 0.0721, -0.0900,  0.0665],\n",
            "          [-0.1260, -0.3127, -0.1733]],\n",
            "\n",
            "         [[-0.1441,  0.2017, -0.0143],\n",
            "          [ 0.0981,  0.2157,  0.0939],\n",
            "          [-0.0933, -0.1735,  0.0905]],\n",
            "\n",
            "         [[-0.2129,  0.0109, -0.1249],\n",
            "          [ 0.2147,  0.2667,  0.0806],\n",
            "          [ 0.0045, -0.2233, -0.1043]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3584,  0.4198,  0.0082],\n",
            "          [-0.0666, -0.0327, -0.2260],\n",
            "          [-0.1696, -0.3413,  0.0915]],\n",
            "\n",
            "         [[ 0.3407,  0.2133,  0.2846],\n",
            "          [-0.2246, -0.2679, -0.2333],\n",
            "          [ 0.0398, -0.1506, -0.0129]],\n",
            "\n",
            "         [[-0.0582,  0.3015,  0.1911],\n",
            "          [-0.2736,  0.0127, -0.0768],\n",
            "          [ 0.0971, -0.2242, -0.0113]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.0516,  0.0951,  0.0048],\n",
            "          [-0.1990, -0.0611,  0.0823],\n",
            "          [ 0.0975,  0.1456, -0.1776]],\n",
            "\n",
            "         [[-0.0306,  0.0234, -0.1482],\n",
            "          [ 0.1720,  0.0797,  0.0585],\n",
            "          [ 0.1518,  0.0231,  0.0545]],\n",
            "\n",
            "         [[ 0.0991, -0.1346,  0.0729],\n",
            "          [ 0.0754,  0.0973,  0.1120],\n",
            "          [ 0.1782,  0.1899, -0.1961]]],\n",
            "\n",
            "\n",
            "        [[[-0.0476,  0.0185, -0.1424],\n",
            "          [-0.0975,  0.0724, -0.1751],\n",
            "          [-0.2256, -0.1652, -0.2199]],\n",
            "\n",
            "         [[ 0.0870,  0.2384,  0.1185],\n",
            "          [-0.0255,  0.0211, -0.1359],\n",
            "          [-0.1383,  0.0971, -0.0132]],\n",
            "\n",
            "         [[-0.1139,  0.2112,  0.1243],\n",
            "          [-0.0934,  0.1213,  0.0802],\n",
            "          [-0.1705,  0.1090,  0.0710]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0803,  0.0805, -0.2928],\n",
            "          [ 0.2636,  0.1788, -0.3373],\n",
            "          [ 0.2163,  0.0973, -0.0495]],\n",
            "\n",
            "         [[ 0.2925, -0.1340,  0.0446],\n",
            "          [ 0.3892, -0.1039, -0.1950],\n",
            "          [ 0.1041,  0.1922, -0.1485]],\n",
            "\n",
            "         [[-0.0089, -0.0341, -0.1952],\n",
            "          [-0.0444, -0.0433, -0.1957],\n",
            "          [ 0.0422, -0.1094, -0.3230]]]], device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0301,  0.0105, -0.0469,  0.0473, -0.1475, -0.0746,  0.0011, -0.1366,\n",
            "         0.1881,  0.2837,  0.2384, -0.1048, -0.2568, -0.1633, -0.1894,  0.0096,\n",
            "        -0.0616,  0.1114, -0.0298,  0.0116,  0.0063, -0.0164,  0.0137,  0.2229,\n",
            "         0.1321,  0.0428, -0.0016, -0.2139, -0.0366, -0.0172,  0.1737, -0.0777,\n",
            "        -0.0188,  0.0567, -0.0620,  0.2354, -0.0137,  0.1631, -0.2529, -0.0372,\n",
            "         0.2649, -0.0347,  0.1135, -0.0412, -0.1295,  0.3023,  0.3410, -0.0666,\n",
            "        -0.1154, -0.0153, -0.0106,  0.1522, -0.2352,  0.2969, -0.0158,  0.1157,\n",
            "         0.2923,  0.0673, -0.0261,  0.0478,  0.1350, -0.1102, -0.0135,  0.1241],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 0.0517, -0.0175,  0.0301],\n",
            "          [ 0.0410,  0.0211, -0.0031],\n",
            "          [-0.0120, -0.0078, -0.0242]],\n",
            "\n",
            "         [[-0.0473, -0.0400,  0.0240],\n",
            "          [-0.0406, -0.0158, -0.0263],\n",
            "          [-0.0443, -0.0169,  0.0073]],\n",
            "\n",
            "         [[ 0.0126,  0.0021,  0.0048],\n",
            "          [-0.0199,  0.0094, -0.0055],\n",
            "          [-0.0348,  0.0245,  0.0062]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0384, -0.0219, -0.0393],\n",
            "          [-0.0270, -0.0406,  0.0213],\n",
            "          [-0.0244,  0.0025,  0.0373]],\n",
            "\n",
            "         [[ 0.0030,  0.0274, -0.0135],\n",
            "          [-0.0163, -0.0280, -0.0392],\n",
            "          [ 0.0035,  0.0325, -0.0009]],\n",
            "\n",
            "         [[-0.0321,  0.0352,  0.0327],\n",
            "          [-0.0218, -0.0307, -0.0209],\n",
            "          [-0.0034,  0.0301,  0.0322]]],\n",
            "\n",
            "\n",
            "        [[[-0.0041, -0.0312,  0.0066],\n",
            "          [-0.0284, -0.0228, -0.0140],\n",
            "          [ 0.0339, -0.0066,  0.0448]],\n",
            "\n",
            "         [[ 0.0318,  0.0613,  0.0134],\n",
            "          [-0.0115,  0.0432, -0.0102],\n",
            "          [ 0.0410,  0.0606,  0.0254]],\n",
            "\n",
            "         [[ 0.0069, -0.0255,  0.0289],\n",
            "          [ 0.0318,  0.0241,  0.0486],\n",
            "          [-0.0134,  0.0059,  0.0003]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0159,  0.0702,  0.0082],\n",
            "          [ 0.0173,  0.0706,  0.0253],\n",
            "          [ 0.0378, -0.0198, -0.0128]],\n",
            "\n",
            "         [[ 0.0465,  0.0094,  0.0002],\n",
            "          [-0.0096, -0.0053, -0.0224],\n",
            "          [ 0.0239,  0.0210,  0.0116]],\n",
            "\n",
            "         [[-0.0166,  0.0187,  0.0458],\n",
            "          [-0.0601,  0.0207, -0.0077],\n",
            "          [-0.0318, -0.0477,  0.0100]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0211, -0.0230,  0.0062],\n",
            "          [ 0.0077,  0.0069,  0.0242],\n",
            "          [-0.0189, -0.0497,  0.0003]],\n",
            "\n",
            "         [[-0.0461, -0.0285, -0.0100],\n",
            "          [-0.0086, -0.0188,  0.0064],\n",
            "          [-0.0088,  0.0189,  0.0463]],\n",
            "\n",
            "         [[ 0.0065,  0.0183,  0.0470],\n",
            "          [-0.0145,  0.0021, -0.0291],\n",
            "          [-0.0226, -0.0405,  0.0024]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0298, -0.0093, -0.0619],\n",
            "          [-0.0346, -0.0093, -0.0432],\n",
            "          [ 0.0079,  0.0290,  0.0072]],\n",
            "\n",
            "         [[ 0.0020, -0.0165, -0.0352],\n",
            "          [-0.0498, -0.0562, -0.0060],\n",
            "          [-0.0540, -0.0387,  0.0220]],\n",
            "\n",
            "         [[ 0.0433,  0.0111,  0.0301],\n",
            "          [ 0.0059,  0.0024,  0.0338],\n",
            "          [ 0.0398,  0.0386,  0.0063]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.0070,  0.0297,  0.0078],\n",
            "          [ 0.0861,  0.0304,  0.0515],\n",
            "          [ 0.0961,  0.1021,  0.0971]],\n",
            "\n",
            "         [[ 0.0086,  0.0037,  0.0385],\n",
            "          [-0.0127,  0.0728,  0.0033],\n",
            "          [ 0.0226,  0.0570, -0.0080]],\n",
            "\n",
            "         [[-0.0258,  0.0186,  0.0215],\n",
            "          [ 0.0445,  0.0656,  0.0960],\n",
            "          [ 0.0476,  0.0679,  0.0492]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0337, -0.0151, -0.0058],\n",
            "          [-0.0046,  0.0419,  0.0447],\n",
            "          [ 0.0176, -0.0272,  0.0019]],\n",
            "\n",
            "         [[ 0.0066,  0.0070, -0.0473],\n",
            "          [ 0.0118, -0.0091, -0.0412],\n",
            "          [-0.0407,  0.0283,  0.0054]],\n",
            "\n",
            "         [[ 0.0122,  0.0912,  0.0985],\n",
            "          [-0.0473,  0.0255,  0.0532],\n",
            "          [-0.0160,  0.0789,  0.0642]]],\n",
            "\n",
            "\n",
            "        [[[-0.0192,  0.0661,  0.0825],\n",
            "          [ 0.0183,  0.0747,  0.1003],\n",
            "          [-0.0411,  0.0143,  0.0455]],\n",
            "\n",
            "         [[ 0.0247,  0.0662,  0.0477],\n",
            "          [ 0.0388,  0.0039,  0.0345],\n",
            "          [-0.0346, -0.0253, -0.0095]],\n",
            "\n",
            "         [[ 0.0077,  0.0445,  0.0702],\n",
            "          [ 0.1063,  0.1381,  0.1006],\n",
            "          [ 0.0378,  0.0638,  0.0868]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0010,  0.0022, -0.0057],\n",
            "          [-0.0076, -0.0238,  0.0255],\n",
            "          [-0.0218,  0.0120,  0.0401]],\n",
            "\n",
            "         [[ 0.0385, -0.0122,  0.0280],\n",
            "          [ 0.0018,  0.0332,  0.0152],\n",
            "          [ 0.0163, -0.0044,  0.0228]],\n",
            "\n",
            "         [[ 0.0052, -0.0076,  0.0292],\n",
            "          [ 0.0746,  0.0253, -0.0985],\n",
            "          [ 0.0497, -0.0400, -0.0489]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0373,  0.0035,  0.0446],\n",
            "          [ 0.0274, -0.0323, -0.0182],\n",
            "          [-0.0155, -0.0265, -0.0306]],\n",
            "\n",
            "         [[ 0.0407,  0.0498,  0.0052],\n",
            "          [ 0.0241,  0.0234, -0.0035],\n",
            "          [ 0.0353,  0.0282,  0.0583]],\n",
            "\n",
            "         [[ 0.0176,  0.0383,  0.0609],\n",
            "          [ 0.0279, -0.0042,  0.0029],\n",
            "          [ 0.0231,  0.0176,  0.0419]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0023, -0.0079, -0.0117],\n",
            "          [ 0.0146,  0.0181, -0.0081],\n",
            "          [ 0.0091,  0.0005, -0.0155]],\n",
            "\n",
            "         [[ 0.0392, -0.0071, -0.0016],\n",
            "          [-0.0456,  0.0241, -0.0138],\n",
            "          [-0.0150, -0.0310, -0.0087]],\n",
            "\n",
            "         [[ 0.0136, -0.0134, -0.0168],\n",
            "          [ 0.0579,  0.0019,  0.0587],\n",
            "          [-0.0201,  0.0152,  0.0520]]]], device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 2.5223e-02, -6.7030e-02,  2.5103e-01, -4.7806e-02, -2.6351e-02,\n",
            "         2.5393e-01,  5.4950e-02,  6.1424e-02,  8.5633e-06, -1.4331e-01,\n",
            "        -4.3093e-04,  1.0272e-01, -2.2347e-02,  4.0590e-01, -5.4142e-02,\n",
            "         3.3019e-03, -1.7162e-02,  1.3540e-01, -2.2758e-02,  2.9230e-01,\n",
            "         2.2749e-02, -7.8563e-02,  2.1746e-01, -3.3498e-02,  4.0971e-01,\n",
            "        -2.6998e-02,  1.5206e-01,  1.1368e-02,  3.5717e-02, -2.3405e-02,\n",
            "         7.5202e-02, -3.1353e-02, -7.0346e-05,  2.0347e-01, -2.9659e-02,\n",
            "        -5.1213e-02,  6.7883e-03, -2.5231e-02, -1.6708e-01, -6.4754e-02,\n",
            "         2.5218e-01,  1.4438e-01, -2.5866e-02,  5.2550e-03, -6.4121e-02,\n",
            "         6.3966e-02,  4.2533e-02,  1.1383e-01, -5.9676e-02,  7.1821e-03,\n",
            "        -6.4877e-02, -1.4902e-01,  1.4964e-02,  1.1508e-01, -5.8099e-02,\n",
            "        -4.9545e-02,  2.9462e-01,  1.2203e-01,  3.7885e-02,  2.5941e-02,\n",
            "         1.0078e-02, -1.2026e-02, -8.5287e-02, -1.3418e-02], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[ 3.4626e-02, -4.6595e-03,  1.4499e-02],\n",
            "          [ 1.4076e-02,  1.8905e-02, -1.8034e-03],\n",
            "          [ 5.1125e-03, -4.5117e-02,  2.4908e-02]],\n",
            "\n",
            "         [[-3.0814e-02,  3.0113e-02, -1.7621e-02],\n",
            "          [-1.0952e-02,  1.8643e-02,  5.2669e-02],\n",
            "          [-4.8650e-04,  2.8552e-02, -1.6121e-02]],\n",
            "\n",
            "         [[-1.4106e-02,  1.5565e-02,  4.2248e-02],\n",
            "          [ 1.9850e-02, -3.2039e-04, -2.8494e-02],\n",
            "          [-2.1500e-02, -2.2698e-02,  3.4731e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.2941e-02, -8.5006e-04, -2.3639e-02],\n",
            "          [-5.3040e-02,  1.5819e-02, -3.3155e-02],\n",
            "          [-2.0948e-03, -2.7850e-02, -2.7639e-02]],\n",
            "\n",
            "         [[-5.6677e-03, -3.3344e-02, -5.4079e-02],\n",
            "          [-4.9271e-02, -3.0883e-02,  9.6468e-03],\n",
            "          [ 8.9350e-03, -5.2298e-02, -1.9076e-02]],\n",
            "\n",
            "         [[-4.6774e-02,  2.6037e-03,  1.1574e-02],\n",
            "          [ 3.7149e-02, -2.1667e-02, -2.7277e-02],\n",
            "          [ 1.8603e-02, -1.8990e-02,  2.2000e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.2096e-02,  1.0393e-02,  1.9613e-02],\n",
            "          [-4.9768e-03,  2.6734e-04, -6.6517e-05],\n",
            "          [-1.7393e-02,  3.1685e-02, -2.2002e-02]],\n",
            "\n",
            "         [[-1.6925e-02,  2.6994e-02,  9.3463e-03],\n",
            "          [ 1.5674e-02, -1.1799e-02,  4.7955e-02],\n",
            "          [ 2.2364e-02,  3.7042e-02, -2.3655e-02]],\n",
            "\n",
            "         [[-2.6231e-02,  4.0988e-02, -3.4184e-02],\n",
            "          [-8.0955e-03,  4.5178e-02,  3.6090e-02],\n",
            "          [ 1.6891e-02, -2.6632e-02, -2.4496e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.3421e-03, -1.9242e-02,  1.8943e-02],\n",
            "          [ 2.8786e-03, -2.6919e-02, -1.8842e-02],\n",
            "          [-1.4960e-02, -3.0355e-02,  3.8281e-02]],\n",
            "\n",
            "         [[-3.9029e-02, -5.6352e-02, -6.3206e-02],\n",
            "          [ 1.8576e-02,  5.0691e-03, -3.6689e-02],\n",
            "          [-4.7043e-02,  1.1519e-02, -4.2204e-02]],\n",
            "\n",
            "         [[-7.4504e-03, -1.8083e-02,  2.5816e-02],\n",
            "          [ 2.2835e-02, -2.6359e-02, -5.7481e-04],\n",
            "          [-1.0854e-02, -1.3317e-02, -1.6285e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 2.0154e-02,  3.3552e-02,  3.9417e-03],\n",
            "          [-2.6881e-02,  1.1239e-03,  1.6238e-02],\n",
            "          [-1.4401e-02,  2.3622e-02,  3.9293e-02]],\n",
            "\n",
            "         [[-1.2789e-02,  1.5748e-03, -4.4629e-02],\n",
            "          [-4.4432e-02, -4.9217e-03, -5.0669e-02],\n",
            "          [-3.9893e-02,  2.0248e-02, -4.6002e-02]],\n",
            "\n",
            "         [[-2.0310e-02,  3.9012e-02,  1.9612e-02],\n",
            "          [ 2.3234e-02,  1.8031e-02, -2.8533e-02],\n",
            "          [-4.0584e-02,  2.7775e-02,  3.1457e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.8961e-02, -4.3207e-03, -3.7205e-02],\n",
            "          [-6.1907e-02, -2.1038e-02, -1.3063e-05],\n",
            "          [-2.2119e-02, -4.4255e-02,  7.4601e-03]],\n",
            "\n",
            "         [[ 1.1462e-02,  3.8933e-02,  1.3110e-02],\n",
            "          [ 4.5013e-02,  9.2387e-03, -6.1911e-04],\n",
            "          [-2.8198e-02, -3.6940e-02, -3.1382e-02]],\n",
            "\n",
            "         [[-1.9991e-03, -4.5632e-02, -7.3957e-05],\n",
            "          [-1.7897e-02,  6.5416e-03,  3.2468e-02],\n",
            "          [-2.8409e-03,  3.4759e-02, -3.6076e-03]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 3.0741e-02, -5.6853e-03,  2.2380e-02],\n",
            "          [ 5.6538e-03,  3.7641e-02,  1.3322e-02],\n",
            "          [-6.2035e-03,  4.0728e-02,  4.5531e-03]],\n",
            "\n",
            "         [[-3.8736e-02, -2.8141e-02, -7.1004e-03],\n",
            "          [ 3.0544e-02,  2.2781e-02,  1.0454e-02],\n",
            "          [-2.7979e-02, -3.8326e-02, -1.7178e-02]],\n",
            "\n",
            "         [[-1.0098e-02, -2.9304e-02, -2.0211e-02],\n",
            "          [-1.1979e-02,  5.9223e-03, -1.3749e-02],\n",
            "          [ 1.8142e-02, -1.9166e-02, -1.9517e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.7246e-02,  7.7194e-02,  4.3242e-02],\n",
            "          [ 1.8233e-02,  4.7718e-03,  4.2086e-02],\n",
            "          [-1.5213e-02, -3.9621e-02,  2.7031e-02]],\n",
            "\n",
            "         [[ 1.4855e-02, -1.8786e-02,  3.2044e-02],\n",
            "          [ 6.9414e-02,  8.4829e-02,  3.2204e-02],\n",
            "          [ 1.2784e-02,  5.9205e-02,  4.2990e-02]],\n",
            "\n",
            "         [[-4.4375e-02, -2.9822e-02,  2.8202e-02],\n",
            "          [-3.1405e-02, -7.6557e-03,  1.6898e-02],\n",
            "          [ 2.1664e-02, -2.6214e-02, -1.9131e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.9200e-02,  2.0480e-02, -8.6929e-03],\n",
            "          [ 1.9217e-02,  3.8905e-02, -1.2561e-02],\n",
            "          [-3.9574e-02,  1.4454e-02,  2.3903e-03]],\n",
            "\n",
            "         [[ 2.9691e-02, -3.4991e-02,  6.4891e-03],\n",
            "          [ 5.6950e-02, -1.7217e-02, -3.9653e-02],\n",
            "          [ 1.4340e-02, -5.7258e-02, -4.4735e-02]],\n",
            "\n",
            "         [[-5.4550e-03,  1.7119e-02, -3.0946e-02],\n",
            "          [ 1.2787e-02,  2.6508e-03, -2.7191e-02],\n",
            "          [-1.2514e-02, -5.3887e-03,  1.1168e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.0976e-02, -2.8717e-02,  5.3186e-03],\n",
            "          [ 5.1734e-03,  1.0014e-02, -5.1204e-02],\n",
            "          [ 1.2705e-02, -6.3588e-02, -5.1596e-02]],\n",
            "\n",
            "         [[-4.4269e-02,  9.5589e-03,  7.4079e-02],\n",
            "          [ 2.5183e-02,  6.5123e-02,  1.6874e-02],\n",
            "          [ 1.5227e-02, -6.5580e-03,  2.1850e-02]],\n",
            "\n",
            "         [[-2.5134e-02, -4.1426e-02,  9.7919e-03],\n",
            "          [ 1.7157e-02, -2.8346e-02, -2.8219e-02],\n",
            "          [ 4.4441e-02, -6.8879e-03,  2.2491e-02]]],\n",
            "\n",
            "\n",
            "        [[[-3.9323e-03, -1.1406e-03,  4.0809e-02],\n",
            "          [ 1.4711e-02,  1.8420e-02,  2.5556e-02],\n",
            "          [ 4.4488e-02,  9.0756e-03, -6.2122e-03]],\n",
            "\n",
            "         [[-3.8576e-02,  1.3468e-02, -6.1298e-03],\n",
            "          [-4.8631e-02, -2.3761e-03, -1.1938e-02],\n",
            "          [-1.4787e-03,  3.0961e-02, -1.5593e-02]],\n",
            "\n",
            "         [[-4.7041e-02,  3.3454e-03,  1.0405e-02],\n",
            "          [-3.3627e-02, -2.0960e-02,  2.5503e-02],\n",
            "          [ 1.8840e-03,  1.5590e-02,  2.3061e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.6847e-02, -1.5207e-02,  5.4271e-02],\n",
            "          [ 6.8698e-02,  6.1393e-02,  3.2795e-02],\n",
            "          [ 2.0875e-02,  7.6348e-02,  5.9368e-02]],\n",
            "\n",
            "         [[-9.6898e-03, -1.1035e-02, -3.9809e-02],\n",
            "          [ 2.2599e-02, -4.7233e-03,  2.7048e-02],\n",
            "          [ 3.6063e-02, -5.4300e-03, -2.1131e-02]],\n",
            "\n",
            "         [[ 1.7216e-02, -8.1123e-03,  2.0497e-03],\n",
            "          [ 3.5932e-02,  6.5652e-03, -9.0931e-03],\n",
            "          [ 3.2150e-02,  2.8699e-02,  1.5057e-02]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-2.3715e-02,  2.3861e-02, -7.1090e-02, -7.4776e-03,  1.5370e-01,\n",
            "         8.0764e-02, -4.9749e-02, -3.5784e-02, -2.8966e-02, -4.9894e-02,\n",
            "        -1.0697e-01, -1.7258e-02,  4.3389e-02, -5.0448e-04,  2.3130e-02,\n",
            "         9.2402e-02,  2.6930e-02, -2.6119e-02, -2.6679e-02,  7.0494e-03,\n",
            "        -5.7844e-02,  1.0948e-01,  2.2680e-01,  1.1913e-01, -1.1819e-02,\n",
            "        -1.0205e-02,  1.2527e-03, -2.2699e-03, -8.7090e-03, -8.7350e-03,\n",
            "        -6.7125e-02,  2.0293e-02, -1.8016e-02, -1.6932e-02,  6.7430e-02,\n",
            "        -1.3724e-02,  1.5317e-01, -6.3236e-02,  1.2970e-01, -1.1752e-03,\n",
            "        -6.6569e-02, -9.3255e-02, -2.9840e-02,  3.0209e-02,  3.2914e-02,\n",
            "        -1.1643e-03,  7.0609e-02,  1.7545e-02, -8.7578e-03,  7.9702e-03,\n",
            "         3.1932e-03, -2.4950e-02, -1.2741e-02, -4.5159e-02,  1.3691e-01,\n",
            "         1.1385e-01,  3.4217e-02,  5.8086e-02, -2.4186e-02,  6.4835e-02,\n",
            "        -6.8992e-02,  5.0952e-03, -1.6119e-02, -4.3861e-02,  1.0845e-02,\n",
            "         3.1170e-02, -1.6043e-02, -4.3304e-02, -6.5738e-02,  1.0401e-01,\n",
            "         7.8815e-02,  6.3466e-03,  7.3419e-02,  1.0201e-01,  5.5639e-02,\n",
            "         2.8595e-02,  1.1153e-01,  1.7598e-01, -5.5079e-02,  6.2890e-03,\n",
            "        -8.4849e-02,  2.1370e-02,  1.2054e-02,  1.6771e-02, -3.1937e-02,\n",
            "         1.4997e-01,  3.3789e-02,  5.6407e-02,  1.4560e-02,  5.5517e-02,\n",
            "         6.9355e-02,  3.4218e-02, -4.2589e-02, -2.7509e-02,  1.3539e-02,\n",
            "         8.8167e-03, -1.6076e-03,  2.7595e-04,  1.6985e-02, -3.5088e-02,\n",
            "        -1.1110e-03,  1.0746e-02,  3.5067e-02, -1.4428e-02, -8.9168e-02,\n",
            "         1.8559e-01,  2.7413e-02,  6.5041e-02,  2.4145e-03, -4.8447e-02,\n",
            "         7.0476e-04,  9.7666e-02,  5.8957e-02,  4.2890e-02,  8.9255e-03,\n",
            "         2.9361e-02,  2.3015e-02,  4.1605e-02, -1.8883e-04, -3.9358e-02,\n",
            "         2.5058e-02,  1.3592e-02,  1.0586e-01, -3.9561e-02,  4.1241e-04,\n",
            "        -9.1743e-03,  4.3953e-02,  1.9609e-02], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[[-3.5233e-02, -2.3770e-02,  5.8925e-03],\n",
            "          [-1.6700e-02,  7.1138e-03,  1.5006e-02],\n",
            "          [-3.9917e-03, -1.7893e-03, -8.0348e-04]],\n",
            "\n",
            "         [[-6.8629e-03, -3.4599e-02, -3.1392e-02],\n",
            "          [ 2.1459e-04, -7.6361e-05,  1.9168e-02],\n",
            "          [-8.7548e-03,  3.7297e-03,  4.2103e-03]],\n",
            "\n",
            "         [[-2.9791e-02, -1.5243e-02,  1.2616e-02],\n",
            "          [ 8.8411e-04,  3.6220e-03,  1.6133e-02],\n",
            "          [ 2.4319e-02,  3.3935e-02, -3.4608e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.1073e-03, -3.6275e-02, -3.3208e-02],\n",
            "          [-2.1561e-02,  7.7902e-03, -3.1871e-02],\n",
            "          [-3.6367e-02, -4.3885e-02, -4.0022e-02]],\n",
            "\n",
            "         [[-8.2886e-03,  2.5857e-02, -1.2933e-02],\n",
            "          [ 3.4558e-02, -1.3836e-02, -1.4900e-02],\n",
            "          [ 1.3430e-02, -3.3071e-02, -1.6434e-02]],\n",
            "\n",
            "         [[ 2.4955e-02, -2.8446e-02, -1.6878e-02],\n",
            "          [ 9.6389e-03,  4.4577e-03, -1.3638e-02],\n",
            "          [ 1.8844e-02, -2.2063e-02, -1.4159e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.4007e-02, -1.8871e-02, -2.1101e-02],\n",
            "          [-1.8805e-02, -1.4945e-03,  1.1506e-02],\n",
            "          [ 7.7183e-03,  8.0212e-03,  2.6266e-02]],\n",
            "\n",
            "         [[ 6.4536e-04,  1.8233e-02, -9.2371e-03],\n",
            "          [-1.0318e-02, -1.6446e-02, -1.5965e-02],\n",
            "          [ 1.1347e-02,  1.7162e-03,  6.8274e-04]],\n",
            "\n",
            "         [[-9.0624e-03,  1.9865e-02, -1.8608e-02],\n",
            "          [-3.3829e-02, -2.5829e-02,  3.4187e-02],\n",
            "          [ 1.6330e-02, -3.1897e-02, -1.4359e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.4868e-02,  3.7079e-02,  2.7095e-02],\n",
            "          [-6.0797e-03, -1.0632e-03, -9.4285e-03],\n",
            "          [ 7.5923e-03, -2.5109e-02, -3.3250e-02]],\n",
            "\n",
            "         [[-2.7092e-03,  2.7985e-02,  4.7927e-03],\n",
            "          [-1.5764e-02, -1.7569e-02,  1.8975e-02],\n",
            "          [ 1.1320e-03, -2.0955e-02, -1.6269e-02]],\n",
            "\n",
            "         [[ 4.1474e-03, -2.0806e-02, -1.8972e-02],\n",
            "          [-7.6547e-03, -8.3379e-03,  3.0023e-03],\n",
            "          [-2.1061e-02,  1.2516e-03,  1.6056e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.6502e-03, -3.4292e-02, -3.5787e-02],\n",
            "          [-1.5410e-02, -2.8428e-02, -2.6417e-02],\n",
            "          [ 8.5857e-03, -1.4984e-02, -1.1492e-02]],\n",
            "\n",
            "         [[ 7.9233e-03, -9.0605e-03,  2.2980e-02],\n",
            "          [-2.4555e-02,  2.9554e-02, -1.5374e-02],\n",
            "          [-4.0894e-03, -2.3321e-02, -2.0423e-02]],\n",
            "\n",
            "         [[-8.8321e-03, -1.4936e-02,  1.2841e-02],\n",
            "          [-3.0133e-03, -1.0199e-02, -3.4364e-02],\n",
            "          [-5.4099e-03,  1.5228e-02, -2.2896e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.6723e-02, -5.9633e-03, -1.0083e-02],\n",
            "          [-3.0935e-02, -5.8135e-03,  4.4064e-03],\n",
            "          [ 6.2743e-03,  1.8525e-02,  4.0319e-03]],\n",
            "\n",
            "         [[-2.2604e-02, -3.3843e-02, -2.5342e-02],\n",
            "          [-1.2649e-03, -8.6352e-03,  9.9989e-03],\n",
            "          [-4.3079e-03, -2.5510e-02, -4.9242e-02]],\n",
            "\n",
            "         [[ 1.7740e-03, -2.8115e-02, -4.4458e-03],\n",
            "          [-2.7375e-02, -1.2030e-02,  2.2782e-03],\n",
            "          [ 2.2603e-02, -5.4265e-03,  3.2432e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 2.4608e-02, -2.5437e-02,  2.1556e-02],\n",
            "          [ 3.0807e-02,  1.2477e-02, -2.4074e-02],\n",
            "          [ 2.7114e-02,  3.1476e-02,  9.6571e-03]],\n",
            "\n",
            "         [[-2.6372e-02, -1.7245e-02, -7.5359e-03],\n",
            "          [ 1.2581e-02,  1.0976e-02, -1.6428e-02],\n",
            "          [ 2.6984e-02, -7.8190e-03, -1.3050e-02]],\n",
            "\n",
            "         [[ 3.2362e-02,  2.8367e-02, -8.1259e-03],\n",
            "          [-1.0390e-02,  9.1102e-04, -2.2192e-02],\n",
            "          [-2.1308e-02,  5.9297e-03,  5.1805e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.1121e-02,  2.2675e-03, -8.6622e-03],\n",
            "          [ 2.0739e-02,  1.1482e-02,  1.5224e-02],\n",
            "          [ 1.7987e-02, -9.3996e-03,  1.7309e-02]],\n",
            "\n",
            "         [[ 2.8557e-02, -1.4422e-02,  2.2673e-02],\n",
            "          [ 5.4916e-03, -7.3762e-03, -1.1017e-02],\n",
            "          [ 8.5843e-03,  1.0738e-02, -1.8040e-02]],\n",
            "\n",
            "         [[ 6.6615e-04, -3.4710e-02, -3.1210e-02],\n",
            "          [-2.9635e-02, -1.3346e-02, -2.0811e-02],\n",
            "          [-6.5415e-03,  1.5305e-02, -3.0771e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.0585e-04, -9.2240e-03,  5.1441e-03],\n",
            "          [ 1.2043e-03, -3.2454e-02, -2.0950e-02],\n",
            "          [-9.2835e-03,  7.7588e-04, -2.8092e-02]],\n",
            "\n",
            "         [[-2.3910e-02, -2.2134e-04, -2.5663e-02],\n",
            "          [-2.2155e-02,  1.3960e-02, -7.5707e-03],\n",
            "          [-2.8894e-02,  3.6426e-03, -3.8127e-02]],\n",
            "\n",
            "         [[ 1.1127e-02, -3.9703e-02,  3.3917e-03],\n",
            "          [ 3.2932e-02,  2.0553e-02,  2.5474e-02],\n",
            "          [-2.3298e-02,  2.4178e-02, -1.3042e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.5560e-02,  6.3740e-04, -2.4987e-02],\n",
            "          [ 3.2255e-02,  3.2896e-02,  2.1262e-02],\n",
            "          [-1.4453e-02, -8.9813e-03,  6.8786e-03]],\n",
            "\n",
            "         [[ 2.6723e-03, -2.1797e-02,  9.3972e-03],\n",
            "          [ 4.2911e-02,  1.4935e-03, -3.2764e-02],\n",
            "          [-3.9824e-03, -6.5525e-03, -3.6285e-02]],\n",
            "\n",
            "         [[ 1.6835e-02,  1.9841e-02, -1.4901e-02],\n",
            "          [-2.5695e-02, -3.5772e-03, -7.9566e-03],\n",
            "          [ 1.5349e-02, -2.8715e-02, -2.7507e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 2.2520e-02, -2.9694e-02, -9.6571e-03],\n",
            "          [-4.4720e-04, -3.1266e-02,  2.4385e-02],\n",
            "          [-2.6584e-02, -2.0007e-02, -3.9698e-03]],\n",
            "\n",
            "         [[-1.5541e-03, -1.3736e-02,  3.1293e-03],\n",
            "          [-1.6671e-02,  1.2878e-02,  1.5099e-04],\n",
            "          [-7.0160e-03, -3.1486e-02,  7.5685e-03]],\n",
            "\n",
            "         [[ 2.6081e-02, -1.6629e-03, -3.6289e-02],\n",
            "          [ 2.6837e-02,  1.9523e-03,  2.9169e-02],\n",
            "          [-2.1998e-02,  3.4883e-02,  4.4808e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.2058e-02,  1.0674e-02, -2.5914e-02],\n",
            "          [-4.9889e-02, -8.5812e-03,  1.9211e-02],\n",
            "          [-4.4872e-02, -4.2417e-02,  7.6130e-03]],\n",
            "\n",
            "         [[ 3.1836e-02,  1.3182e-02, -4.0866e-03],\n",
            "          [ 2.5191e-02,  4.8571e-03, -7.8343e-03],\n",
            "          [-8.7786e-03,  2.2606e-02,  2.3510e-02]],\n",
            "\n",
            "         [[-1.3841e-02, -2.6470e-02,  1.2238e-02],\n",
            "          [-1.9655e-02,  9.5686e-03, -1.0282e-02],\n",
            "          [ 2.0481e-02,  8.4534e-03,  2.7784e-03]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 6.5764e-02, -3.5683e-02, -1.1497e-02, -1.5651e-02, -5.6180e-02,\n",
            "         7.3352e-02,  5.3025e-04, -6.9200e-03,  7.9908e-02, -9.2850e-03,\n",
            "         3.4429e-02, -2.4924e-02,  9.6388e-02, -1.3857e-02,  1.2093e-01,\n",
            "        -7.9739e-03, -3.1918e-02,  1.4954e-02,  6.7956e-02, -4.6734e-02,\n",
            "        -5.6940e-02, -6.8940e-03,  2.9657e-02, -3.7898e-02, -1.1406e-02,\n",
            "        -4.1812e-02, -7.9214e-02,  7.5223e-02, -4.9278e-03, -9.8886e-03,\n",
            "         4.0491e-02, -7.2542e-02, -9.5474e-03,  2.4482e-02,  7.9476e-02,\n",
            "         1.2145e-01, -1.4514e-02,  4.4787e-02,  5.1283e-02, -7.3459e-03,\n",
            "        -1.6087e-02,  2.9156e-02, -1.8771e-02, -3.1097e-02, -7.5149e-02,\n",
            "        -1.7682e-04, -4.3696e-02,  1.1455e-02, -2.8791e-02, -2.6728e-02,\n",
            "        -5.7734e-05, -2.0002e-02, -8.0030e-02, -8.9515e-03,  9.0767e-02,\n",
            "        -2.4848e-03, -3.0636e-02, -5.5087e-02, -2.4100e-02,  3.2193e-02,\n",
            "         1.0211e-02,  1.5776e-02, -1.7421e-02,  1.9532e-02, -2.1658e-02,\n",
            "         7.9575e-02,  9.7399e-02,  1.0131e-01, -5.0308e-02, -3.6936e-02,\n",
            "         1.7715e-02,  7.8757e-02, -4.1378e-02,  8.6555e-02,  2.0258e-02,\n",
            "         1.7104e-01,  2.4518e-02,  3.7341e-02, -1.9803e-02,  3.6117e-02,\n",
            "         4.2466e-02, -2.1380e-02, -9.4927e-02,  7.5221e-02, -5.5481e-04,\n",
            "         1.1948e-01, -4.9553e-03,  5.0898e-02,  5.6577e-02,  1.3049e-01,\n",
            "        -2.5716e-02, -3.3741e-02, -4.6883e-02, -3.4761e-02, -3.4833e-02,\n",
            "         4.5446e-02, -2.1609e-03, -2.8693e-03, -2.1178e-03, -1.1170e-02,\n",
            "         1.9495e-02, -1.7681e-02,  1.0063e-01, -7.9476e-03, -1.9035e-02,\n",
            "        -6.2964e-02,  2.4514e-02, -7.6044e-03, -5.7480e-02, -8.7730e-03,\n",
            "         8.9832e-03,  2.0721e-02, -1.0635e-01,  8.5132e-02,  8.3502e-03,\n",
            "        -2.0282e-02,  8.7324e-02,  3.3919e-02,  7.2621e-02, -3.4860e-02,\n",
            "        -4.0238e-02, -2.3661e-02, -9.4937e-03, -1.7020e-02, -5.5877e-03,\n",
            "         1.7843e-02, -7.4935e-02,  2.4408e-02], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 2.2028e-03,  1.4709e-03,  3.0148e-03,  ..., -1.0119e-03,\n",
            "         -2.1741e-03, -1.5711e-03],\n",
            "        [ 2.1094e-03,  6.5458e-03,  2.9443e-03,  ..., -1.7045e-04,\n",
            "         -4.2024e-03,  1.2213e-03],\n",
            "        [-6.2115e-04,  2.1193e-04, -1.3517e-03,  ..., -8.1871e-04,\n",
            "         -1.2601e-03,  1.5834e-04],\n",
            "        ...,\n",
            "        [ 1.9803e-04, -2.1803e-03, -1.5149e-03,  ..., -6.6710e-03,\n",
            "         -8.1932e-04, -1.3591e-04],\n",
            "        [ 1.5994e-04, -1.4079e-03, -1.5350e-03,  ..., -2.0174e-04,\n",
            "         -3.0950e-03, -4.9047e-03],\n",
            "        [ 1.1106e-03,  3.6411e-03,  4.9585e-05,  ..., -1.8086e-03,\n",
            "          5.2633e-04,  2.8258e-03]], device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 1.9766e-02,  2.7292e-02, -7.4502e-03,  3.7277e-03, -2.4223e-03,\n",
            "         2.1825e-02,  5.5891e-03, -3.7776e-03,  1.5831e-03,  4.8088e-03,\n",
            "        -5.6634e-03, -1.6082e-03,  1.3494e-03, -1.4438e-03, -9.9392e-03,\n",
            "        -8.6492e-03,  4.3441e-03,  2.6132e-03, -2.0935e-03,  1.2728e-02,\n",
            "         7.0502e-03, -1.2317e-03, -1.0740e-02, -3.1601e-04, -2.3839e-03,\n",
            "         8.1220e-04,  7.1384e-03, -1.3064e-03,  2.3592e-03, -1.0845e-02,\n",
            "        -6.9096e-03,  2.5008e-03, -8.0120e-03, -7.5177e-03, -2.9161e-04,\n",
            "         2.7964e-04, -2.8475e-03,  7.9571e-03,  9.2535e-03, -1.4872e-03,\n",
            "        -4.1106e-03,  1.5125e-02, -3.9263e-03,  2.7291e-02, -3.4479e-03,\n",
            "         4.0602e-03, -3.3972e-04, -1.1164e-04, -1.0928e-02, -1.0719e-02,\n",
            "         1.5186e-02, -2.8114e-03,  1.8364e-02,  3.5538e-04,  5.6847e-03,\n",
            "         4.5275e-03,  4.2965e-03,  2.1365e-03, -2.3030e-03,  1.3460e-02,\n",
            "         4.2752e-04,  1.7322e-02,  1.7794e-03,  1.4657e-03,  7.8786e-03,\n",
            "         5.6744e-03,  5.3714e-03, -3.9468e-03, -6.6443e-03,  2.4787e-03,\n",
            "         3.7910e-03,  1.5393e-02,  2.5718e-03,  1.1990e-02,  5.0136e-04,\n",
            "         7.1810e-03,  6.9034e-03,  1.3136e-02, -7.4546e-03, -1.1141e-02,\n",
            "        -1.0818e-02,  1.3442e-02,  4.3910e-03, -7.5722e-03,  1.2223e-04,\n",
            "        -8.3480e-03, -2.0080e-03,  2.4352e-03, -7.1845e-03, -4.1911e-04,\n",
            "        -1.3606e-03,  1.3172e-02,  9.9991e-03,  1.9974e-02,  8.9681e-03,\n",
            "         2.8159e-04, -5.8783e-03,  5.8321e-05,  6.4500e-04,  1.5398e-03,\n",
            "         9.7632e-03,  1.6674e-02, -3.5016e-03, -1.0507e-03,  9.9478e-04,\n",
            "         2.5366e-03,  1.8881e-02,  1.7663e-04,  3.8378e-03,  3.8474e-03,\n",
            "        -1.6868e-03,  1.5137e-02,  1.9823e-02, -1.3378e-03, -1.2007e-04,\n",
            "        -1.5705e-03,  7.3171e-03, -4.2489e-03,  4.3597e-03,  1.3061e-04,\n",
            "        -8.3107e-03, -6.2861e-04, -1.4505e-03,  1.2661e-02,  1.1852e-02,\n",
            "         4.5903e-03,  2.9290e-03, -6.5104e-03, -6.4470e-03, -1.9350e-02,\n",
            "        -7.8451e-03, -4.4800e-03,  4.2584e-03, -3.6150e-03,  1.3845e-03,\n",
            "         8.8203e-03, -1.0199e-02,  1.5953e-03,  1.4247e-03,  5.4326e-03,\n",
            "        -5.6327e-04, -1.4142e-02, -8.3301e-03,  4.2897e-04,  3.1655e-03,\n",
            "         3.1757e-03, -6.7762e-03,  1.6154e-03,  2.3675e-03,  1.2385e-02,\n",
            "         6.8188e-03,  9.9395e-03,  5.7764e-03,  8.1153e-03, -5.7309e-03,\n",
            "         1.4928e-02,  1.9655e-03, -1.6554e-06,  1.4028e-02, -1.1400e-03,\n",
            "         1.7247e-02, -3.0565e-03, -6.6128e-03, -5.0568e-03, -5.1290e-03,\n",
            "         1.2408e-03, -7.5152e-03,  1.3359e-02, -1.4800e-03, -3.8817e-03,\n",
            "        -6.8097e-03,  7.7920e-03, -3.9119e-03,  6.4606e-03,  6.1568e-04,\n",
            "        -2.5631e-03, -2.4155e-03, -2.9801e-04, -6.1919e-05,  1.1271e-02,\n",
            "         1.0207e-02,  1.6587e-02, -2.2878e-03, -3.2110e-03, -1.1574e-03,\n",
            "         3.3590e-03,  8.3084e-03, -5.9526e-03,  8.5053e-03,  1.3647e-03,\n",
            "         5.3848e-03, -2.9929e-04,  1.3828e-02,  2.3769e-03, -5.1293e-03,\n",
            "         5.3650e-03, -8.2381e-04, -6.1756e-03,  2.8239e-03,  4.1366e-03,\n",
            "         8.3145e-03, -4.4412e-03,  1.0083e-02,  8.2946e-03, -1.8662e-04,\n",
            "        -1.4656e-02,  5.4018e-03, -5.2671e-03, -1.2513e-03, -5.4324e-03,\n",
            "         1.6631e-03, -6.5916e-03, -1.2483e-03, -1.6975e-03, -8.5231e-03,\n",
            "        -7.6497e-03, -2.6215e-02,  2.3991e-02, -6.1909e-03,  9.9149e-03,\n",
            "        -2.6174e-03, -6.0589e-03,  8.4265e-03,  2.5438e-03,  6.5575e-04,\n",
            "         1.4049e-02,  1.4439e-02,  3.3497e-03,  1.9149e-03,  2.9212e-02,\n",
            "        -3.9427e-03,  1.7168e-02,  1.7253e-02, -9.6616e-03, -6.0491e-03,\n",
            "        -3.9117e-03, -3.0042e-03, -7.0862e-03,  2.0765e-03, -2.6589e-03,\n",
            "        -1.3381e-03,  5.8153e-03, -4.7640e-03,  2.2388e-03,  1.2325e-03,\n",
            "         6.2998e-03,  3.3719e-03, -1.0703e-02,  1.1885e-03, -8.8944e-03,\n",
            "        -1.1395e-03, -1.0231e-02,  1.7510e-03, -4.2428e-03, -4.9664e-03,\n",
            "         6.3337e-03], device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.0119,  0.0409, -0.0640,  ...,  0.0521,  0.0068,  0.0249],\n",
            "        [-0.0486,  0.0264,  0.0021,  ..., -0.0356,  0.0059, -0.0352],\n",
            "        [ 0.0629,  0.1130, -0.0943,  ..., -0.0686,  0.0840,  0.0553],\n",
            "        ...,\n",
            "        [-0.0308,  0.0149,  0.0382,  ..., -0.0408,  0.0249,  0.0752],\n",
            "        [ 0.0359, -0.0608, -0.0079,  ..., -0.0504, -0.0439, -0.0080],\n",
            "        [-0.0133,  0.0182,  0.0377,  ..., -0.0640,  0.0320, -0.0279]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0533, -0.0270,  0.0779,  0.0269, -0.0186, -0.0030, -0.0234, -0.0164,\n",
            "         0.0537,  0.0234, -0.0159,  0.0382, -0.0434, -0.0020,  0.0358,  0.0175,\n",
            "        -0.0298, -0.0276,  0.0190,  0.0311,  0.0194, -0.0275,  0.0462,  0.0272,\n",
            "         0.0561,  0.0498,  0.0107,  0.0690, -0.0132, -0.0215,  0.0630,  0.0413,\n",
            "         0.0129,  0.0123,  0.0413,  0.0534,  0.0281, -0.0561, -0.0096,  0.0374,\n",
            "         0.0403, -0.0349,  0.0382, -0.0049, -0.0220, -0.0131,  0.0601, -0.0142,\n",
            "        -0.0069,  0.0548, -0.0143, -0.0078, -0.0417,  0.0114,  0.0302,  0.0610,\n",
            "         0.0536, -0.0594, -0.0105, -0.0254, -0.0569,  0.0177,  0.0615,  0.0598,\n",
            "        -0.0557,  0.0245, -0.0076, -0.0571, -0.0269,  0.0416, -0.0015,  0.0410,\n",
            "         0.0073, -0.0539,  0.0057, -0.0541, -0.0250, -0.0086,  0.0328, -0.0610,\n",
            "        -0.0256,  0.0483,  0.0278,  0.0826,  0.0313,  0.0639, -0.0185,  0.0675,\n",
            "         0.0279,  0.0134, -0.0860, -0.0258,  0.0404, -0.0395, -0.0070,  0.0498,\n",
            "         0.0329,  0.0440, -0.0190,  0.0163, -0.0384, -0.0075, -0.0228,  0.0383,\n",
            "         0.0255,  0.0636,  0.0455, -0.0395,  0.0396, -0.0014,  0.0259, -0.0588,\n",
            "        -0.0469, -0.0516, -0.0327,  0.0225,  0.0171, -0.0204, -0.0174, -0.0217,\n",
            "        -0.0561,  0.0068,  0.0081, -0.0215, -0.0289, -0.0168, -0.0134,  0.0695,\n",
            "        -0.0587, -0.0347,  0.0214, -0.0229, -0.0350, -0.0196,  0.0384,  0.0159,\n",
            "         0.0896,  0.0340,  0.0433,  0.0672, -0.0228, -0.0513, -0.0585, -0.0152,\n",
            "         0.0519, -0.0356,  0.0041,  0.0265,  0.0391,  0.0050, -0.0439, -0.0016,\n",
            "         0.0705, -0.0378, -0.0599, -0.0445, -0.0129,  0.0321, -0.0106, -0.0237,\n",
            "        -0.0548, -0.0553,  0.0573,  0.0616, -0.0604,  0.0159,  0.0324, -0.0771,\n",
            "         0.0341,  0.0534, -0.0436,  0.0520, -0.0471,  0.0317,  0.0218, -0.0734,\n",
            "        -0.0369, -0.0347,  0.0260, -0.0579, -0.0380, -0.0288, -0.0508, -0.0207,\n",
            "        -0.0155, -0.0401, -0.0253,  0.0252,  0.0355,  0.0444, -0.0107, -0.0104,\n",
            "         0.0328,  0.0079, -0.0058,  0.0450,  0.0319,  0.0136,  0.0312,  0.0356,\n",
            "        -0.0509,  0.0008,  0.0608,  0.0297,  0.0437,  0.0339,  0.0324, -0.0275,\n",
            "         0.0037, -0.0537, -0.0012, -0.0058,  0.0533, -0.0071,  0.0574, -0.0547,\n",
            "         0.0171,  0.0012,  0.0497,  0.0653,  0.0071,  0.0481,  0.0421, -0.0493,\n",
            "         0.0480, -0.0374,  0.0970, -0.0154, -0.0199,  0.0227, -0.0467,  0.0459,\n",
            "        -0.0487,  0.0758, -0.0020, -0.0508,  0.0674,  0.0200,  0.0476, -0.0609,\n",
            "        -0.0085,  0.0317, -0.0114,  0.0925,  0.0316,  0.0423,  0.0360, -0.0472,\n",
            "         0.0627,  0.0008,  0.0130,  0.0131,  0.0429,  0.0463, -0.0061,  0.0265],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.0919, -0.1529, -0.1582,  ..., -0.1210, -0.0496,  0.0704],\n",
            "        [-0.0338,  0.0061, -0.1440,  ...,  0.0182,  0.0452,  0.0198],\n",
            "        [-0.1698,  0.0602, -0.0125,  ..., -0.0844, -0.0070,  0.2223],\n",
            "        ...,\n",
            "        [-0.0516, -0.1368,  0.0825,  ...,  0.0097,  0.0544, -0.0175],\n",
            "        [ 0.0088,  0.1071, -0.0258,  ...,  0.0820,  0.0160, -0.1237],\n",
            "        [ 0.0706, -0.0481,  0.1850,  ..., -0.0720,  0.0405,  0.0318]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0602, -0.1876, -0.0314, -0.0437,  0.1963, -0.0859,  0.1337, -0.0619,\n",
            "         0.0432, -0.0874], device='cuda:0', requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_params = [x.data for x in relu_model.parameters()]\n",
        "\n",
        "i=0\n",
        "for (name, params) in model.named_parameters():\n",
        "  if name[0:2]=='N1':\n",
        "    params.data = trained_params[i]\n",
        "    params.requires_grad = False\n",
        "    i+=1\n",
        "  print(name, params)"
      ],
      "metadata": {
        "id": "PLQO8iqgZoTs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7191ba2-a483-4198-cbb9-79be279c7750"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N1_C1.weight Parameter containing:\n",
            "tensor([[[[ 0.1850,  0.1115,  0.1831],\n",
            "          [-0.1391,  0.0578,  0.0505],\n",
            "          [-0.2225,  0.0643,  0.0665]],\n",
            "\n",
            "         [[ 0.2318,  0.0711,  0.2237],\n",
            "          [-0.2885, -0.1129, -0.1827],\n",
            "          [-0.2874, -0.0576,  0.1046]],\n",
            "\n",
            "         [[ 0.2614,  0.3017,  0.1691],\n",
            "          [-0.2203, -0.1606, -0.1812],\n",
            "          [-0.2189, -0.2360,  0.1888]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2074,  0.0269,  0.1690],\n",
            "          [ 0.0721, -0.0900,  0.0665],\n",
            "          [-0.1260, -0.3127, -0.1733]],\n",
            "\n",
            "         [[-0.1441,  0.2017, -0.0143],\n",
            "          [ 0.0981,  0.2157,  0.0939],\n",
            "          [-0.0933, -0.1735,  0.0905]],\n",
            "\n",
            "         [[-0.2129,  0.0109, -0.1249],\n",
            "          [ 0.2147,  0.2667,  0.0806],\n",
            "          [ 0.0045, -0.2233, -0.1043]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3584,  0.4198,  0.0082],\n",
            "          [-0.0666, -0.0327, -0.2260],\n",
            "          [-0.1696, -0.3413,  0.0915]],\n",
            "\n",
            "         [[ 0.3407,  0.2133,  0.2846],\n",
            "          [-0.2246, -0.2679, -0.2333],\n",
            "          [ 0.0398, -0.1506, -0.0129]],\n",
            "\n",
            "         [[-0.0582,  0.3015,  0.1911],\n",
            "          [-0.2736,  0.0127, -0.0768],\n",
            "          [ 0.0971, -0.2242, -0.0113]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.0516,  0.0951,  0.0048],\n",
            "          [-0.1990, -0.0611,  0.0823],\n",
            "          [ 0.0975,  0.1456, -0.1776]],\n",
            "\n",
            "         [[-0.0306,  0.0234, -0.1482],\n",
            "          [ 0.1720,  0.0797,  0.0585],\n",
            "          [ 0.1518,  0.0231,  0.0545]],\n",
            "\n",
            "         [[ 0.0991, -0.1346,  0.0729],\n",
            "          [ 0.0754,  0.0973,  0.1120],\n",
            "          [ 0.1782,  0.1899, -0.1961]]],\n",
            "\n",
            "\n",
            "        [[[-0.0476,  0.0185, -0.1424],\n",
            "          [-0.0975,  0.0724, -0.1751],\n",
            "          [-0.2256, -0.1652, -0.2199]],\n",
            "\n",
            "         [[ 0.0870,  0.2384,  0.1185],\n",
            "          [-0.0255,  0.0211, -0.1359],\n",
            "          [-0.1383,  0.0971, -0.0132]],\n",
            "\n",
            "         [[-0.1139,  0.2112,  0.1243],\n",
            "          [-0.0934,  0.1213,  0.0802],\n",
            "          [-0.1705,  0.1090,  0.0710]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0803,  0.0805, -0.2928],\n",
            "          [ 0.2636,  0.1788, -0.3373],\n",
            "          [ 0.2163,  0.0973, -0.0495]],\n",
            "\n",
            "         [[ 0.2925, -0.1340,  0.0446],\n",
            "          [ 0.3892, -0.1039, -0.1950],\n",
            "          [ 0.1041,  0.1922, -0.1485]],\n",
            "\n",
            "         [[-0.0089, -0.0341, -0.1952],\n",
            "          [-0.0444, -0.0433, -0.1957],\n",
            "          [ 0.0422, -0.1094, -0.3230]]]], device='cuda:0')\n",
            "N1_C1.bias Parameter containing:\n",
            "tensor([-0.0301,  0.0105, -0.0469,  0.0473, -0.1475, -0.0746,  0.0011, -0.1366,\n",
            "         0.1881,  0.2837,  0.2384, -0.1048, -0.2568, -0.1633, -0.1894,  0.0096,\n",
            "        -0.0616,  0.1114, -0.0298,  0.0116,  0.0063, -0.0164,  0.0137,  0.2229,\n",
            "         0.1321,  0.0428, -0.0016, -0.2139, -0.0366, -0.0172,  0.1737, -0.0777,\n",
            "        -0.0188,  0.0567, -0.0620,  0.2354, -0.0137,  0.1631, -0.2529, -0.0372,\n",
            "         0.2649, -0.0347,  0.1135, -0.0412, -0.1295,  0.3023,  0.3410, -0.0666,\n",
            "        -0.1154, -0.0153, -0.0106,  0.1522, -0.2352,  0.2969, -0.0158,  0.1157,\n",
            "         0.2923,  0.0673, -0.0261,  0.0478,  0.1350, -0.1102, -0.0135,  0.1241],\n",
            "       device='cuda:0')\n",
            "N1_C2.weight Parameter containing:\n",
            "tensor([[[[ 0.0517, -0.0175,  0.0301],\n",
            "          [ 0.0410,  0.0211, -0.0031],\n",
            "          [-0.0120, -0.0078, -0.0242]],\n",
            "\n",
            "         [[-0.0473, -0.0400,  0.0240],\n",
            "          [-0.0406, -0.0158, -0.0263],\n",
            "          [-0.0443, -0.0169,  0.0073]],\n",
            "\n",
            "         [[ 0.0126,  0.0021,  0.0048],\n",
            "          [-0.0199,  0.0094, -0.0055],\n",
            "          [-0.0348,  0.0245,  0.0062]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0384, -0.0219, -0.0393],\n",
            "          [-0.0270, -0.0406,  0.0213],\n",
            "          [-0.0244,  0.0025,  0.0373]],\n",
            "\n",
            "         [[ 0.0030,  0.0274, -0.0135],\n",
            "          [-0.0163, -0.0280, -0.0392],\n",
            "          [ 0.0035,  0.0325, -0.0009]],\n",
            "\n",
            "         [[-0.0321,  0.0352,  0.0327],\n",
            "          [-0.0218, -0.0307, -0.0209],\n",
            "          [-0.0034,  0.0301,  0.0322]]],\n",
            "\n",
            "\n",
            "        [[[-0.0041, -0.0312,  0.0066],\n",
            "          [-0.0284, -0.0228, -0.0140],\n",
            "          [ 0.0339, -0.0066,  0.0448]],\n",
            "\n",
            "         [[ 0.0318,  0.0613,  0.0134],\n",
            "          [-0.0115,  0.0432, -0.0102],\n",
            "          [ 0.0410,  0.0606,  0.0254]],\n",
            "\n",
            "         [[ 0.0069, -0.0255,  0.0289],\n",
            "          [ 0.0318,  0.0241,  0.0486],\n",
            "          [-0.0134,  0.0059,  0.0003]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0159,  0.0702,  0.0082],\n",
            "          [ 0.0173,  0.0706,  0.0253],\n",
            "          [ 0.0378, -0.0198, -0.0128]],\n",
            "\n",
            "         [[ 0.0465,  0.0094,  0.0002],\n",
            "          [-0.0096, -0.0053, -0.0224],\n",
            "          [ 0.0239,  0.0210,  0.0116]],\n",
            "\n",
            "         [[-0.0166,  0.0187,  0.0458],\n",
            "          [-0.0601,  0.0207, -0.0077],\n",
            "          [-0.0318, -0.0477,  0.0100]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0211, -0.0230,  0.0062],\n",
            "          [ 0.0077,  0.0069,  0.0242],\n",
            "          [-0.0189, -0.0497,  0.0003]],\n",
            "\n",
            "         [[-0.0461, -0.0285, -0.0100],\n",
            "          [-0.0086, -0.0188,  0.0064],\n",
            "          [-0.0088,  0.0189,  0.0463]],\n",
            "\n",
            "         [[ 0.0065,  0.0183,  0.0470],\n",
            "          [-0.0145,  0.0021, -0.0291],\n",
            "          [-0.0226, -0.0405,  0.0024]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0298, -0.0093, -0.0619],\n",
            "          [-0.0346, -0.0093, -0.0432],\n",
            "          [ 0.0079,  0.0290,  0.0072]],\n",
            "\n",
            "         [[ 0.0020, -0.0165, -0.0352],\n",
            "          [-0.0498, -0.0562, -0.0060],\n",
            "          [-0.0540, -0.0387,  0.0220]],\n",
            "\n",
            "         [[ 0.0433,  0.0111,  0.0301],\n",
            "          [ 0.0059,  0.0024,  0.0338],\n",
            "          [ 0.0398,  0.0386,  0.0063]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.0070,  0.0297,  0.0078],\n",
            "          [ 0.0861,  0.0304,  0.0515],\n",
            "          [ 0.0961,  0.1021,  0.0971]],\n",
            "\n",
            "         [[ 0.0086,  0.0037,  0.0385],\n",
            "          [-0.0127,  0.0728,  0.0033],\n",
            "          [ 0.0226,  0.0570, -0.0080]],\n",
            "\n",
            "         [[-0.0258,  0.0186,  0.0215],\n",
            "          [ 0.0445,  0.0656,  0.0960],\n",
            "          [ 0.0476,  0.0679,  0.0492]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0337, -0.0151, -0.0058],\n",
            "          [-0.0046,  0.0419,  0.0447],\n",
            "          [ 0.0176, -0.0272,  0.0019]],\n",
            "\n",
            "         [[ 0.0066,  0.0070, -0.0473],\n",
            "          [ 0.0118, -0.0091, -0.0412],\n",
            "          [-0.0407,  0.0283,  0.0054]],\n",
            "\n",
            "         [[ 0.0122,  0.0912,  0.0985],\n",
            "          [-0.0473,  0.0255,  0.0532],\n",
            "          [-0.0160,  0.0789,  0.0642]]],\n",
            "\n",
            "\n",
            "        [[[-0.0192,  0.0661,  0.0825],\n",
            "          [ 0.0183,  0.0747,  0.1003],\n",
            "          [-0.0411,  0.0143,  0.0455]],\n",
            "\n",
            "         [[ 0.0247,  0.0662,  0.0477],\n",
            "          [ 0.0388,  0.0039,  0.0345],\n",
            "          [-0.0346, -0.0253, -0.0095]],\n",
            "\n",
            "         [[ 0.0077,  0.0445,  0.0702],\n",
            "          [ 0.1063,  0.1381,  0.1006],\n",
            "          [ 0.0378,  0.0638,  0.0868]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0010,  0.0022, -0.0057],\n",
            "          [-0.0076, -0.0238,  0.0255],\n",
            "          [-0.0218,  0.0120,  0.0401]],\n",
            "\n",
            "         [[ 0.0385, -0.0122,  0.0280],\n",
            "          [ 0.0018,  0.0332,  0.0152],\n",
            "          [ 0.0163, -0.0044,  0.0228]],\n",
            "\n",
            "         [[ 0.0052, -0.0076,  0.0292],\n",
            "          [ 0.0746,  0.0253, -0.0985],\n",
            "          [ 0.0497, -0.0400, -0.0489]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0373,  0.0035,  0.0446],\n",
            "          [ 0.0274, -0.0323, -0.0182],\n",
            "          [-0.0155, -0.0265, -0.0306]],\n",
            "\n",
            "         [[ 0.0407,  0.0498,  0.0052],\n",
            "          [ 0.0241,  0.0234, -0.0035],\n",
            "          [ 0.0353,  0.0282,  0.0583]],\n",
            "\n",
            "         [[ 0.0176,  0.0383,  0.0609],\n",
            "          [ 0.0279, -0.0042,  0.0029],\n",
            "          [ 0.0231,  0.0176,  0.0419]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0023, -0.0079, -0.0117],\n",
            "          [ 0.0146,  0.0181, -0.0081],\n",
            "          [ 0.0091,  0.0005, -0.0155]],\n",
            "\n",
            "         [[ 0.0392, -0.0071, -0.0016],\n",
            "          [-0.0456,  0.0241, -0.0138],\n",
            "          [-0.0150, -0.0310, -0.0087]],\n",
            "\n",
            "         [[ 0.0136, -0.0134, -0.0168],\n",
            "          [ 0.0579,  0.0019,  0.0587],\n",
            "          [-0.0201,  0.0152,  0.0520]]]], device='cuda:0')\n",
            "N1_C2.bias Parameter containing:\n",
            "tensor([ 2.5223e-02, -6.7030e-02,  2.5103e-01, -4.7806e-02, -2.6351e-02,\n",
            "         2.5393e-01,  5.4950e-02,  6.1424e-02,  8.5633e-06, -1.4331e-01,\n",
            "        -4.3093e-04,  1.0272e-01, -2.2347e-02,  4.0590e-01, -5.4142e-02,\n",
            "         3.3019e-03, -1.7162e-02,  1.3540e-01, -2.2758e-02,  2.9230e-01,\n",
            "         2.2749e-02, -7.8563e-02,  2.1746e-01, -3.3498e-02,  4.0971e-01,\n",
            "        -2.6998e-02,  1.5206e-01,  1.1368e-02,  3.5717e-02, -2.3405e-02,\n",
            "         7.5202e-02, -3.1353e-02, -7.0346e-05,  2.0347e-01, -2.9659e-02,\n",
            "        -5.1213e-02,  6.7883e-03, -2.5231e-02, -1.6708e-01, -6.4754e-02,\n",
            "         2.5218e-01,  1.4438e-01, -2.5866e-02,  5.2550e-03, -6.4121e-02,\n",
            "         6.3966e-02,  4.2533e-02,  1.1383e-01, -5.9676e-02,  7.1821e-03,\n",
            "        -6.4877e-02, -1.4902e-01,  1.4964e-02,  1.1508e-01, -5.8099e-02,\n",
            "        -4.9545e-02,  2.9462e-01,  1.2203e-01,  3.7885e-02,  2.5941e-02,\n",
            "         1.0078e-02, -1.2026e-02, -8.5287e-02, -1.3418e-02], device='cuda:0')\n",
            "N1_C3.weight Parameter containing:\n",
            "tensor([[[[ 3.4626e-02, -4.6595e-03,  1.4499e-02],\n",
            "          [ 1.4076e-02,  1.8905e-02, -1.8034e-03],\n",
            "          [ 5.1125e-03, -4.5117e-02,  2.4908e-02]],\n",
            "\n",
            "         [[-3.0814e-02,  3.0113e-02, -1.7621e-02],\n",
            "          [-1.0952e-02,  1.8643e-02,  5.2669e-02],\n",
            "          [-4.8650e-04,  2.8552e-02, -1.6121e-02]],\n",
            "\n",
            "         [[-1.4106e-02,  1.5565e-02,  4.2248e-02],\n",
            "          [ 1.9850e-02, -3.2039e-04, -2.8494e-02],\n",
            "          [-2.1500e-02, -2.2698e-02,  3.4731e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.2941e-02, -8.5006e-04, -2.3639e-02],\n",
            "          [-5.3040e-02,  1.5819e-02, -3.3155e-02],\n",
            "          [-2.0948e-03, -2.7850e-02, -2.7639e-02]],\n",
            "\n",
            "         [[-5.6677e-03, -3.3344e-02, -5.4079e-02],\n",
            "          [-4.9271e-02, -3.0883e-02,  9.6468e-03],\n",
            "          [ 8.9350e-03, -5.2298e-02, -1.9076e-02]],\n",
            "\n",
            "         [[-4.6774e-02,  2.6037e-03,  1.1574e-02],\n",
            "          [ 3.7149e-02, -2.1667e-02, -2.7277e-02],\n",
            "          [ 1.8603e-02, -1.8990e-02,  2.2000e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.2096e-02,  1.0393e-02,  1.9613e-02],\n",
            "          [-4.9768e-03,  2.6734e-04, -6.6517e-05],\n",
            "          [-1.7393e-02,  3.1685e-02, -2.2002e-02]],\n",
            "\n",
            "         [[-1.6925e-02,  2.6994e-02,  9.3463e-03],\n",
            "          [ 1.5674e-02, -1.1799e-02,  4.7955e-02],\n",
            "          [ 2.2364e-02,  3.7042e-02, -2.3655e-02]],\n",
            "\n",
            "         [[-2.6231e-02,  4.0988e-02, -3.4184e-02],\n",
            "          [-8.0955e-03,  4.5178e-02,  3.6090e-02],\n",
            "          [ 1.6891e-02, -2.6632e-02, -2.4496e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.3421e-03, -1.9242e-02,  1.8943e-02],\n",
            "          [ 2.8786e-03, -2.6919e-02, -1.8842e-02],\n",
            "          [-1.4960e-02, -3.0355e-02,  3.8281e-02]],\n",
            "\n",
            "         [[-3.9029e-02, -5.6352e-02, -6.3206e-02],\n",
            "          [ 1.8576e-02,  5.0691e-03, -3.6689e-02],\n",
            "          [-4.7043e-02,  1.1519e-02, -4.2204e-02]],\n",
            "\n",
            "         [[-7.4504e-03, -1.8083e-02,  2.5816e-02],\n",
            "          [ 2.2835e-02, -2.6359e-02, -5.7481e-04],\n",
            "          [-1.0854e-02, -1.3317e-02, -1.6285e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 2.0154e-02,  3.3552e-02,  3.9417e-03],\n",
            "          [-2.6881e-02,  1.1239e-03,  1.6238e-02],\n",
            "          [-1.4401e-02,  2.3622e-02,  3.9293e-02]],\n",
            "\n",
            "         [[-1.2789e-02,  1.5748e-03, -4.4629e-02],\n",
            "          [-4.4432e-02, -4.9217e-03, -5.0669e-02],\n",
            "          [-3.9893e-02,  2.0248e-02, -4.6002e-02]],\n",
            "\n",
            "         [[-2.0310e-02,  3.9012e-02,  1.9612e-02],\n",
            "          [ 2.3234e-02,  1.8031e-02, -2.8533e-02],\n",
            "          [-4.0584e-02,  2.7775e-02,  3.1457e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.8961e-02, -4.3207e-03, -3.7205e-02],\n",
            "          [-6.1907e-02, -2.1038e-02, -1.3063e-05],\n",
            "          [-2.2119e-02, -4.4255e-02,  7.4601e-03]],\n",
            "\n",
            "         [[ 1.1462e-02,  3.8933e-02,  1.3110e-02],\n",
            "          [ 4.5013e-02,  9.2387e-03, -6.1911e-04],\n",
            "          [-2.8198e-02, -3.6940e-02, -3.1382e-02]],\n",
            "\n",
            "         [[-1.9991e-03, -4.5632e-02, -7.3957e-05],\n",
            "          [-1.7897e-02,  6.5416e-03,  3.2468e-02],\n",
            "          [-2.8409e-03,  3.4759e-02, -3.6076e-03]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 3.0741e-02, -5.6853e-03,  2.2380e-02],\n",
            "          [ 5.6538e-03,  3.7641e-02,  1.3322e-02],\n",
            "          [-6.2035e-03,  4.0728e-02,  4.5531e-03]],\n",
            "\n",
            "         [[-3.8736e-02, -2.8141e-02, -7.1004e-03],\n",
            "          [ 3.0544e-02,  2.2781e-02,  1.0454e-02],\n",
            "          [-2.7979e-02, -3.8326e-02, -1.7178e-02]],\n",
            "\n",
            "         [[-1.0098e-02, -2.9304e-02, -2.0211e-02],\n",
            "          [-1.1979e-02,  5.9223e-03, -1.3749e-02],\n",
            "          [ 1.8142e-02, -1.9166e-02, -1.9517e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.7246e-02,  7.7194e-02,  4.3242e-02],\n",
            "          [ 1.8233e-02,  4.7718e-03,  4.2086e-02],\n",
            "          [-1.5213e-02, -3.9621e-02,  2.7031e-02]],\n",
            "\n",
            "         [[ 1.4855e-02, -1.8786e-02,  3.2044e-02],\n",
            "          [ 6.9414e-02,  8.4829e-02,  3.2204e-02],\n",
            "          [ 1.2784e-02,  5.9205e-02,  4.2990e-02]],\n",
            "\n",
            "         [[-4.4375e-02, -2.9822e-02,  2.8202e-02],\n",
            "          [-3.1405e-02, -7.6557e-03,  1.6898e-02],\n",
            "          [ 2.1664e-02, -2.6214e-02, -1.9131e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.9200e-02,  2.0480e-02, -8.6929e-03],\n",
            "          [ 1.9217e-02,  3.8905e-02, -1.2561e-02],\n",
            "          [-3.9574e-02,  1.4454e-02,  2.3903e-03]],\n",
            "\n",
            "         [[ 2.9691e-02, -3.4991e-02,  6.4891e-03],\n",
            "          [ 5.6950e-02, -1.7217e-02, -3.9653e-02],\n",
            "          [ 1.4340e-02, -5.7258e-02, -4.4735e-02]],\n",
            "\n",
            "         [[-5.4550e-03,  1.7119e-02, -3.0946e-02],\n",
            "          [ 1.2787e-02,  2.6508e-03, -2.7191e-02],\n",
            "          [-1.2514e-02, -5.3887e-03,  1.1168e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.0976e-02, -2.8717e-02,  5.3186e-03],\n",
            "          [ 5.1734e-03,  1.0014e-02, -5.1204e-02],\n",
            "          [ 1.2705e-02, -6.3588e-02, -5.1596e-02]],\n",
            "\n",
            "         [[-4.4269e-02,  9.5589e-03,  7.4079e-02],\n",
            "          [ 2.5183e-02,  6.5123e-02,  1.6874e-02],\n",
            "          [ 1.5227e-02, -6.5580e-03,  2.1850e-02]],\n",
            "\n",
            "         [[-2.5134e-02, -4.1426e-02,  9.7919e-03],\n",
            "          [ 1.7157e-02, -2.8346e-02, -2.8219e-02],\n",
            "          [ 4.4441e-02, -6.8879e-03,  2.2491e-02]]],\n",
            "\n",
            "\n",
            "        [[[-3.9323e-03, -1.1406e-03,  4.0809e-02],\n",
            "          [ 1.4711e-02,  1.8420e-02,  2.5556e-02],\n",
            "          [ 4.4488e-02,  9.0756e-03, -6.2122e-03]],\n",
            "\n",
            "         [[-3.8576e-02,  1.3468e-02, -6.1298e-03],\n",
            "          [-4.8631e-02, -2.3761e-03, -1.1938e-02],\n",
            "          [-1.4787e-03,  3.0961e-02, -1.5593e-02]],\n",
            "\n",
            "         [[-4.7041e-02,  3.3454e-03,  1.0405e-02],\n",
            "          [-3.3627e-02, -2.0960e-02,  2.5503e-02],\n",
            "          [ 1.8840e-03,  1.5590e-02,  2.3061e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.6847e-02, -1.5207e-02,  5.4271e-02],\n",
            "          [ 6.8698e-02,  6.1393e-02,  3.2795e-02],\n",
            "          [ 2.0875e-02,  7.6348e-02,  5.9368e-02]],\n",
            "\n",
            "         [[-9.6898e-03, -1.1035e-02, -3.9809e-02],\n",
            "          [ 2.2599e-02, -4.7233e-03,  2.7048e-02],\n",
            "          [ 3.6063e-02, -5.4300e-03, -2.1131e-02]],\n",
            "\n",
            "         [[ 1.7216e-02, -8.1123e-03,  2.0497e-03],\n",
            "          [ 3.5932e-02,  6.5652e-03, -9.0931e-03],\n",
            "          [ 3.2150e-02,  2.8699e-02,  1.5057e-02]]]], device='cuda:0')\n",
            "N1_C3.bias Parameter containing:\n",
            "tensor([-2.3715e-02,  2.3861e-02, -7.1090e-02, -7.4776e-03,  1.5370e-01,\n",
            "         8.0764e-02, -4.9749e-02, -3.5784e-02, -2.8966e-02, -4.9894e-02,\n",
            "        -1.0697e-01, -1.7258e-02,  4.3389e-02, -5.0448e-04,  2.3130e-02,\n",
            "         9.2402e-02,  2.6930e-02, -2.6119e-02, -2.6679e-02,  7.0494e-03,\n",
            "        -5.7844e-02,  1.0948e-01,  2.2680e-01,  1.1913e-01, -1.1819e-02,\n",
            "        -1.0205e-02,  1.2527e-03, -2.2699e-03, -8.7090e-03, -8.7350e-03,\n",
            "        -6.7125e-02,  2.0293e-02, -1.8016e-02, -1.6932e-02,  6.7430e-02,\n",
            "        -1.3724e-02,  1.5317e-01, -6.3236e-02,  1.2970e-01, -1.1752e-03,\n",
            "        -6.6569e-02, -9.3255e-02, -2.9840e-02,  3.0209e-02,  3.2914e-02,\n",
            "        -1.1643e-03,  7.0609e-02,  1.7545e-02, -8.7578e-03,  7.9702e-03,\n",
            "         3.1932e-03, -2.4950e-02, -1.2741e-02, -4.5159e-02,  1.3691e-01,\n",
            "         1.1385e-01,  3.4217e-02,  5.8086e-02, -2.4186e-02,  6.4835e-02,\n",
            "        -6.8992e-02,  5.0952e-03, -1.6119e-02, -4.3861e-02,  1.0845e-02,\n",
            "         3.1170e-02, -1.6043e-02, -4.3304e-02, -6.5738e-02,  1.0401e-01,\n",
            "         7.8815e-02,  6.3466e-03,  7.3419e-02,  1.0201e-01,  5.5639e-02,\n",
            "         2.8595e-02,  1.1153e-01,  1.7598e-01, -5.5079e-02,  6.2890e-03,\n",
            "        -8.4849e-02,  2.1370e-02,  1.2054e-02,  1.6771e-02, -3.1937e-02,\n",
            "         1.4997e-01,  3.3789e-02,  5.6407e-02,  1.4560e-02,  5.5517e-02,\n",
            "         6.9355e-02,  3.4218e-02, -4.2589e-02, -2.7509e-02,  1.3539e-02,\n",
            "         8.8167e-03, -1.6076e-03,  2.7595e-04,  1.6985e-02, -3.5088e-02,\n",
            "        -1.1110e-03,  1.0746e-02,  3.5067e-02, -1.4428e-02, -8.9168e-02,\n",
            "         1.8559e-01,  2.7413e-02,  6.5041e-02,  2.4145e-03, -4.8447e-02,\n",
            "         7.0476e-04,  9.7666e-02,  5.8957e-02,  4.2890e-02,  8.9255e-03,\n",
            "         2.9361e-02,  2.3015e-02,  4.1605e-02, -1.8883e-04, -3.9358e-02,\n",
            "         2.5058e-02,  1.3592e-02,  1.0586e-01, -3.9561e-02,  4.1241e-04,\n",
            "        -9.1743e-03,  4.3953e-02,  1.9609e-02], device='cuda:0')\n",
            "N1_C4.weight Parameter containing:\n",
            "tensor([[[[-3.5233e-02, -2.3770e-02,  5.8925e-03],\n",
            "          [-1.6700e-02,  7.1138e-03,  1.5006e-02],\n",
            "          [-3.9917e-03, -1.7893e-03, -8.0348e-04]],\n",
            "\n",
            "         [[-6.8629e-03, -3.4599e-02, -3.1392e-02],\n",
            "          [ 2.1459e-04, -7.6361e-05,  1.9168e-02],\n",
            "          [-8.7548e-03,  3.7297e-03,  4.2103e-03]],\n",
            "\n",
            "         [[-2.9791e-02, -1.5243e-02,  1.2616e-02],\n",
            "          [ 8.8411e-04,  3.6220e-03,  1.6133e-02],\n",
            "          [ 2.4319e-02,  3.3935e-02, -3.4608e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.1073e-03, -3.6275e-02, -3.3208e-02],\n",
            "          [-2.1561e-02,  7.7902e-03, -3.1871e-02],\n",
            "          [-3.6367e-02, -4.3885e-02, -4.0022e-02]],\n",
            "\n",
            "         [[-8.2886e-03,  2.5857e-02, -1.2933e-02],\n",
            "          [ 3.4558e-02, -1.3836e-02, -1.4900e-02],\n",
            "          [ 1.3430e-02, -3.3071e-02, -1.6434e-02]],\n",
            "\n",
            "         [[ 2.4955e-02, -2.8446e-02, -1.6878e-02],\n",
            "          [ 9.6389e-03,  4.4577e-03, -1.3638e-02],\n",
            "          [ 1.8844e-02, -2.2063e-02, -1.4159e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.4007e-02, -1.8871e-02, -2.1101e-02],\n",
            "          [-1.8805e-02, -1.4945e-03,  1.1506e-02],\n",
            "          [ 7.7183e-03,  8.0212e-03,  2.6266e-02]],\n",
            "\n",
            "         [[ 6.4536e-04,  1.8233e-02, -9.2371e-03],\n",
            "          [-1.0318e-02, -1.6446e-02, -1.5965e-02],\n",
            "          [ 1.1347e-02,  1.7162e-03,  6.8274e-04]],\n",
            "\n",
            "         [[-9.0624e-03,  1.9865e-02, -1.8608e-02],\n",
            "          [-3.3829e-02, -2.5829e-02,  3.4187e-02],\n",
            "          [ 1.6330e-02, -3.1897e-02, -1.4359e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.4868e-02,  3.7079e-02,  2.7095e-02],\n",
            "          [-6.0797e-03, -1.0632e-03, -9.4285e-03],\n",
            "          [ 7.5923e-03, -2.5109e-02, -3.3250e-02]],\n",
            "\n",
            "         [[-2.7092e-03,  2.7985e-02,  4.7927e-03],\n",
            "          [-1.5764e-02, -1.7569e-02,  1.8975e-02],\n",
            "          [ 1.1320e-03, -2.0955e-02, -1.6269e-02]],\n",
            "\n",
            "         [[ 4.1474e-03, -2.0806e-02, -1.8972e-02],\n",
            "          [-7.6547e-03, -8.3379e-03,  3.0023e-03],\n",
            "          [-2.1061e-02,  1.2516e-03,  1.6056e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.6502e-03, -3.4292e-02, -3.5787e-02],\n",
            "          [-1.5410e-02, -2.8428e-02, -2.6417e-02],\n",
            "          [ 8.5857e-03, -1.4984e-02, -1.1492e-02]],\n",
            "\n",
            "         [[ 7.9233e-03, -9.0605e-03,  2.2980e-02],\n",
            "          [-2.4555e-02,  2.9554e-02, -1.5374e-02],\n",
            "          [-4.0894e-03, -2.3321e-02, -2.0423e-02]],\n",
            "\n",
            "         [[-8.8321e-03, -1.4936e-02,  1.2841e-02],\n",
            "          [-3.0133e-03, -1.0199e-02, -3.4364e-02],\n",
            "          [-5.4099e-03,  1.5228e-02, -2.2896e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.6723e-02, -5.9633e-03, -1.0083e-02],\n",
            "          [-3.0935e-02, -5.8135e-03,  4.4064e-03],\n",
            "          [ 6.2743e-03,  1.8525e-02,  4.0319e-03]],\n",
            "\n",
            "         [[-2.2604e-02, -3.3843e-02, -2.5342e-02],\n",
            "          [-1.2649e-03, -8.6352e-03,  9.9989e-03],\n",
            "          [-4.3079e-03, -2.5510e-02, -4.9242e-02]],\n",
            "\n",
            "         [[ 1.7740e-03, -2.8115e-02, -4.4458e-03],\n",
            "          [-2.7375e-02, -1.2030e-02,  2.2782e-03],\n",
            "          [ 2.2603e-02, -5.4265e-03,  3.2432e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 2.4608e-02, -2.5437e-02,  2.1556e-02],\n",
            "          [ 3.0807e-02,  1.2477e-02, -2.4074e-02],\n",
            "          [ 2.7114e-02,  3.1476e-02,  9.6571e-03]],\n",
            "\n",
            "         [[-2.6372e-02, -1.7245e-02, -7.5359e-03],\n",
            "          [ 1.2581e-02,  1.0976e-02, -1.6428e-02],\n",
            "          [ 2.6984e-02, -7.8190e-03, -1.3050e-02]],\n",
            "\n",
            "         [[ 3.2362e-02,  2.8367e-02, -8.1259e-03],\n",
            "          [-1.0390e-02,  9.1102e-04, -2.2192e-02],\n",
            "          [-2.1308e-02,  5.9297e-03,  5.1805e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.1121e-02,  2.2675e-03, -8.6622e-03],\n",
            "          [ 2.0739e-02,  1.1482e-02,  1.5224e-02],\n",
            "          [ 1.7987e-02, -9.3996e-03,  1.7309e-02]],\n",
            "\n",
            "         [[ 2.8557e-02, -1.4422e-02,  2.2673e-02],\n",
            "          [ 5.4916e-03, -7.3762e-03, -1.1017e-02],\n",
            "          [ 8.5843e-03,  1.0738e-02, -1.8040e-02]],\n",
            "\n",
            "         [[ 6.6615e-04, -3.4710e-02, -3.1210e-02],\n",
            "          [-2.9635e-02, -1.3346e-02, -2.0811e-02],\n",
            "          [-6.5415e-03,  1.5305e-02, -3.0771e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.0585e-04, -9.2240e-03,  5.1441e-03],\n",
            "          [ 1.2043e-03, -3.2454e-02, -2.0950e-02],\n",
            "          [-9.2835e-03,  7.7588e-04, -2.8092e-02]],\n",
            "\n",
            "         [[-2.3910e-02, -2.2134e-04, -2.5663e-02],\n",
            "          [-2.2155e-02,  1.3960e-02, -7.5707e-03],\n",
            "          [-2.8894e-02,  3.6426e-03, -3.8127e-02]],\n",
            "\n",
            "         [[ 1.1127e-02, -3.9703e-02,  3.3917e-03],\n",
            "          [ 3.2932e-02,  2.0553e-02,  2.5474e-02],\n",
            "          [-2.3298e-02,  2.4178e-02, -1.3042e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.5560e-02,  6.3740e-04, -2.4987e-02],\n",
            "          [ 3.2255e-02,  3.2896e-02,  2.1262e-02],\n",
            "          [-1.4453e-02, -8.9813e-03,  6.8786e-03]],\n",
            "\n",
            "         [[ 2.6723e-03, -2.1797e-02,  9.3972e-03],\n",
            "          [ 4.2911e-02,  1.4935e-03, -3.2764e-02],\n",
            "          [-3.9824e-03, -6.5525e-03, -3.6285e-02]],\n",
            "\n",
            "         [[ 1.6835e-02,  1.9841e-02, -1.4901e-02],\n",
            "          [-2.5695e-02, -3.5772e-03, -7.9566e-03],\n",
            "          [ 1.5349e-02, -2.8715e-02, -2.7507e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 2.2520e-02, -2.9694e-02, -9.6571e-03],\n",
            "          [-4.4720e-04, -3.1266e-02,  2.4385e-02],\n",
            "          [-2.6584e-02, -2.0007e-02, -3.9698e-03]],\n",
            "\n",
            "         [[-1.5541e-03, -1.3736e-02,  3.1293e-03],\n",
            "          [-1.6671e-02,  1.2878e-02,  1.5099e-04],\n",
            "          [-7.0160e-03, -3.1486e-02,  7.5685e-03]],\n",
            "\n",
            "         [[ 2.6081e-02, -1.6629e-03, -3.6289e-02],\n",
            "          [ 2.6837e-02,  1.9523e-03,  2.9169e-02],\n",
            "          [-2.1998e-02,  3.4883e-02,  4.4808e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.2058e-02,  1.0674e-02, -2.5914e-02],\n",
            "          [-4.9889e-02, -8.5812e-03,  1.9211e-02],\n",
            "          [-4.4872e-02, -4.2417e-02,  7.6130e-03]],\n",
            "\n",
            "         [[ 3.1836e-02,  1.3182e-02, -4.0866e-03],\n",
            "          [ 2.5191e-02,  4.8571e-03, -7.8343e-03],\n",
            "          [-8.7786e-03,  2.2606e-02,  2.3510e-02]],\n",
            "\n",
            "         [[-1.3841e-02, -2.6470e-02,  1.2238e-02],\n",
            "          [-1.9655e-02,  9.5686e-03, -1.0282e-02],\n",
            "          [ 2.0481e-02,  8.4534e-03,  2.7784e-03]]]], device='cuda:0')\n",
            "N1_C4.bias Parameter containing:\n",
            "tensor([ 6.5764e-02, -3.5683e-02, -1.1497e-02, -1.5651e-02, -5.6180e-02,\n",
            "         7.3352e-02,  5.3025e-04, -6.9200e-03,  7.9908e-02, -9.2850e-03,\n",
            "         3.4429e-02, -2.4924e-02,  9.6388e-02, -1.3857e-02,  1.2093e-01,\n",
            "        -7.9739e-03, -3.1918e-02,  1.4954e-02,  6.7956e-02, -4.6734e-02,\n",
            "        -5.6940e-02, -6.8940e-03,  2.9657e-02, -3.7898e-02, -1.1406e-02,\n",
            "        -4.1812e-02, -7.9214e-02,  7.5223e-02, -4.9278e-03, -9.8886e-03,\n",
            "         4.0491e-02, -7.2542e-02, -9.5474e-03,  2.4482e-02,  7.9476e-02,\n",
            "         1.2145e-01, -1.4514e-02,  4.4787e-02,  5.1283e-02, -7.3459e-03,\n",
            "        -1.6087e-02,  2.9156e-02, -1.8771e-02, -3.1097e-02, -7.5149e-02,\n",
            "        -1.7682e-04, -4.3696e-02,  1.1455e-02, -2.8791e-02, -2.6728e-02,\n",
            "        -5.7734e-05, -2.0002e-02, -8.0030e-02, -8.9515e-03,  9.0767e-02,\n",
            "        -2.4848e-03, -3.0636e-02, -5.5087e-02, -2.4100e-02,  3.2193e-02,\n",
            "         1.0211e-02,  1.5776e-02, -1.7421e-02,  1.9532e-02, -2.1658e-02,\n",
            "         7.9575e-02,  9.7399e-02,  1.0131e-01, -5.0308e-02, -3.6936e-02,\n",
            "         1.7715e-02,  7.8757e-02, -4.1378e-02,  8.6555e-02,  2.0258e-02,\n",
            "         1.7104e-01,  2.4518e-02,  3.7341e-02, -1.9803e-02,  3.6117e-02,\n",
            "         4.2466e-02, -2.1380e-02, -9.4927e-02,  7.5221e-02, -5.5481e-04,\n",
            "         1.1948e-01, -4.9553e-03,  5.0898e-02,  5.6577e-02,  1.3049e-01,\n",
            "        -2.5716e-02, -3.3741e-02, -4.6883e-02, -3.4761e-02, -3.4833e-02,\n",
            "         4.5446e-02, -2.1609e-03, -2.8693e-03, -2.1178e-03, -1.1170e-02,\n",
            "         1.9495e-02, -1.7681e-02,  1.0063e-01, -7.9476e-03, -1.9035e-02,\n",
            "        -6.2964e-02,  2.4514e-02, -7.6044e-03, -5.7480e-02, -8.7730e-03,\n",
            "         8.9832e-03,  2.0721e-02, -1.0635e-01,  8.5132e-02,  8.3502e-03,\n",
            "        -2.0282e-02,  8.7324e-02,  3.3919e-02,  7.2621e-02, -3.4860e-02,\n",
            "        -4.0238e-02, -2.3661e-02, -9.4937e-03, -1.7020e-02, -5.5877e-03,\n",
            "         1.7843e-02, -7.4935e-02,  2.4408e-02], device='cuda:0')\n",
            "N1_D1.weight Parameter containing:\n",
            "tensor([[ 2.2028e-03,  1.4709e-03,  3.0148e-03,  ..., -1.0119e-03,\n",
            "         -2.1741e-03, -1.5711e-03],\n",
            "        [ 2.1094e-03,  6.5458e-03,  2.9443e-03,  ..., -1.7045e-04,\n",
            "         -4.2024e-03,  1.2213e-03],\n",
            "        [-6.2115e-04,  2.1193e-04, -1.3517e-03,  ..., -8.1871e-04,\n",
            "         -1.2601e-03,  1.5834e-04],\n",
            "        ...,\n",
            "        [ 1.9803e-04, -2.1803e-03, -1.5149e-03,  ..., -6.6710e-03,\n",
            "         -8.1932e-04, -1.3591e-04],\n",
            "        [ 1.5994e-04, -1.4079e-03, -1.5350e-03,  ..., -2.0174e-04,\n",
            "         -3.0950e-03, -4.9047e-03],\n",
            "        [ 1.1106e-03,  3.6411e-03,  4.9585e-05,  ..., -1.8086e-03,\n",
            "          5.2633e-04,  2.8258e-03]], device='cuda:0')\n",
            "N1_D1.bias Parameter containing:\n",
            "tensor([ 1.9766e-02,  2.7292e-02, -7.4502e-03,  3.7277e-03, -2.4223e-03,\n",
            "         2.1825e-02,  5.5891e-03, -3.7776e-03,  1.5831e-03,  4.8088e-03,\n",
            "        -5.6634e-03, -1.6082e-03,  1.3494e-03, -1.4438e-03, -9.9392e-03,\n",
            "        -8.6492e-03,  4.3441e-03,  2.6132e-03, -2.0935e-03,  1.2728e-02,\n",
            "         7.0502e-03, -1.2317e-03, -1.0740e-02, -3.1601e-04, -2.3839e-03,\n",
            "         8.1220e-04,  7.1384e-03, -1.3064e-03,  2.3592e-03, -1.0845e-02,\n",
            "        -6.9096e-03,  2.5008e-03, -8.0120e-03, -7.5177e-03, -2.9161e-04,\n",
            "         2.7964e-04, -2.8475e-03,  7.9571e-03,  9.2535e-03, -1.4872e-03,\n",
            "        -4.1106e-03,  1.5125e-02, -3.9263e-03,  2.7291e-02, -3.4479e-03,\n",
            "         4.0602e-03, -3.3972e-04, -1.1164e-04, -1.0928e-02, -1.0719e-02,\n",
            "         1.5186e-02, -2.8114e-03,  1.8364e-02,  3.5538e-04,  5.6847e-03,\n",
            "         4.5275e-03,  4.2965e-03,  2.1365e-03, -2.3030e-03,  1.3460e-02,\n",
            "         4.2752e-04,  1.7322e-02,  1.7794e-03,  1.4657e-03,  7.8786e-03,\n",
            "         5.6744e-03,  5.3714e-03, -3.9468e-03, -6.6443e-03,  2.4787e-03,\n",
            "         3.7910e-03,  1.5393e-02,  2.5718e-03,  1.1990e-02,  5.0136e-04,\n",
            "         7.1810e-03,  6.9034e-03,  1.3136e-02, -7.4546e-03, -1.1141e-02,\n",
            "        -1.0818e-02,  1.3442e-02,  4.3910e-03, -7.5722e-03,  1.2223e-04,\n",
            "        -8.3480e-03, -2.0080e-03,  2.4352e-03, -7.1845e-03, -4.1911e-04,\n",
            "        -1.3606e-03,  1.3172e-02,  9.9991e-03,  1.9974e-02,  8.9681e-03,\n",
            "         2.8159e-04, -5.8783e-03,  5.8321e-05,  6.4500e-04,  1.5398e-03,\n",
            "         9.7632e-03,  1.6674e-02, -3.5016e-03, -1.0507e-03,  9.9478e-04,\n",
            "         2.5366e-03,  1.8881e-02,  1.7663e-04,  3.8378e-03,  3.8474e-03,\n",
            "        -1.6868e-03,  1.5137e-02,  1.9823e-02, -1.3378e-03, -1.2007e-04,\n",
            "        -1.5705e-03,  7.3171e-03, -4.2489e-03,  4.3597e-03,  1.3061e-04,\n",
            "        -8.3107e-03, -6.2861e-04, -1.4505e-03,  1.2661e-02,  1.1852e-02,\n",
            "         4.5903e-03,  2.9290e-03, -6.5104e-03, -6.4470e-03, -1.9350e-02,\n",
            "        -7.8451e-03, -4.4800e-03,  4.2584e-03, -3.6150e-03,  1.3845e-03,\n",
            "         8.8203e-03, -1.0199e-02,  1.5953e-03,  1.4247e-03,  5.4326e-03,\n",
            "        -5.6327e-04, -1.4142e-02, -8.3301e-03,  4.2897e-04,  3.1655e-03,\n",
            "         3.1757e-03, -6.7762e-03,  1.6154e-03,  2.3675e-03,  1.2385e-02,\n",
            "         6.8188e-03,  9.9395e-03,  5.7764e-03,  8.1153e-03, -5.7309e-03,\n",
            "         1.4928e-02,  1.9655e-03, -1.6554e-06,  1.4028e-02, -1.1400e-03,\n",
            "         1.7247e-02, -3.0565e-03, -6.6128e-03, -5.0568e-03, -5.1290e-03,\n",
            "         1.2408e-03, -7.5152e-03,  1.3359e-02, -1.4800e-03, -3.8817e-03,\n",
            "        -6.8097e-03,  7.7920e-03, -3.9119e-03,  6.4606e-03,  6.1568e-04,\n",
            "        -2.5631e-03, -2.4155e-03, -2.9801e-04, -6.1919e-05,  1.1271e-02,\n",
            "         1.0207e-02,  1.6587e-02, -2.2878e-03, -3.2110e-03, -1.1574e-03,\n",
            "         3.3590e-03,  8.3084e-03, -5.9526e-03,  8.5053e-03,  1.3647e-03,\n",
            "         5.3848e-03, -2.9929e-04,  1.3828e-02,  2.3769e-03, -5.1293e-03,\n",
            "         5.3650e-03, -8.2381e-04, -6.1756e-03,  2.8239e-03,  4.1366e-03,\n",
            "         8.3145e-03, -4.4412e-03,  1.0083e-02,  8.2946e-03, -1.8662e-04,\n",
            "        -1.4656e-02,  5.4018e-03, -5.2671e-03, -1.2513e-03, -5.4324e-03,\n",
            "         1.6631e-03, -6.5916e-03, -1.2483e-03, -1.6975e-03, -8.5231e-03,\n",
            "        -7.6497e-03, -2.6215e-02,  2.3991e-02, -6.1909e-03,  9.9149e-03,\n",
            "        -2.6174e-03, -6.0589e-03,  8.4265e-03,  2.5438e-03,  6.5575e-04,\n",
            "         1.4049e-02,  1.4439e-02,  3.3497e-03,  1.9149e-03,  2.9212e-02,\n",
            "        -3.9427e-03,  1.7168e-02,  1.7253e-02, -9.6616e-03, -6.0491e-03,\n",
            "        -3.9117e-03, -3.0042e-03, -7.0862e-03,  2.0765e-03, -2.6589e-03,\n",
            "        -1.3381e-03,  5.8153e-03, -4.7640e-03,  2.2388e-03,  1.2325e-03,\n",
            "         6.2998e-03,  3.3719e-03, -1.0703e-02,  1.1885e-03, -8.8944e-03,\n",
            "        -1.1395e-03, -1.0231e-02,  1.7510e-03, -4.2428e-03, -4.9664e-03,\n",
            "         6.3337e-03], device='cuda:0')\n",
            "N1_D2.weight Parameter containing:\n",
            "tensor([[ 0.0119,  0.0409, -0.0640,  ...,  0.0521,  0.0068,  0.0249],\n",
            "        [-0.0486,  0.0264,  0.0021,  ..., -0.0356,  0.0059, -0.0352],\n",
            "        [ 0.0629,  0.1130, -0.0943,  ..., -0.0686,  0.0840,  0.0553],\n",
            "        ...,\n",
            "        [-0.0308,  0.0149,  0.0382,  ..., -0.0408,  0.0249,  0.0752],\n",
            "        [ 0.0359, -0.0608, -0.0079,  ..., -0.0504, -0.0439, -0.0080],\n",
            "        [-0.0133,  0.0182,  0.0377,  ..., -0.0640,  0.0320, -0.0279]],\n",
            "       device='cuda:0')\n",
            "N1_D2.bias Parameter containing:\n",
            "tensor([ 0.0533, -0.0270,  0.0779,  0.0269, -0.0186, -0.0030, -0.0234, -0.0164,\n",
            "         0.0537,  0.0234, -0.0159,  0.0382, -0.0434, -0.0020,  0.0358,  0.0175,\n",
            "        -0.0298, -0.0276,  0.0190,  0.0311,  0.0194, -0.0275,  0.0462,  0.0272,\n",
            "         0.0561,  0.0498,  0.0107,  0.0690, -0.0132, -0.0215,  0.0630,  0.0413,\n",
            "         0.0129,  0.0123,  0.0413,  0.0534,  0.0281, -0.0561, -0.0096,  0.0374,\n",
            "         0.0403, -0.0349,  0.0382, -0.0049, -0.0220, -0.0131,  0.0601, -0.0142,\n",
            "        -0.0069,  0.0548, -0.0143, -0.0078, -0.0417,  0.0114,  0.0302,  0.0610,\n",
            "         0.0536, -0.0594, -0.0105, -0.0254, -0.0569,  0.0177,  0.0615,  0.0598,\n",
            "        -0.0557,  0.0245, -0.0076, -0.0571, -0.0269,  0.0416, -0.0015,  0.0410,\n",
            "         0.0073, -0.0539,  0.0057, -0.0541, -0.0250, -0.0086,  0.0328, -0.0610,\n",
            "        -0.0256,  0.0483,  0.0278,  0.0826,  0.0313,  0.0639, -0.0185,  0.0675,\n",
            "         0.0279,  0.0134, -0.0860, -0.0258,  0.0404, -0.0395, -0.0070,  0.0498,\n",
            "         0.0329,  0.0440, -0.0190,  0.0163, -0.0384, -0.0075, -0.0228,  0.0383,\n",
            "         0.0255,  0.0636,  0.0455, -0.0395,  0.0396, -0.0014,  0.0259, -0.0588,\n",
            "        -0.0469, -0.0516, -0.0327,  0.0225,  0.0171, -0.0204, -0.0174, -0.0217,\n",
            "        -0.0561,  0.0068,  0.0081, -0.0215, -0.0289, -0.0168, -0.0134,  0.0695,\n",
            "        -0.0587, -0.0347,  0.0214, -0.0229, -0.0350, -0.0196,  0.0384,  0.0159,\n",
            "         0.0896,  0.0340,  0.0433,  0.0672, -0.0228, -0.0513, -0.0585, -0.0152,\n",
            "         0.0519, -0.0356,  0.0041,  0.0265,  0.0391,  0.0050, -0.0439, -0.0016,\n",
            "         0.0705, -0.0378, -0.0599, -0.0445, -0.0129,  0.0321, -0.0106, -0.0237,\n",
            "        -0.0548, -0.0553,  0.0573,  0.0616, -0.0604,  0.0159,  0.0324, -0.0771,\n",
            "         0.0341,  0.0534, -0.0436,  0.0520, -0.0471,  0.0317,  0.0218, -0.0734,\n",
            "        -0.0369, -0.0347,  0.0260, -0.0579, -0.0380, -0.0288, -0.0508, -0.0207,\n",
            "        -0.0155, -0.0401, -0.0253,  0.0252,  0.0355,  0.0444, -0.0107, -0.0104,\n",
            "         0.0328,  0.0079, -0.0058,  0.0450,  0.0319,  0.0136,  0.0312,  0.0356,\n",
            "        -0.0509,  0.0008,  0.0608,  0.0297,  0.0437,  0.0339,  0.0324, -0.0275,\n",
            "         0.0037, -0.0537, -0.0012, -0.0058,  0.0533, -0.0071,  0.0574, -0.0547,\n",
            "         0.0171,  0.0012,  0.0497,  0.0653,  0.0071,  0.0481,  0.0421, -0.0493,\n",
            "         0.0480, -0.0374,  0.0970, -0.0154, -0.0199,  0.0227, -0.0467,  0.0459,\n",
            "        -0.0487,  0.0758, -0.0020, -0.0508,  0.0674,  0.0200,  0.0476, -0.0609,\n",
            "        -0.0085,  0.0317, -0.0114,  0.0925,  0.0316,  0.0423,  0.0360, -0.0472,\n",
            "         0.0627,  0.0008,  0.0130,  0.0131,  0.0429,  0.0463, -0.0061,  0.0265],\n",
            "       device='cuda:0')\n",
            "N2_C1.weight Parameter containing:\n",
            "tensor([[[[-0.1091, -0.1822, -0.0864],\n",
            "          [-0.1389,  0.0264, -0.1698],\n",
            "          [ 0.1167, -0.1168,  0.0707]],\n",
            "\n",
            "         [[-0.0495, -0.0544, -0.1184],\n",
            "          [-0.1894, -0.1036,  0.0635],\n",
            "          [-0.0462,  0.1355, -0.0145]],\n",
            "\n",
            "         [[-0.0691,  0.0954,  0.0194],\n",
            "          [ 0.1790,  0.1692, -0.0369],\n",
            "          [-0.1543,  0.1810,  0.1076]]],\n",
            "\n",
            "\n",
            "        [[[-0.0163, -0.1453, -0.0226],\n",
            "          [-0.0570, -0.0908, -0.1120],\n",
            "          [-0.1726, -0.1307, -0.1760]],\n",
            "\n",
            "         [[ 0.0434, -0.1738,  0.1133],\n",
            "          [ 0.0082, -0.0568, -0.0532],\n",
            "          [-0.0974, -0.1202, -0.0040]],\n",
            "\n",
            "         [[-0.1907,  0.1021,  0.0703],\n",
            "          [ 0.1325,  0.0201, -0.1570],\n",
            "          [-0.1556,  0.0547,  0.0299]]],\n",
            "\n",
            "\n",
            "        [[[-0.1645, -0.0258, -0.1875],\n",
            "          [ 0.1622,  0.0840,  0.0409],\n",
            "          [-0.0733, -0.1095,  0.0757]],\n",
            "\n",
            "         [[-0.0337, -0.1863, -0.1457],\n",
            "          [ 0.0094, -0.1218,  0.0928],\n",
            "          [ 0.0927, -0.0519,  0.0050]],\n",
            "\n",
            "         [[-0.0856,  0.0294,  0.0107],\n",
            "          [ 0.1608,  0.1713, -0.0759],\n",
            "          [ 0.0830, -0.0452,  0.0107]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.0232, -0.0346, -0.0174],\n",
            "          [ 0.1397, -0.0713, -0.1378],\n",
            "          [ 0.1140, -0.1460,  0.1649]],\n",
            "\n",
            "         [[-0.0766, -0.1019,  0.0894],\n",
            "          [ 0.0175, -0.1032,  0.1402],\n",
            "          [-0.1691, -0.1771,  0.0302]],\n",
            "\n",
            "         [[ 0.1854,  0.0945,  0.1628],\n",
            "          [ 0.0374,  0.1556,  0.0924],\n",
            "          [-0.1031,  0.0945,  0.1362]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0076, -0.0465,  0.0731],\n",
            "          [ 0.0412, -0.1200, -0.0874],\n",
            "          [-0.1779, -0.0035,  0.0637]],\n",
            "\n",
            "         [[ 0.0963, -0.1019,  0.1915],\n",
            "          [-0.1731, -0.0082, -0.1921],\n",
            "          [-0.1127, -0.1420,  0.0815]],\n",
            "\n",
            "         [[ 0.0550, -0.1656,  0.0243],\n",
            "          [-0.0521,  0.1159, -0.1012],\n",
            "          [ 0.0594,  0.1473,  0.0182]]],\n",
            "\n",
            "\n",
            "        [[[-0.1446, -0.0966, -0.1426],\n",
            "          [ 0.0408, -0.1496,  0.0052],\n",
            "          [-0.1305,  0.0287, -0.1590]],\n",
            "\n",
            "         [[ 0.1902, -0.1201,  0.0920],\n",
            "          [-0.0870,  0.0417,  0.1199],\n",
            "          [-0.0342,  0.1537,  0.1849]],\n",
            "\n",
            "         [[ 0.1375,  0.1586,  0.0535],\n",
            "          [-0.0752, -0.1179,  0.0759],\n",
            "          [-0.0383, -0.1646, -0.0952]]]], device='cuda:0', requires_grad=True)\n",
            "N2_C1.bias Parameter containing:\n",
            "tensor([-0.1091, -0.1486,  0.0296, -0.1285,  0.0531,  0.1725, -0.0363, -0.0661,\n",
            "        -0.0810, -0.0245, -0.1433,  0.1808, -0.1187, -0.0763,  0.0186, -0.1530,\n",
            "        -0.0075,  0.0372, -0.0522, -0.1108,  0.0301,  0.0706, -0.1649, -0.1760,\n",
            "         0.0708,  0.1504, -0.0437, -0.1228,  0.1564, -0.1501, -0.0200, -0.0427,\n",
            "        -0.1629, -0.0115, -0.0527, -0.0703, -0.1569,  0.0700,  0.0089,  0.1578,\n",
            "         0.0639,  0.0567,  0.0088, -0.1343,  0.1726,  0.0982, -0.1166,  0.1373,\n",
            "        -0.1649,  0.0863, -0.1000, -0.1533,  0.1378, -0.1677,  0.1674,  0.1853,\n",
            "        -0.0996,  0.0412, -0.1553, -0.0923,  0.0045,  0.1227, -0.1552, -0.0213],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "N2_C2.weight Parameter containing:\n",
            "tensor([[[[ 0.0161, -0.0078, -0.0031],\n",
            "          [ 0.0005, -0.0349,  0.0294],\n",
            "          [-0.0097,  0.0355,  0.0262]],\n",
            "\n",
            "         [[-0.0104,  0.0364, -0.0240],\n",
            "          [ 0.0401,  0.0209, -0.0079],\n",
            "          [ 0.0313, -0.0357, -0.0413]],\n",
            "\n",
            "         [[-0.0296,  0.0304, -0.0073],\n",
            "          [ 0.0002, -0.0329,  0.0245],\n",
            "          [ 0.0061, -0.0151,  0.0280]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0201,  0.0375, -0.0251],\n",
            "          [-0.0007,  0.0283,  0.0383],\n",
            "          [-0.0328, -0.0340, -0.0179]],\n",
            "\n",
            "         [[ 0.0315,  0.0158, -0.0206],\n",
            "          [ 0.0178,  0.0172,  0.0286],\n",
            "          [-0.0231, -0.0073,  0.0394]],\n",
            "\n",
            "         [[ 0.0310, -0.0164,  0.0403],\n",
            "          [ 0.0133, -0.0226, -0.0310],\n",
            "          [ 0.0174,  0.0057, -0.0090]]],\n",
            "\n",
            "\n",
            "        [[[-0.0067,  0.0214, -0.0408],\n",
            "          [ 0.0128,  0.0307,  0.0240],\n",
            "          [ 0.0159, -0.0185, -0.0279]],\n",
            "\n",
            "         [[ 0.0409, -0.0142, -0.0369],\n",
            "          [ 0.0231,  0.0405, -0.0211],\n",
            "          [-0.0184, -0.0054,  0.0408]],\n",
            "\n",
            "         [[-0.0268,  0.0079, -0.0314],\n",
            "          [ 0.0079,  0.0399, -0.0046],\n",
            "          [-0.0290, -0.0117, -0.0393]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0245, -0.0024,  0.0249],\n",
            "          [-0.0193,  0.0143,  0.0200],\n",
            "          [-0.0169,  0.0197, -0.0092]],\n",
            "\n",
            "         [[-0.0296, -0.0285,  0.0366],\n",
            "          [-0.0194,  0.0342, -0.0369],\n",
            "          [ 0.0253, -0.0039,  0.0269]],\n",
            "\n",
            "         [[ 0.0336, -0.0263,  0.0050],\n",
            "          [-0.0151, -0.0219, -0.0218],\n",
            "          [ 0.0134,  0.0388, -0.0202]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0311, -0.0237,  0.0272],\n",
            "          [ 0.0232,  0.0225,  0.0275],\n",
            "          [ 0.0205, -0.0129, -0.0215]],\n",
            "\n",
            "         [[-0.0097,  0.0075, -0.0208],\n",
            "          [ 0.0068,  0.0187, -0.0245],\n",
            "          [-0.0138,  0.0090,  0.0136]],\n",
            "\n",
            "         [[-0.0047,  0.0259,  0.0171],\n",
            "          [-0.0346,  0.0217,  0.0008],\n",
            "          [ 0.0326,  0.0322,  0.0262]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0285, -0.0375, -0.0071],\n",
            "          [-0.0010,  0.0187, -0.0157],\n",
            "          [ 0.0080, -0.0367, -0.0135]],\n",
            "\n",
            "         [[ 0.0034,  0.0036, -0.0248],\n",
            "          [ 0.0410,  0.0092,  0.0233],\n",
            "          [ 0.0196,  0.0154,  0.0160]],\n",
            "\n",
            "         [[ 0.0194, -0.0185, -0.0091],\n",
            "          [ 0.0185, -0.0110,  0.0183],\n",
            "          [ 0.0401,  0.0211,  0.0268]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0044,  0.0172, -0.0308],\n",
            "          [-0.0055,  0.0113,  0.0042],\n",
            "          [ 0.0272, -0.0389, -0.0233]],\n",
            "\n",
            "         [[ 0.0247,  0.0039,  0.0152],\n",
            "          [-0.0050,  0.0296, -0.0295],\n",
            "          [ 0.0144, -0.0105, -0.0180]],\n",
            "\n",
            "         [[ 0.0282, -0.0260, -0.0260],\n",
            "          [ 0.0132, -0.0128, -0.0340],\n",
            "          [ 0.0211, -0.0271, -0.0120]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0132, -0.0200,  0.0065],\n",
            "          [ 0.0154,  0.0052, -0.0406],\n",
            "          [-0.0165, -0.0153, -0.0157]],\n",
            "\n",
            "         [[-0.0159, -0.0132,  0.0297],\n",
            "          [ 0.0108, -0.0251,  0.0402],\n",
            "          [-0.0189, -0.0324,  0.0031]],\n",
            "\n",
            "         [[-0.0209,  0.0343, -0.0396],\n",
            "          [-0.0197,  0.0366, -0.0282],\n",
            "          [ 0.0383,  0.0117, -0.0117]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0408,  0.0128,  0.0071],\n",
            "          [ 0.0252, -0.0035,  0.0094],\n",
            "          [-0.0092,  0.0252,  0.0385]],\n",
            "\n",
            "         [[ 0.0117, -0.0078,  0.0118],\n",
            "          [-0.0051,  0.0397,  0.0342],\n",
            "          [ 0.0360, -0.0005,  0.0066]],\n",
            "\n",
            "         [[ 0.0387,  0.0328,  0.0037],\n",
            "          [ 0.0296, -0.0317,  0.0183],\n",
            "          [ 0.0075,  0.0030,  0.0355]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0221, -0.0093,  0.0088],\n",
            "          [-0.0206, -0.0312, -0.0265],\n",
            "          [ 0.0048,  0.0298, -0.0186]],\n",
            "\n",
            "         [[-0.0257, -0.0240, -0.0091],\n",
            "          [ 0.0264, -0.0147,  0.0344],\n",
            "          [ 0.0168, -0.0081,  0.0309]],\n",
            "\n",
            "         [[ 0.0043,  0.0416,  0.0110],\n",
            "          [ 0.0100, -0.0234,  0.0233],\n",
            "          [ 0.0054,  0.0280, -0.0416]]],\n",
            "\n",
            "\n",
            "        [[[-0.0319, -0.0014,  0.0089],\n",
            "          [ 0.0176,  0.0021,  0.0076],\n",
            "          [-0.0331,  0.0175,  0.0141]],\n",
            "\n",
            "         [[ 0.0105,  0.0002,  0.0091],\n",
            "          [ 0.0346, -0.0123, -0.0143],\n",
            "          [ 0.0391,  0.0090, -0.0126]],\n",
            "\n",
            "         [[ 0.0068, -0.0264, -0.0084],\n",
            "          [-0.0140,  0.0227,  0.0346],\n",
            "          [-0.0120, -0.0016, -0.0253]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0107, -0.0306,  0.0244],\n",
            "          [ 0.0235, -0.0063,  0.0192],\n",
            "          [-0.0187,  0.0309,  0.0137]],\n",
            "\n",
            "         [[-0.0322, -0.0096,  0.0122],\n",
            "          [-0.0318,  0.0273, -0.0093],\n",
            "          [-0.0165, -0.0236, -0.0087]],\n",
            "\n",
            "         [[-0.0184,  0.0312, -0.0292],\n",
            "          [ 0.0326,  0.0202, -0.0046],\n",
            "          [-0.0327,  0.0032,  0.0376]]]], device='cuda:0', requires_grad=True)\n",
            "N2_C2.bias Parameter containing:\n",
            "tensor([ 0.0191, -0.0326,  0.0163,  0.0146, -0.0396, -0.0368, -0.0309,  0.0106,\n",
            "         0.0288, -0.0127,  0.0282, -0.0235, -0.0128, -0.0177, -0.0355, -0.0264,\n",
            "        -0.0260, -0.0072,  0.0180, -0.0315, -0.0248, -0.0391,  0.0239, -0.0148,\n",
            "         0.0295, -0.0017,  0.0102,  0.0235, -0.0174, -0.0217, -0.0119,  0.0048,\n",
            "         0.0192,  0.0119,  0.0402,  0.0183,  0.0416, -0.0148,  0.0180,  0.0231,\n",
            "        -0.0333,  0.0374,  0.0368,  0.0183, -0.0281, -0.0383, -0.0352, -0.0294,\n",
            "         0.0210, -0.0123,  0.0077, -0.0028, -0.0379, -0.0154,  0.0119, -0.0230,\n",
            "         0.0371, -0.0065,  0.0324, -0.0092, -0.0351,  0.0190,  0.0395,  0.0054],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "N2_C3.weight Parameter containing:\n",
            "tensor([[[[-0.0302, -0.0063,  0.0070],\n",
            "          [ 0.0002,  0.0131, -0.0375],\n",
            "          [-0.0358,  0.0262,  0.0073]],\n",
            "\n",
            "         [[ 0.0121,  0.0349, -0.0162],\n",
            "          [ 0.0405,  0.0153, -0.0077],\n",
            "          [ 0.0220, -0.0056, -0.0355]],\n",
            "\n",
            "         [[-0.0326,  0.0340, -0.0070],\n",
            "          [-0.0097, -0.0237, -0.0028],\n",
            "          [ 0.0051,  0.0223, -0.0210]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0158,  0.0064, -0.0181],\n",
            "          [ 0.0298, -0.0340,  0.0254],\n",
            "          [-0.0357, -0.0259,  0.0200]],\n",
            "\n",
            "         [[-0.0403,  0.0141,  0.0291],\n",
            "          [ 0.0322,  0.0321, -0.0027],\n",
            "          [-0.0137, -0.0087, -0.0321]],\n",
            "\n",
            "         [[-0.0106, -0.0086,  0.0037],\n",
            "          [-0.0200,  0.0380,  0.0260],\n",
            "          [ 0.0194, -0.0058,  0.0251]]],\n",
            "\n",
            "\n",
            "        [[[-0.0148, -0.0150, -0.0293],\n",
            "          [-0.0011,  0.0067,  0.0234],\n",
            "          [ 0.0283, -0.0045, -0.0109]],\n",
            "\n",
            "         [[-0.0286,  0.0286, -0.0291],\n",
            "          [ 0.0348, -0.0042,  0.0306],\n",
            "          [ 0.0269, -0.0218, -0.0141]],\n",
            "\n",
            "         [[ 0.0121, -0.0359, -0.0361],\n",
            "          [ 0.0277,  0.0001,  0.0322],\n",
            "          [-0.0049,  0.0410, -0.0138]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0158,  0.0405,  0.0012],\n",
            "          [-0.0114, -0.0331, -0.0355],\n",
            "          [-0.0048,  0.0063, -0.0075]],\n",
            "\n",
            "         [[-0.0283, -0.0113, -0.0091],\n",
            "          [ 0.0407,  0.0344,  0.0264],\n",
            "          [-0.0210,  0.0095,  0.0290]],\n",
            "\n",
            "         [[-0.0347, -0.0112,  0.0314],\n",
            "          [-0.0071, -0.0030,  0.0083],\n",
            "          [ 0.0156,  0.0027,  0.0127]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0298,  0.0081, -0.0165],\n",
            "          [-0.0048, -0.0189, -0.0076],\n",
            "          [-0.0331,  0.0323,  0.0091]],\n",
            "\n",
            "         [[-0.0373, -0.0404, -0.0095],\n",
            "          [-0.0211, -0.0368,  0.0206],\n",
            "          [-0.0291,  0.0387,  0.0400]],\n",
            "\n",
            "         [[-0.0087,  0.0069, -0.0074],\n",
            "          [-0.0018, -0.0300,  0.0026],\n",
            "          [ 0.0398, -0.0290, -0.0257]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0343,  0.0060,  0.0351],\n",
            "          [-0.0345, -0.0224,  0.0088],\n",
            "          [ 0.0227, -0.0015, -0.0377]],\n",
            "\n",
            "         [[ 0.0138,  0.0173, -0.0188],\n",
            "          [ 0.0162,  0.0113,  0.0095],\n",
            "          [-0.0021,  0.0196, -0.0284]],\n",
            "\n",
            "         [[-0.0200, -0.0275, -0.0148],\n",
            "          [-0.0193, -0.0070, -0.0202],\n",
            "          [ 0.0035, -0.0300,  0.0252]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0392, -0.0057, -0.0410],\n",
            "          [ 0.0030,  0.0088,  0.0021],\n",
            "          [-0.0307, -0.0331, -0.0077]],\n",
            "\n",
            "         [[ 0.0181, -0.0371,  0.0101],\n",
            "          [ 0.0056, -0.0275,  0.0121],\n",
            "          [-0.0103, -0.0392,  0.0054]],\n",
            "\n",
            "         [[-0.0158, -0.0109, -0.0014],\n",
            "          [-0.0352, -0.0197,  0.0031],\n",
            "          [-0.0075,  0.0062,  0.0084]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0077,  0.0160,  0.0285],\n",
            "          [-0.0416, -0.0044,  0.0162],\n",
            "          [-0.0275,  0.0311, -0.0242]],\n",
            "\n",
            "         [[-0.0074,  0.0248, -0.0203],\n",
            "          [ 0.0354, -0.0254, -0.0368],\n",
            "          [-0.0238, -0.0169,  0.0068]],\n",
            "\n",
            "         [[ 0.0068, -0.0283, -0.0267],\n",
            "          [ 0.0165,  0.0249, -0.0162],\n",
            "          [-0.0135, -0.0226,  0.0159]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0101,  0.0243, -0.0280],\n",
            "          [ 0.0028,  0.0277,  0.0410],\n",
            "          [ 0.0194,  0.0323, -0.0055]],\n",
            "\n",
            "         [[ 0.0312,  0.0088, -0.0006],\n",
            "          [ 0.0369,  0.0323,  0.0386],\n",
            "          [-0.0398, -0.0292, -0.0328]],\n",
            "\n",
            "         [[-0.0363,  0.0148, -0.0166],\n",
            "          [-0.0413,  0.0222, -0.0206],\n",
            "          [ 0.0207,  0.0387, -0.0265]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0279,  0.0186, -0.0018],\n",
            "          [-0.0182, -0.0338, -0.0413],\n",
            "          [-0.0073,  0.0145,  0.0329]],\n",
            "\n",
            "         [[ 0.0263, -0.0153, -0.0085],\n",
            "          [-0.0216, -0.0306,  0.0221],\n",
            "          [ 0.0097, -0.0177, -0.0292]],\n",
            "\n",
            "         [[ 0.0261, -0.0189,  0.0192],\n",
            "          [ 0.0182,  0.0310,  0.0221],\n",
            "          [-0.0126, -0.0052,  0.0340]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0055,  0.0330,  0.0383],\n",
            "          [ 0.0080, -0.0140, -0.0270],\n",
            "          [-0.0074, -0.0367, -0.0273]],\n",
            "\n",
            "         [[-0.0072, -0.0112, -0.0369],\n",
            "          [ 0.0205, -0.0023, -0.0142],\n",
            "          [ 0.0138, -0.0389, -0.0213]],\n",
            "\n",
            "         [[-0.0125,  0.0220, -0.0131],\n",
            "          [-0.0110, -0.0288,  0.0270],\n",
            "          [-0.0391,  0.0008, -0.0072]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0317,  0.0045, -0.0243],\n",
            "          [ 0.0333,  0.0334,  0.0123],\n",
            "          [-0.0327,  0.0321,  0.0185]],\n",
            "\n",
            "         [[-0.0192, -0.0365, -0.0258],\n",
            "          [-0.0042, -0.0113,  0.0077],\n",
            "          [ 0.0297,  0.0411, -0.0089]],\n",
            "\n",
            "         [[-0.0054, -0.0205, -0.0057],\n",
            "          [ 0.0411,  0.0283, -0.0006],\n",
            "          [-0.0165,  0.0304,  0.0232]]]], device='cuda:0', requires_grad=True)\n",
            "N2_C3.bias Parameter containing:\n",
            "tensor([-0.0383,  0.0395,  0.0049,  0.0411,  0.0058,  0.0402,  0.0007, -0.0410,\n",
            "         0.0223, -0.0240,  0.0115,  0.0044, -0.0159,  0.0158,  0.0008, -0.0124,\n",
            "         0.0356,  0.0087,  0.0222,  0.0224,  0.0103, -0.0260,  0.0219,  0.0408,\n",
            "         0.0094,  0.0125, -0.0291, -0.0210,  0.0390,  0.0330, -0.0197,  0.0351,\n",
            "        -0.0130,  0.0217, -0.0156,  0.0201, -0.0043,  0.0087,  0.0076, -0.0388,\n",
            "         0.0393, -0.0308, -0.0287,  0.0341,  0.0392, -0.0177, -0.0062, -0.0143,\n",
            "        -0.0200,  0.0244, -0.0207, -0.0134,  0.0099, -0.0227,  0.0370,  0.0200,\n",
            "         0.0399,  0.0227, -0.0096,  0.0045,  0.0146,  0.0257, -0.0003,  0.0248,\n",
            "        -0.0132,  0.0099, -0.0225, -0.0134, -0.0396,  0.0403,  0.0293,  0.0264,\n",
            "         0.0393, -0.0245, -0.0375,  0.0369,  0.0217,  0.0336,  0.0047,  0.0320,\n",
            "         0.0064, -0.0221, -0.0303, -0.0382,  0.0040, -0.0298,  0.0097, -0.0205,\n",
            "         0.0384,  0.0216, -0.0386,  0.0001,  0.0037,  0.0112, -0.0293, -0.0144,\n",
            "         0.0317, -0.0252,  0.0201,  0.0401, -0.0117, -0.0049, -0.0087,  0.0194,\n",
            "        -0.0164,  0.0308, -0.0319,  0.0328, -0.0257,  0.0272, -0.0337,  0.0049,\n",
            "        -0.0125,  0.0342, -0.0147, -0.0271,  0.0388, -0.0298, -0.0099, -0.0223,\n",
            "         0.0346, -0.0218, -0.0241,  0.0373, -0.0149,  0.0321, -0.0276,  0.0186],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "N2_C4.weight Parameter containing:\n",
            "tensor([[[[-0.0289,  0.0262, -0.0233],\n",
            "          [-0.0108,  0.0123, -0.0057],\n",
            "          [ 0.0053, -0.0279,  0.0165]],\n",
            "\n",
            "         [[ 0.0201,  0.0136,  0.0290],\n",
            "          [-0.0064, -0.0024, -0.0221],\n",
            "          [ 0.0292,  0.0025,  0.0159]],\n",
            "\n",
            "         [[-0.0123, -0.0009, -0.0189],\n",
            "          [-0.0123, -0.0147, -0.0105],\n",
            "          [-0.0092,  0.0197,  0.0256]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0293,  0.0254,  0.0119],\n",
            "          [-0.0219, -0.0147, -0.0096],\n",
            "          [-0.0257,  0.0292, -0.0199]],\n",
            "\n",
            "         [[-0.0193, -0.0205, -0.0158],\n",
            "          [ 0.0186, -0.0020, -0.0119],\n",
            "          [-0.0142,  0.0072, -0.0127]],\n",
            "\n",
            "         [[ 0.0038, -0.0293,  0.0030],\n",
            "          [ 0.0122, -0.0052,  0.0234],\n",
            "          [-0.0043,  0.0030,  0.0074]]],\n",
            "\n",
            "\n",
            "        [[[-0.0030, -0.0123,  0.0211],\n",
            "          [-0.0082, -0.0014, -0.0263],\n",
            "          [ 0.0243, -0.0072, -0.0075]],\n",
            "\n",
            "         [[-0.0002, -0.0008,  0.0204],\n",
            "          [ 0.0147,  0.0202, -0.0140],\n",
            "          [ 0.0280,  0.0275, -0.0163]],\n",
            "\n",
            "         [[-0.0113,  0.0238, -0.0186],\n",
            "          [-0.0045,  0.0157,  0.0223],\n",
            "          [-0.0246,  0.0192, -0.0226]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0065, -0.0037,  0.0263],\n",
            "          [-0.0009,  0.0219,  0.0096],\n",
            "          [-0.0262,  0.0208,  0.0023]],\n",
            "\n",
            "         [[-0.0157, -0.0287, -0.0164],\n",
            "          [ 0.0171, -0.0079,  0.0029],\n",
            "          [ 0.0060, -0.0108,  0.0170]],\n",
            "\n",
            "         [[ 0.0049, -0.0068, -0.0014],\n",
            "          [ 0.0006,  0.0234, -0.0293],\n",
            "          [-0.0169,  0.0024, -0.0238]]],\n",
            "\n",
            "\n",
            "        [[[-0.0292,  0.0266,  0.0262],\n",
            "          [-0.0233, -0.0147, -0.0042],\n",
            "          [-0.0088, -0.0256,  0.0157]],\n",
            "\n",
            "         [[-0.0077, -0.0085,  0.0178],\n",
            "          [-0.0002,  0.0133, -0.0177],\n",
            "          [ 0.0101,  0.0246,  0.0092]],\n",
            "\n",
            "         [[-0.0223,  0.0105,  0.0075],\n",
            "          [ 0.0109, -0.0059,  0.0069],\n",
            "          [-0.0057,  0.0250, -0.0035]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0261,  0.0256, -0.0147],\n",
            "          [-0.0138, -0.0106, -0.0263],\n",
            "          [ 0.0176, -0.0109, -0.0053]],\n",
            "\n",
            "         [[-0.0255,  0.0176,  0.0238],\n",
            "          [ 0.0182, -0.0290,  0.0238],\n",
            "          [-0.0106, -0.0226, -0.0080]],\n",
            "\n",
            "         [[ 0.0246,  0.0247, -0.0071],\n",
            "          [ 0.0248, -0.0162,  0.0028],\n",
            "          [ 0.0107, -0.0131, -0.0228]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0287, -0.0125,  0.0252],\n",
            "          [-0.0130, -0.0030,  0.0104],\n",
            "          [-0.0101,  0.0107, -0.0115]],\n",
            "\n",
            "         [[-0.0098, -0.0202,  0.0126],\n",
            "          [ 0.0131, -0.0095, -0.0152],\n",
            "          [-0.0055,  0.0043, -0.0264]],\n",
            "\n",
            "         [[-0.0194,  0.0208, -0.0241],\n",
            "          [-0.0017, -0.0053,  0.0292],\n",
            "          [-0.0092, -0.0080,  0.0054]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0190, -0.0092, -0.0172],\n",
            "          [ 0.0162, -0.0112,  0.0082],\n",
            "          [-0.0217,  0.0278, -0.0288]],\n",
            "\n",
            "         [[-0.0254, -0.0220, -0.0021],\n",
            "          [-0.0235,  0.0228, -0.0088],\n",
            "          [-0.0201,  0.0247, -0.0279]],\n",
            "\n",
            "         [[ 0.0224, -0.0159,  0.0264],\n",
            "          [ 0.0275, -0.0261, -0.0125],\n",
            "          [ 0.0294, -0.0185, -0.0269]]],\n",
            "\n",
            "\n",
            "        [[[-0.0041,  0.0228, -0.0273],\n",
            "          [-0.0160,  0.0264,  0.0087],\n",
            "          [ 0.0052, -0.0034,  0.0125]],\n",
            "\n",
            "         [[-0.0138, -0.0154,  0.0132],\n",
            "          [-0.0183, -0.0046,  0.0236],\n",
            "          [ 0.0140,  0.0162, -0.0099]],\n",
            "\n",
            "         [[ 0.0264,  0.0185,  0.0082],\n",
            "          [ 0.0276, -0.0196, -0.0030],\n",
            "          [ 0.0012, -0.0252, -0.0243]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0083,  0.0166, -0.0077],\n",
            "          [ 0.0236,  0.0233, -0.0086],\n",
            "          [ 0.0129, -0.0023,  0.0030]],\n",
            "\n",
            "         [[-0.0110, -0.0114,  0.0179],\n",
            "          [ 0.0055,  0.0291,  0.0174],\n",
            "          [ 0.0244, -0.0057,  0.0231]],\n",
            "\n",
            "         [[-0.0009, -0.0205, -0.0020],\n",
            "          [-0.0083,  0.0073,  0.0112],\n",
            "          [ 0.0062,  0.0255,  0.0146]]],\n",
            "\n",
            "\n",
            "        [[[-0.0106, -0.0094,  0.0213],\n",
            "          [-0.0292,  0.0098, -0.0029],\n",
            "          [ 0.0266,  0.0265,  0.0036]],\n",
            "\n",
            "         [[-0.0284, -0.0121,  0.0250],\n",
            "          [ 0.0189,  0.0136, -0.0146],\n",
            "          [-0.0200, -0.0084,  0.0116]],\n",
            "\n",
            "         [[ 0.0136,  0.0148, -0.0058],\n",
            "          [ 0.0111, -0.0061, -0.0106],\n",
            "          [-0.0033,  0.0094,  0.0099]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0013,  0.0261,  0.0001],\n",
            "          [-0.0202,  0.0286,  0.0124],\n",
            "          [-0.0123, -0.0252,  0.0117]],\n",
            "\n",
            "         [[-0.0137, -0.0117,  0.0143],\n",
            "          [ 0.0227,  0.0265, -0.0145],\n",
            "          [-0.0272, -0.0125, -0.0164]],\n",
            "\n",
            "         [[ 0.0018, -0.0179,  0.0011],\n",
            "          [ 0.0289, -0.0004,  0.0103],\n",
            "          [ 0.0113, -0.0171, -0.0099]]]], device='cuda:0', requires_grad=True)\n",
            "N2_C4.bias Parameter containing:\n",
            "tensor([-0.0135,  0.0265, -0.0071,  0.0156,  0.0116,  0.0082,  0.0261,  0.0039,\n",
            "        -0.0100, -0.0156, -0.0047, -0.0177, -0.0042,  0.0151,  0.0185, -0.0076,\n",
            "        -0.0248, -0.0036, -0.0198,  0.0025, -0.0048,  0.0214, -0.0002, -0.0113,\n",
            "         0.0251, -0.0273,  0.0123, -0.0143,  0.0094,  0.0073, -0.0002, -0.0264,\n",
            "         0.0281, -0.0056, -0.0058,  0.0060, -0.0193, -0.0205,  0.0058, -0.0027,\n",
            "        -0.0228,  0.0205, -0.0079, -0.0125,  0.0035,  0.0035,  0.0083,  0.0266,\n",
            "         0.0147,  0.0080,  0.0216,  0.0130, -0.0254, -0.0071,  0.0101, -0.0051,\n",
            "         0.0169,  0.0086, -0.0250, -0.0007,  0.0182, -0.0212,  0.0164, -0.0283,\n",
            "         0.0245,  0.0063,  0.0126, -0.0018,  0.0160,  0.0016, -0.0094,  0.0200,\n",
            "         0.0074, -0.0084, -0.0209, -0.0177, -0.0169, -0.0192,  0.0117, -0.0184,\n",
            "         0.0160, -0.0214,  0.0066,  0.0283,  0.0152,  0.0205,  0.0095, -0.0011,\n",
            "        -0.0122, -0.0057,  0.0091, -0.0095, -0.0011, -0.0182, -0.0010, -0.0202,\n",
            "        -0.0200,  0.0216, -0.0137,  0.0080, -0.0129,  0.0246, -0.0082,  0.0235,\n",
            "        -0.0112,  0.0221, -0.0036,  0.0265, -0.0220,  0.0006,  0.0091, -0.0119,\n",
            "        -0.0051, -0.0055,  0.0290,  0.0083, -0.0285, -0.0011, -0.0266, -0.0293,\n",
            "         0.0001,  0.0169, -0.0131, -0.0137, -0.0096, -0.0201,  0.0172, -0.0141],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "N2_D1.weight Parameter containing:\n",
            "tensor([[ 8.5191e-04, -1.8550e-03, -2.0380e-03,  ...,  1.2587e-03,\n",
            "          3.6572e-04, -2.7370e-03],\n",
            "        [ 2.5768e-04,  6.0957e-04,  3.6125e-04,  ..., -2.2069e-03,\n",
            "          2.3139e-03, -2.5153e-03],\n",
            "        [ 8.8733e-04,  3.9857e-04,  2.4667e-03,  ...,  1.9881e-03,\n",
            "          1.9701e-03, -1.2505e-04],\n",
            "        ...,\n",
            "        [ 7.4648e-04,  2.3590e-03,  8.7131e-04,  ..., -2.6040e-05,\n",
            "          2.5775e-03,  2.5248e-03],\n",
            "        [ 5.9711e-04, -1.8871e-03, -7.0276e-04,  ..., -2.4941e-03,\n",
            "          2.5989e-03, -9.4210e-04],\n",
            "        [ 1.4635e-03, -2.7541e-03, -1.6735e-03,  ..., -2.4397e-03,\n",
            "          4.1450e-04, -1.5301e-03]], device='cuda:0', requires_grad=True)\n",
            "N2_D1.bias Parameter containing:\n",
            "tensor([-1.2024e-03, -2.0768e-03, -7.3893e-04, -1.2598e-03, -1.2588e-03,\n",
            "         1.2836e-03,  1.8661e-03,  1.3356e-03,  2.4065e-03, -7.7909e-06,\n",
            "         2.4517e-04, -9.0333e-04, -1.2529e-03,  8.0630e-04,  7.2772e-04,\n",
            "         2.3327e-03, -1.8569e-03,  1.9670e-04, -2.2162e-03,  1.0029e-03,\n",
            "        -1.1861e-03, -5.4159e-04, -1.9288e-03,  4.9186e-04,  2.0499e-03,\n",
            "        -6.8328e-04,  1.4267e-03,  4.8612e-04, -1.9827e-03,  2.2513e-04,\n",
            "        -1.3772e-03,  1.9266e-03, -1.4271e-03, -2.5804e-03,  1.4537e-03,\n",
            "         2.1953e-03,  3.8780e-04, -1.1045e-03, -1.5004e-03,  3.8365e-04,\n",
            "         1.1926e-03,  2.6620e-03, -2.0781e-03, -4.1540e-05,  2.5671e-03,\n",
            "        -2.5050e-03,  3.6568e-04,  1.1716e-04, -1.7622e-03, -2.6576e-03,\n",
            "         1.7532e-03, -1.0395e-04,  4.7727e-04, -4.8666e-06, -2.4289e-03,\n",
            "         1.5165e-04,  1.5586e-03, -4.2741e-04, -1.1802e-03,  1.8629e-03,\n",
            "        -4.6451e-04, -3.5044e-04,  8.7103e-04, -1.8373e-03,  1.6461e-03,\n",
            "         2.7214e-03, -2.4612e-03,  5.1966e-04, -3.5212e-04,  2.2012e-03,\n",
            "         3.6200e-04,  1.6387e-03, -9.8801e-04,  2.7235e-03, -1.1442e-04,\n",
            "         2.3780e-03,  2.6199e-03, -1.6872e-03, -1.2106e-03,  1.1070e-03,\n",
            "        -1.1881e-03,  1.0787e-04,  2.4317e-03, -1.4905e-03,  1.0539e-03,\n",
            "        -1.3566e-03, -8.5641e-04,  2.2117e-03, -6.5167e-05,  1.4339e-03,\n",
            "         7.9079e-04, -9.4538e-04, -7.9334e-04,  5.9340e-04, -4.4363e-04,\n",
            "         2.2649e-04, -4.3392e-04, -2.3211e-03,  2.6275e-03, -1.6187e-04,\n",
            "        -2.1786e-03, -1.9526e-03,  1.8452e-03, -2.4618e-03, -2.2694e-03,\n",
            "         1.0129e-04, -1.1057e-03, -1.8035e-03,  8.2397e-04, -1.8748e-03,\n",
            "         1.6742e-03,  2.0960e-03,  2.4910e-03,  2.4984e-03,  2.2419e-03,\n",
            "        -1.1441e-03, -7.8786e-04,  1.9501e-03, -1.0808e-03, -2.4724e-03,\n",
            "        -1.5844e-03,  9.9985e-04,  4.7754e-04, -4.0169e-04,  2.2734e-03,\n",
            "         2.8077e-04,  2.1811e-03,  1.9533e-03,  3.6214e-04, -1.2049e-04,\n",
            "        -1.7160e-03,  4.5465e-05, -1.7391e-03, -2.4512e-03, -1.9859e-03,\n",
            "         1.9789e-03,  1.9252e-03,  2.6232e-03, -1.7700e-05,  1.6117e-03,\n",
            "        -1.8365e-03,  1.9155e-04, -2.9577e-05,  6.1586e-04, -2.7315e-03,\n",
            "        -5.2568e-04,  2.2897e-03, -6.0903e-04,  2.7754e-04, -1.3207e-03,\n",
            "         2.8981e-04,  3.9990e-04, -1.2909e-03, -1.5678e-03,  1.8612e-03,\n",
            "         3.0813e-04, -1.6554e-03, -1.3299e-04, -1.6333e-03,  2.2416e-04,\n",
            "         9.8987e-05,  2.0915e-03,  4.6140e-05, -2.3430e-04,  1.7098e-03,\n",
            "        -5.4218e-05, -1.4743e-03,  4.8102e-04,  4.1976e-04,  2.2652e-04,\n",
            "         1.7985e-04,  2.0157e-03,  1.2549e-03, -9.6173e-04,  2.5713e-03,\n",
            "        -2.4474e-03, -1.9202e-03,  8.1899e-04, -1.5707e-03,  1.2619e-03,\n",
            "        -2.4962e-03, -8.3321e-04, -1.1553e-03,  2.2665e-03,  1.9707e-03,\n",
            "         1.3545e-03,  2.3130e-03, -2.1949e-03,  1.3145e-03, -9.1775e-04,\n",
            "         2.0227e-03,  5.7335e-04, -7.0750e-04, -1.4330e-03, -1.8213e-03,\n",
            "        -2.4399e-03, -1.1341e-03, -2.1126e-03, -1.0771e-04, -2.2210e-03,\n",
            "         2.7039e-03, -1.6881e-03, -7.0378e-04,  3.7523e-04,  1.9895e-03,\n",
            "        -2.7089e-03,  2.5154e-03,  4.8578e-04, -6.9878e-04, -7.6750e-04,\n",
            "        -8.9933e-04,  1.3985e-03, -1.7288e-03, -9.4916e-04,  1.2745e-03,\n",
            "        -2.4980e-03, -1.8359e-04, -1.5639e-03, -2.0507e-03,  2.0271e-04,\n",
            "         5.9025e-04,  1.6895e-04, -2.2963e-03,  8.4749e-04,  7.1477e-05,\n",
            "        -2.6389e-03,  1.2130e-03, -7.9796e-04,  1.1235e-04, -1.6877e-03,\n",
            "         2.0942e-03,  6.7002e-04, -1.2656e-03,  2.0421e-03,  1.6719e-03,\n",
            "        -1.4981e-03, -1.9961e-03,  1.1572e-03,  1.1532e-03,  1.8905e-03,\n",
            "         1.6025e-03, -2.1795e-03, -1.7674e-03, -2.3241e-03,  2.7202e-03,\n",
            "         1.4858e-03, -4.5314e-04, -1.3467e-03,  2.4806e-03, -1.5382e-03,\n",
            "         3.3975e-04, -1.7792e-03, -4.2911e-04,  8.6674e-04, -1.3881e-03,\n",
            "         2.3551e-03], device='cuda:0', requires_grad=True)\n",
            "N2_D2.weight Parameter containing:\n",
            "tensor([[-0.0328,  0.0030, -0.0537,  ...,  0.0423,  0.0327, -0.0189],\n",
            "        [-0.0402, -0.0538, -0.0162,  ...,  0.0560, -0.0564,  0.0010],\n",
            "        [-0.0435, -0.0040, -0.0060,  ...,  0.0122, -0.0376, -0.0437],\n",
            "        ...,\n",
            "        [-0.0037, -0.0217,  0.0357,  ..., -0.0336, -0.0014, -0.0473],\n",
            "        [ 0.0471, -0.0432, -0.0075,  ...,  0.0200, -0.0433,  0.0340],\n",
            "        [-0.0607,  0.0555, -0.0278,  ..., -0.0167, -0.0526, -0.0362]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "N2_D2.bias Parameter containing:\n",
            "tensor([-0.0265, -0.0027,  0.0114, -0.0551, -0.0352, -0.0450, -0.0406,  0.0579,\n",
            "         0.0308,  0.0280,  0.0361, -0.0159, -0.0595,  0.0359,  0.0533,  0.0141,\n",
            "        -0.0377, -0.0311, -0.0003,  0.0558,  0.0281,  0.0014, -0.0189,  0.0103,\n",
            "         0.0397,  0.0504, -0.0090, -0.0037,  0.0394,  0.0233, -0.0384,  0.0457,\n",
            "         0.0623, -0.0529, -0.0132, -0.0061, -0.0426, -0.0453, -0.0174,  0.0056,\n",
            "        -0.0103, -0.0125,  0.0002,  0.0082,  0.0335, -0.0126,  0.0553,  0.0339,\n",
            "        -0.0338, -0.0022,  0.0600,  0.0531, -0.0465,  0.0209,  0.0563, -0.0584,\n",
            "        -0.0031,  0.0259, -0.0201,  0.0454,  0.0544,  0.0622,  0.0553,  0.0331,\n",
            "        -0.0469,  0.0608, -0.0070, -0.0395, -0.0290, -0.0328,  0.0397,  0.0504,\n",
            "        -0.0022, -0.0333,  0.0447, -0.0159,  0.0066, -0.0391, -0.0624, -0.0505,\n",
            "         0.0472,  0.0044, -0.0462,  0.0624,  0.0499, -0.0338, -0.0405,  0.0297,\n",
            "        -0.0489,  0.0496,  0.0390, -0.0263,  0.0297,  0.0467,  0.0131, -0.0622,\n",
            "         0.0167,  0.0192,  0.0097,  0.0303,  0.0490, -0.0007, -0.0316, -0.0269,\n",
            "         0.0343,  0.0418,  0.0283, -0.0192,  0.0193, -0.0388,  0.0447, -0.0435,\n",
            "         0.0320,  0.0294, -0.0036, -0.0332,  0.0329,  0.0162, -0.0139,  0.0279,\n",
            "        -0.0230,  0.0144,  0.0067, -0.0294,  0.0613, -0.0253, -0.0309,  0.0474,\n",
            "         0.0617, -0.0572,  0.0306,  0.0150,  0.0035,  0.0041, -0.0416,  0.0034,\n",
            "        -0.0107,  0.0612, -0.0437, -0.0366,  0.0498,  0.0292, -0.0202, -0.0443,\n",
            "        -0.0090, -0.0080,  0.0288, -0.0133,  0.0041,  0.0547, -0.0407,  0.0011,\n",
            "        -0.0504,  0.0184,  0.0620,  0.0445,  0.0365, -0.0259, -0.0095, -0.0522,\n",
            "         0.0426,  0.0199, -0.0463, -0.0508, -0.0558,  0.0462,  0.0222,  0.0327,\n",
            "        -0.0561, -0.0566, -0.0456,  0.0262, -0.0259,  0.0539, -0.0045,  0.0093,\n",
            "        -0.0379, -0.0132, -0.0258,  0.0324,  0.0423, -0.0192,  0.0001, -0.0618,\n",
            "        -0.0115,  0.0599,  0.0301, -0.0113,  0.0056, -0.0546, -0.0331,  0.0067,\n",
            "        -0.0017, -0.0355, -0.0142,  0.0274,  0.0567,  0.0439,  0.0071,  0.0182,\n",
            "        -0.0100, -0.0582,  0.0534,  0.0261,  0.0342,  0.0076, -0.0039, -0.0215,\n",
            "         0.0017,  0.0045,  0.0415, -0.0133,  0.0279, -0.0554,  0.0274,  0.0544,\n",
            "         0.0558,  0.0573, -0.0536, -0.0591,  0.0573,  0.0589,  0.0610,  0.0411,\n",
            "         0.0570,  0.0581, -0.0320, -0.0245,  0.0473,  0.0123,  0.0072, -0.0509,\n",
            "        -0.0353, -0.0049,  0.0396, -0.0331, -0.0212, -0.0570,  0.0091, -0.0198,\n",
            "         0.0205, -0.0377,  0.0042,  0.0245, -0.0447, -0.0555, -0.0025, -0.0472,\n",
            "         0.0149, -0.0413, -0.0044, -0.0298, -0.0463,  0.0271,  0.0122,  0.0566],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "outputs.weight Parameter containing:\n",
            "tensor([[-0.0598, -0.0430, -0.0371,  ...,  0.0160, -0.0088,  0.0520],\n",
            "        [ 0.0032,  0.0057,  0.0399,  ..., -0.0410, -0.0419, -0.0518],\n",
            "        [ 0.0065, -0.0008,  0.0495,  ...,  0.0251, -0.0581, -0.0580],\n",
            "        ...,\n",
            "        [-0.0571,  0.0293,  0.0298,  ...,  0.0516,  0.0194,  0.0123],\n",
            "        [ 0.0337, -0.0510,  0.0479,  ...,  0.0587,  0.0428,  0.0516],\n",
            "        [ 0.0506, -0.0571, -0.0151,  ..., -0.0265, -0.0399,  0.0330]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "outputs.bias Parameter containing:\n",
            "tensor([-0.0484,  0.0121,  0.0262,  0.0544,  0.0320,  0.0286, -0.0336, -0.0169,\n",
            "         0.0026,  0.0145], device='cuda:0', requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "      print(name)"
      ],
      "metadata": {
        "id": "ApTIVD5jSwtK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bad4d8e-882b-467c-f47f-53f0a85e0d87"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N2_C1.weight\n",
            "N2_C1.bias\n",
            "N2_C2.weight\n",
            "N2_C2.bias\n",
            "N2_C3.weight\n",
            "N2_C3.bias\n",
            "N2_C4.weight\n",
            "N2_C4.bias\n",
            "N2_D1.weight\n",
            "N2_D1.bias\n",
            "N2_D2.weight\n",
            "N2_D2.bias\n",
            "outputs.weight\n",
            "outputs.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "for epoch in range(1, 81):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "id": "sSHe5TyFVx97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92fa9276-1bfb-4f2c-86a1-996d00291330"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [3124/50000 (100%)]\tLoss: 2761.240381\n",
            "\n",
            "Test set: Average loss: 463.8155, Accuracy: 6339/10000 (63%)\n",
            "\n",
            "Train Epoch: 2 [3124/50000 (100%)]\tLoss: 358.385198\n",
            "\n",
            "Test set: Average loss: 580.6111, Accuracy: 6395/10000 (64%)\n",
            "\n",
            "Train Epoch: 3 [3124/50000 (100%)]\tLoss: 180.158290\n",
            "\n",
            "Test set: Average loss: 647.4731, Accuracy: 6466/10000 (65%)\n",
            "\n",
            "Train Epoch: 4 [3124/50000 (100%)]\tLoss: 95.028928\n",
            "\n",
            "Test set: Average loss: 803.2594, Accuracy: 6535/10000 (65%)\n",
            "\n",
            "Train Epoch: 5 [3124/50000 (100%)]\tLoss: 35.183495\n",
            "\n",
            "Test set: Average loss: 1152.2644, Accuracy: 6511/10000 (65%)\n",
            "\n",
            "Train Epoch: 6 [3124/50000 (100%)]\tLoss: 19.837166\n",
            "\n",
            "Test set: Average loss: 1322.4012, Accuracy: 6571/10000 (66%)\n",
            "\n",
            "Train Epoch: 7 [3124/50000 (100%)]\tLoss: 14.681975\n",
            "\n",
            "Test set: Average loss: 1481.6002, Accuracy: 6519/10000 (65%)\n",
            "\n",
            "Train Epoch: 8 [3124/50000 (100%)]\tLoss: 7.099811\n",
            "\n",
            "Test set: Average loss: 1592.7847, Accuracy: 6593/10000 (66%)\n",
            "\n",
            "Train Epoch: 9 [3124/50000 (100%)]\tLoss: 1.909379\n",
            "\n",
            "Test set: Average loss: 1736.8958, Accuracy: 6604/10000 (66%)\n",
            "\n",
            "Train Epoch: 10 [3124/50000 (100%)]\tLoss: 2.448507\n",
            "\n",
            "Test set: Average loss: 1773.2815, Accuracy: 6577/10000 (66%)\n",
            "\n",
            "Train Epoch: 11 [3124/50000 (100%)]\tLoss: 0.380777\n",
            "\n",
            "Test set: Average loss: 1829.4626, Accuracy: 6595/10000 (66%)\n",
            "\n",
            "Train Epoch: 12 [3124/50000 (100%)]\tLoss: 0.157140\n",
            "\n",
            "Test set: Average loss: 1871.8447, Accuracy: 6592/10000 (66%)\n",
            "\n",
            "Train Epoch: 13 [3124/50000 (100%)]\tLoss: 0.113918\n",
            "\n",
            "Test set: Average loss: 1904.9508, Accuracy: 6596/10000 (66%)\n",
            "\n",
            "Train Epoch: 14 [3124/50000 (100%)]\tLoss: 0.087385\n",
            "\n",
            "Test set: Average loss: 1935.0686, Accuracy: 6589/10000 (66%)\n",
            "\n",
            "Train Epoch: 15 [3124/50000 (100%)]\tLoss: 0.074583\n",
            "\n",
            "Test set: Average loss: 1959.1484, Accuracy: 6594/10000 (66%)\n",
            "\n",
            "Train Epoch: 16 [3124/50000 (100%)]\tLoss: 0.063817\n",
            "\n",
            "Test set: Average loss: 1982.5452, Accuracy: 6595/10000 (66%)\n",
            "\n",
            "Train Epoch: 17 [3124/50000 (100%)]\tLoss: 0.056729\n",
            "\n",
            "Test set: Average loss: 2001.9115, Accuracy: 6594/10000 (66%)\n",
            "\n",
            "Train Epoch: 18 [3124/50000 (100%)]\tLoss: 0.050845\n",
            "\n",
            "Test set: Average loss: 2020.9985, Accuracy: 6588/10000 (66%)\n",
            "\n",
            "Train Epoch: 19 [3124/50000 (100%)]\tLoss: 0.045803\n",
            "\n",
            "Test set: Average loss: 2038.2384, Accuracy: 6591/10000 (66%)\n",
            "\n",
            "Train Epoch: 20 [3124/50000 (100%)]\tLoss: 0.041909\n",
            "\n",
            "Test set: Average loss: 2053.5256, Accuracy: 6595/10000 (66%)\n",
            "\n",
            "Train Epoch: 21 [3124/50000 (100%)]\tLoss: 0.038540\n",
            "\n",
            "Test set: Average loss: 2068.4644, Accuracy: 6592/10000 (66%)\n",
            "\n",
            "Train Epoch: 22 [3124/50000 (100%)]\tLoss: 0.035602\n",
            "\n",
            "Test set: Average loss: 2082.1599, Accuracy: 6589/10000 (66%)\n",
            "\n",
            "Train Epoch: 23 [3124/50000 (100%)]\tLoss: 0.033159\n",
            "\n",
            "Test set: Average loss: 2095.3467, Accuracy: 6587/10000 (66%)\n",
            "\n",
            "Train Epoch: 24 [3124/50000 (100%)]\tLoss: 0.030940\n",
            "\n",
            "Test set: Average loss: 2107.4375, Accuracy: 6587/10000 (66%)\n",
            "\n",
            "Train Epoch: 25 [3124/50000 (100%)]\tLoss: 0.028989\n",
            "\n",
            "Test set: Average loss: 2119.2388, Accuracy: 6585/10000 (66%)\n",
            "\n",
            "Train Epoch: 26 [3124/50000 (100%)]\tLoss: 0.027300\n",
            "\n",
            "Test set: Average loss: 2130.1558, Accuracy: 6586/10000 (66%)\n",
            "\n",
            "Train Epoch: 27 [3124/50000 (100%)]\tLoss: 0.025827\n",
            "\n",
            "Test set: Average loss: 2140.8230, Accuracy: 6585/10000 (66%)\n",
            "\n",
            "Train Epoch: 28 [3124/50000 (100%)]\tLoss: 0.024418\n",
            "\n",
            "Test set: Average loss: 2150.7097, Accuracy: 6589/10000 (66%)\n",
            "\n",
            "Train Epoch: 29 [3124/50000 (100%)]\tLoss: 0.023212\n",
            "\n",
            "Test set: Average loss: 2160.5095, Accuracy: 6587/10000 (66%)\n",
            "\n",
            "Train Epoch: 30 [3124/50000 (100%)]\tLoss: 0.022062\n",
            "\n",
            "Test set: Average loss: 2169.5298, Accuracy: 6588/10000 (66%)\n",
            "\n",
            "Train Epoch: 31 [3124/50000 (100%)]\tLoss: 0.021079\n",
            "\n",
            "Test set: Average loss: 2178.5393, Accuracy: 6584/10000 (66%)\n",
            "\n",
            "Train Epoch: 32 [3124/50000 (100%)]\tLoss: 0.020145\n",
            "\n",
            "Test set: Average loss: 2187.0439, Accuracy: 6582/10000 (66%)\n",
            "\n",
            "Train Epoch: 33 [3124/50000 (100%)]\tLoss: 0.019279\n",
            "\n",
            "Test set: Average loss: 2195.3369, Accuracy: 6586/10000 (66%)\n",
            "\n",
            "Train Epoch: 34 [3124/50000 (100%)]\tLoss: 0.018477\n",
            "\n",
            "Test set: Average loss: 2203.3157, Accuracy: 6584/10000 (66%)\n",
            "\n",
            "Train Epoch: 35 [3124/50000 (100%)]\tLoss: 0.017729\n",
            "\n",
            "Test set: Average loss: 2210.7986, Accuracy: 6585/10000 (66%)\n",
            "\n",
            "Train Epoch: 36 [3124/50000 (100%)]\tLoss: 0.017078\n",
            "\n",
            "Test set: Average loss: 2218.2175, Accuracy: 6584/10000 (66%)\n",
            "\n",
            "Train Epoch: 37 [3124/50000 (100%)]\tLoss: 0.016434\n",
            "\n",
            "Test set: Average loss: 2225.3318, Accuracy: 6584/10000 (66%)\n",
            "\n",
            "Train Epoch: 38 [3124/50000 (100%)]\tLoss: 0.015854\n",
            "\n",
            "Test set: Average loss: 2232.2207, Accuracy: 6585/10000 (66%)\n",
            "\n",
            "Train Epoch: 39 [3124/50000 (100%)]\tLoss: 0.015311\n",
            "\n",
            "Test set: Average loss: 2239.0354, Accuracy: 6581/10000 (66%)\n",
            "\n",
            "Train Epoch: 40 [3124/50000 (100%)]\tLoss: 0.014793\n",
            "\n",
            "Test set: Average loss: 2245.6721, Accuracy: 6583/10000 (66%)\n",
            "\n",
            "Train Epoch: 41 [3124/50000 (100%)]\tLoss: 0.014307\n",
            "\n",
            "Test set: Average loss: 2252.1216, Accuracy: 6581/10000 (66%)\n",
            "\n",
            "Train Epoch: 42 [3124/50000 (100%)]\tLoss: 0.013869\n",
            "\n",
            "Test set: Average loss: 2258.2524, Accuracy: 6582/10000 (66%)\n",
            "\n",
            "Train Epoch: 43 [3124/50000 (100%)]\tLoss: 0.013436\n",
            "\n",
            "Test set: Average loss: 2264.2021, Accuracy: 6580/10000 (66%)\n",
            "\n",
            "Train Epoch: 44 [3124/50000 (100%)]\tLoss: 0.013031\n",
            "\n",
            "Test set: Average loss: 2270.1040, Accuracy: 6580/10000 (66%)\n",
            "\n",
            "Train Epoch: 45 [3124/50000 (100%)]\tLoss: 0.012681\n",
            "\n",
            "Test set: Average loss: 2275.8127, Accuracy: 6580/10000 (66%)\n",
            "\n",
            "Train Epoch: 46 [3124/50000 (100%)]\tLoss: 0.012285\n",
            "\n",
            "Test set: Average loss: 2281.3091, Accuracy: 6581/10000 (66%)\n",
            "\n",
            "Train Epoch: 47 [3124/50000 (100%)]\tLoss: 0.011949\n",
            "\n",
            "Test set: Average loss: 2286.6899, Accuracy: 6580/10000 (66%)\n",
            "\n",
            "Train Epoch: 48 [3124/50000 (100%)]\tLoss: 0.011630\n",
            "\n",
            "Test set: Average loss: 2291.9773, Accuracy: 6581/10000 (66%)\n",
            "\n",
            "Train Epoch: 49 [3124/50000 (100%)]\tLoss: 0.011328\n",
            "\n",
            "Test set: Average loss: 2297.2371, Accuracy: 6578/10000 (66%)\n",
            "\n",
            "Train Epoch: 50 [3124/50000 (100%)]\tLoss: 0.011031\n",
            "\n",
            "Test set: Average loss: 2302.3735, Accuracy: 6578/10000 (66%)\n",
            "\n",
            "Train Epoch: 51 [3124/50000 (100%)]\tLoss: 0.010751\n",
            "\n",
            "Test set: Average loss: 2307.3591, Accuracy: 6578/10000 (66%)\n",
            "\n",
            "Train Epoch: 52 [3124/50000 (100%)]\tLoss: 0.010489\n",
            "\n",
            "Test set: Average loss: 2312.2073, Accuracy: 6579/10000 (66%)\n",
            "\n",
            "Train Epoch: 53 [3124/50000 (100%)]\tLoss: 0.010236\n",
            "\n",
            "Test set: Average loss: 2316.9021, Accuracy: 6576/10000 (66%)\n",
            "\n",
            "Train Epoch: 54 [3124/50000 (100%)]\tLoss: 0.010037\n",
            "\n",
            "Test set: Average loss: 2321.6526, Accuracy: 6578/10000 (66%)\n",
            "\n",
            "Train Epoch: 55 [3124/50000 (100%)]\tLoss: 0.009767\n",
            "\n",
            "Test set: Average loss: 2326.1145, Accuracy: 6576/10000 (66%)\n",
            "\n",
            "Train Epoch: 56 [3124/50000 (100%)]\tLoss: 0.009543\n",
            "\n",
            "Test set: Average loss: 2330.5322, Accuracy: 6576/10000 (66%)\n",
            "\n",
            "Train Epoch: 57 [3124/50000 (100%)]\tLoss: 0.009336\n",
            "\n",
            "Test set: Average loss: 2334.8899, Accuracy: 6576/10000 (66%)\n",
            "\n",
            "Train Epoch: 58 [3124/50000 (100%)]\tLoss: 0.009128\n",
            "\n",
            "Test set: Average loss: 2339.1567, Accuracy: 6576/10000 (66%)\n",
            "\n",
            "Train Epoch: 59 [3124/50000 (100%)]\tLoss: 0.008943\n",
            "\n",
            "Test set: Average loss: 2343.3088, Accuracy: 6576/10000 (66%)\n",
            "\n",
            "Train Epoch: 60 [3124/50000 (100%)]\tLoss: 0.008747\n",
            "\n",
            "Test set: Average loss: 2347.4668, Accuracy: 6576/10000 (66%)\n",
            "\n",
            "Train Epoch: 61 [3124/50000 (100%)]\tLoss: 0.008568\n",
            "\n",
            "Test set: Average loss: 2351.5642, Accuracy: 6576/10000 (66%)\n",
            "\n",
            "Train Epoch: 62 [3124/50000 (100%)]\tLoss: 0.008391\n",
            "\n",
            "Test set: Average loss: 2355.5811, Accuracy: 6576/10000 (66%)\n",
            "\n",
            "Train Epoch: 63 [3124/50000 (100%)]\tLoss: 0.008230\n",
            "\n",
            "Test set: Average loss: 2359.4792, Accuracy: 6576/10000 (66%)\n",
            "\n",
            "Train Epoch: 64 [3124/50000 (100%)]\tLoss: 0.008074\n",
            "\n",
            "Test set: Average loss: 2363.2871, Accuracy: 6576/10000 (66%)\n",
            "\n",
            "Train Epoch: 65 [3124/50000 (100%)]\tLoss: 0.007923\n",
            "\n",
            "Test set: Average loss: 2367.0740, Accuracy: 6576/10000 (66%)\n",
            "\n",
            "Train Epoch: 66 [3124/50000 (100%)]\tLoss: 0.007766\n",
            "\n",
            "Test set: Average loss: 2370.7615, Accuracy: 6577/10000 (66%)\n",
            "\n",
            "Train Epoch: 67 [3124/50000 (100%)]\tLoss: 0.007624\n",
            "\n",
            "Test set: Average loss: 2374.4155, Accuracy: 6577/10000 (66%)\n",
            "\n",
            "Train Epoch: 68 [3124/50000 (100%)]\tLoss: 0.007485\n",
            "\n",
            "Test set: Average loss: 2378.0217, Accuracy: 6577/10000 (66%)\n",
            "\n",
            "Train Epoch: 69 [3124/50000 (100%)]\tLoss: 0.007356\n",
            "\n",
            "Test set: Average loss: 2381.5347, Accuracy: 6578/10000 (66%)\n",
            "\n",
            "Train Epoch: 70 [3124/50000 (100%)]\tLoss: 0.007225\n",
            "\n",
            "Test set: Average loss: 2385.0288, Accuracy: 6577/10000 (66%)\n",
            "\n",
            "Train Epoch: 71 [3124/50000 (100%)]\tLoss: 0.007104\n",
            "\n",
            "Test set: Average loss: 2388.4539, Accuracy: 6579/10000 (66%)\n",
            "\n",
            "Train Epoch: 72 [3124/50000 (100%)]\tLoss: 0.006978\n",
            "\n",
            "Test set: Average loss: 2391.8147, Accuracy: 6578/10000 (66%)\n",
            "\n",
            "Train Epoch: 73 [3124/50000 (100%)]\tLoss: 0.006860\n",
            "\n",
            "Test set: Average loss: 2395.1355, Accuracy: 6578/10000 (66%)\n",
            "\n",
            "Train Epoch: 74 [3124/50000 (100%)]\tLoss: 0.006749\n",
            "\n",
            "Test set: Average loss: 2398.4180, Accuracy: 6577/10000 (66%)\n",
            "\n",
            "Train Epoch: 75 [3124/50000 (100%)]\tLoss: 0.006637\n",
            "\n",
            "Test set: Average loss: 2401.6040, Accuracy: 6577/10000 (66%)\n",
            "\n",
            "Train Epoch: 76 [3124/50000 (100%)]\tLoss: 0.006530\n",
            "\n",
            "Test set: Average loss: 2404.8354, Accuracy: 6577/10000 (66%)\n",
            "\n",
            "Train Epoch: 77 [3124/50000 (100%)]\tLoss: 0.006435\n",
            "\n",
            "Test set: Average loss: 2407.9802, Accuracy: 6577/10000 (66%)\n",
            "\n",
            "Train Epoch: 78 [3124/50000 (100%)]\tLoss: 0.006331\n",
            "\n",
            "Test set: Average loss: 2411.0796, Accuracy: 6578/10000 (66%)\n",
            "\n",
            "Train Epoch: 79 [3124/50000 (100%)]\tLoss: 0.006234\n",
            "\n",
            "Test set: Average loss: 2414.1245, Accuracy: 6578/10000 (66%)\n",
            "\n",
            "Train Epoch: 80 [3124/50000 (100%)]\tLoss: 0.006136\n",
            "\n",
            "Test set: Average loss: 2417.1274, Accuracy: 6578/10000 (66%)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}