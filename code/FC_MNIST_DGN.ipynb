{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FC_MNIST_DGN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "mWCIOzvSdGdh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saSn28Qj66LO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import argparse\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as initialization\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Constants"
      ],
      "metadata": {
        "id": "XRgKpvb8dI9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 784\n",
        "depth = 6\n",
        "width = 128"
      ],
      "metadata": {
        "id": "AFPABiev1eKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "OKc1XvYxAyXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args={}\n",
        "kwargs={}\n",
        "args['batch_size']=32\n",
        "args['epochs']=100  #The number of Epochs is the number of times you go through the full dataset. \n",
        "args['lr']=1e-4 #Learning rate is how fast it will decend. \n",
        "args['eps']=1e-7\n",
        "\n",
        "args['cuda']=False\n",
        "args['log_interval']=10\n"
      ],
      "metadata": {
        "id": "HjbVQTflV0Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the dataset"
      ],
      "metadata": {
        "id": "4BGj2e19LvUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the data, and normalize it\n",
        "\n",
        "mnist_transforms=[\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "     ]\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose(mnist_transforms)),\n",
        "    batch_size=args['batch_size'], shuffle=True, **kwargs)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose(mnist_transforms)),\n",
        "    batch_size=args['batch_size'], shuffle=True, **kwargs)\n"
      ],
      "metadata": {
        "id": "8H2kYLEcWgRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt \n",
        "examples = iter(test_loader)\n",
        "example_data, example_targets = examples.next()\n",
        "for i in range(6):\n",
        "    plt.subplot(2,3,i+1)\n",
        "    plt.imshow(example_data[i][0], cmap='gray')\n",
        "plt.show() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "Df8992EiAGLh",
        "outputId": "f9dee0b3-dbd6-4847-c041-0d077b01f1aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and test function for the models"
      ],
      "metadata": {
        "id": "ShpcsJkzL4Vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    lossFn = nn.CrossEntropyLoss()\n",
        "    loss = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        if args['cuda']:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        \n",
        "        data = data.reshape(-1, 28*28).to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        #This will zero out the gradients for this batch. \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(data)\n",
        "        # Calculate the loss The negative log likelihood loss. It is useful to train a classification problem with C classes.\n",
        "        \n",
        "        # loss = F.nll_loss(output, target)\n",
        "        loss = lossFn(output, target)\n",
        "\n",
        "        #dloss/dx for every Variable \n",
        "        loss.backward()\n",
        "        #to do a one-step update on our parameter.\n",
        "        optimizer.step()\n",
        "        #Print out the loss periodically. \n",
        "\n",
        "    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "            100. * batch_idx / len(train_loader), loss.data))\n",
        "\n",
        "def test():\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        if args['cuda']:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "\n",
        "        data = data.reshape(-1, 28*28).to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        output = model(data)\n",
        "\n",
        "        # test_loss += F.nll_loss(output, target, size_average=False).data\n",
        "        test_loss += lossFn(output, target).data.item() # sum up batch loss\n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "YBRxTCZ0WqKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ReLu network"
      ],
      "metadata": {
        "id": "rvePZnAiWhiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Relu(nn.Module):\n",
        "  def __init__(self, depth, width):\n",
        "    super(Relu, self).__init__()\n",
        "    self.depth = depth\n",
        "    self.width = width\n",
        "    setattr(self,'R1', nn.Linear(input_size, width))\n",
        "    for i in range(depth - 2):\n",
        "        setattr(self,'R{}'.format(i+2), nn.Linear(width, width))\n",
        "\n",
        "    self.output = nn.Linear(width, 10) # activation = \"softmax\", name = \"R\"+str(depth))(R1)\n",
        "\n",
        "  def forward(self, x, **kwargs):\n",
        "    R1 = F.relu(getattr(self,'R1')(x))\n",
        "    for i in range(self.depth-2):\n",
        "      R1 = F.relu(getattr(self,'R{}'.format(i+2))(R1))\n",
        "\n",
        "    return self.output(R1)"
      ],
      "metadata": {
        "id": "QPOeBVriQjg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_categorical(y, num_classes):\n",
        "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
        "    return np.eye(num_classes, dtype='uint8')[y]"
      ],
      "metadata": {
        "id": "GIINdUWBdCDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_relu = {'acc':[], 'val_acc':[], 'loss': [], 'val_loss': []}"
      ],
      "metadata": {
        "id": "aPN47nKkapan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Relu(6, 128)\n",
        "print(model)\n",
        "print()\n",
        "lossFn = nn.CrossEntropyLoss()\n",
        "\n",
        "if args['cuda']:\n",
        "    model.cuda()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
        "\n",
        "for epoch in range(1, args['epochs'] + 1):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__QTzIAOWq6R",
        "outputId": "f41a64a0-c19d-4a21-b038-f44c3f6c9e5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relu(\n",
            "  (R1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (R2): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (R3): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (R4): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (R5): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n",
            "\n",
            "Train Epoch: 1 [59968/60000 (100%)]\tLoss: 0.337397\n",
            "\n",
            "Test set: Average loss: 78.0091, Accuracy: 9250/10000 (92%)\n",
            "\n",
            "Train Epoch: 2 [59968/60000 (100%)]\tLoss: 0.246019\n",
            "\n",
            "Test set: Average loss: 56.8312, Accuracy: 9452/10000 (95%)\n",
            "\n",
            "Train Epoch: 3 [59968/60000 (100%)]\tLoss: 0.102343\n",
            "\n",
            "Test set: Average loss: 42.7277, Accuracy: 9567/10000 (96%)\n",
            "\n",
            "Train Epoch: 4 [59968/60000 (100%)]\tLoss: 0.254051\n",
            "\n",
            "Test set: Average loss: 39.7655, Accuracy: 9599/10000 (96%)\n",
            "\n",
            "Train Epoch: 5 [59968/60000 (100%)]\tLoss: 0.019654\n",
            "\n",
            "Test set: Average loss: 37.9948, Accuracy: 9650/10000 (96%)\n",
            "\n",
            "Train Epoch: 6 [59968/60000 (100%)]\tLoss: 0.045088\n",
            "\n",
            "Test set: Average loss: 33.4831, Accuracy: 9671/10000 (97%)\n",
            "\n",
            "Train Epoch: 7 [59968/60000 (100%)]\tLoss: 0.060721\n",
            "\n",
            "Test set: Average loss: 28.6572, Accuracy: 9726/10000 (97%)\n",
            "\n",
            "Train Epoch: 8 [59968/60000 (100%)]\tLoss: 0.101697\n",
            "\n",
            "Test set: Average loss: 27.2130, Accuracy: 9731/10000 (97%)\n",
            "\n",
            "Train Epoch: 9 [59968/60000 (100%)]\tLoss: 0.118390\n",
            "\n",
            "Test set: Average loss: 28.3885, Accuracy: 9722/10000 (97%)\n",
            "\n",
            "Train Epoch: 10 [59968/60000 (100%)]\tLoss: 0.048708\n",
            "\n",
            "Test set: Average loss: 29.2578, Accuracy: 9730/10000 (97%)\n",
            "\n",
            "Train Epoch: 11 [59968/60000 (100%)]\tLoss: 0.008887\n",
            "\n",
            "Test set: Average loss: 26.2381, Accuracy: 9754/10000 (98%)\n",
            "\n",
            "Train Epoch: 12 [59968/60000 (100%)]\tLoss: 0.012306\n",
            "\n",
            "Test set: Average loss: 30.2085, Accuracy: 9714/10000 (97%)\n",
            "\n",
            "Train Epoch: 13 [59968/60000 (100%)]\tLoss: 0.014508\n",
            "\n",
            "Test set: Average loss: 28.5894, Accuracy: 9757/10000 (98%)\n",
            "\n",
            "Train Epoch: 14 [59968/60000 (100%)]\tLoss: 0.083712\n",
            "\n",
            "Test set: Average loss: 27.4466, Accuracy: 9766/10000 (98%)\n",
            "\n",
            "Train Epoch: 15 [59968/60000 (100%)]\tLoss: 0.109560\n",
            "\n",
            "Test set: Average loss: 29.1198, Accuracy: 9741/10000 (97%)\n",
            "\n",
            "Train Epoch: 16 [59968/60000 (100%)]\tLoss: 0.011304\n",
            "\n",
            "Test set: Average loss: 31.9519, Accuracy: 9746/10000 (97%)\n",
            "\n",
            "Train Epoch: 17 [59968/60000 (100%)]\tLoss: 0.025606\n",
            "\n",
            "Test set: Average loss: 30.7792, Accuracy: 9768/10000 (98%)\n",
            "\n",
            "Train Epoch: 18 [59968/60000 (100%)]\tLoss: 0.076748\n",
            "\n",
            "Test set: Average loss: 34.0895, Accuracy: 9729/10000 (97%)\n",
            "\n",
            "Train Epoch: 19 [59968/60000 (100%)]\tLoss: 0.000129\n",
            "\n",
            "Test set: Average loss: 33.1064, Accuracy: 9758/10000 (98%)\n",
            "\n",
            "Train Epoch: 20 [59968/60000 (100%)]\tLoss: 0.012010\n",
            "\n",
            "Test set: Average loss: 35.9792, Accuracy: 9726/10000 (97%)\n",
            "\n",
            "Train Epoch: 21 [59968/60000 (100%)]\tLoss: 0.025370\n",
            "\n",
            "Test set: Average loss: 35.2547, Accuracy: 9768/10000 (98%)\n",
            "\n",
            "Train Epoch: 22 [59968/60000 (100%)]\tLoss: 0.001637\n",
            "\n",
            "Test set: Average loss: 36.8222, Accuracy: 9751/10000 (98%)\n",
            "\n",
            "Train Epoch: 23 [59968/60000 (100%)]\tLoss: 0.000312\n",
            "\n",
            "Test set: Average loss: 33.4172, Accuracy: 9770/10000 (98%)\n",
            "\n",
            "Train Epoch: 24 [59968/60000 (100%)]\tLoss: 0.000050\n",
            "\n",
            "Test set: Average loss: 39.5081, Accuracy: 9745/10000 (97%)\n",
            "\n",
            "Train Epoch: 25 [59968/60000 (100%)]\tLoss: 0.000839\n",
            "\n",
            "Test set: Average loss: 42.5314, Accuracy: 9738/10000 (97%)\n",
            "\n",
            "Train Epoch: 26 [59968/60000 (100%)]\tLoss: 0.018059\n",
            "\n",
            "Test set: Average loss: 42.7256, Accuracy: 9742/10000 (97%)\n",
            "\n",
            "Train Epoch: 27 [59968/60000 (100%)]\tLoss: 0.205330\n",
            "\n",
            "Test set: Average loss: 39.3862, Accuracy: 9763/10000 (98%)\n",
            "\n",
            "Train Epoch: 28 [59968/60000 (100%)]\tLoss: 0.000086\n",
            "\n",
            "Test set: Average loss: 39.7015, Accuracy: 9737/10000 (97%)\n",
            "\n",
            "Train Epoch: 29 [59968/60000 (100%)]\tLoss: 0.004207\n",
            "\n",
            "Test set: Average loss: 40.7796, Accuracy: 9758/10000 (98%)\n",
            "\n",
            "Train Epoch: 30 [59968/60000 (100%)]\tLoss: 0.000958\n",
            "\n",
            "Test set: Average loss: 56.5133, Accuracy: 9640/10000 (96%)\n",
            "\n",
            "Train Epoch: 31 [59968/60000 (100%)]\tLoss: 0.000462\n",
            "\n",
            "Test set: Average loss: 45.0120, Accuracy: 9750/10000 (98%)\n",
            "\n",
            "Train Epoch: 32 [59968/60000 (100%)]\tLoss: 0.009602\n",
            "\n",
            "Test set: Average loss: 46.9464, Accuracy: 9738/10000 (97%)\n",
            "\n",
            "Train Epoch: 33 [59968/60000 (100%)]\tLoss: 0.002564\n",
            "\n",
            "Test set: Average loss: 43.2164, Accuracy: 9763/10000 (98%)\n",
            "\n",
            "Train Epoch: 34 [59968/60000 (100%)]\tLoss: 0.000191\n",
            "\n",
            "Test set: Average loss: 42.9913, Accuracy: 9767/10000 (98%)\n",
            "\n",
            "Train Epoch: 35 [59968/60000 (100%)]\tLoss: 0.000243\n",
            "\n",
            "Test set: Average loss: 39.1211, Accuracy: 9772/10000 (98%)\n",
            "\n",
            "Train Epoch: 36 [59968/60000 (100%)]\tLoss: 0.000012\n",
            "\n",
            "Test set: Average loss: 43.4491, Accuracy: 9779/10000 (98%)\n",
            "\n",
            "Train Epoch: 37 [59968/60000 (100%)]\tLoss: 0.003374\n",
            "\n",
            "Test set: Average loss: 44.0802, Accuracy: 9763/10000 (98%)\n",
            "\n",
            "Train Epoch: 38 [59968/60000 (100%)]\tLoss: 0.004250\n",
            "\n",
            "Test set: Average loss: 38.8602, Accuracy: 9771/10000 (98%)\n",
            "\n",
            "Train Epoch: 39 [59968/60000 (100%)]\tLoss: 0.000034\n",
            "\n",
            "Test set: Average loss: 41.9225, Accuracy: 9780/10000 (98%)\n",
            "\n",
            "Train Epoch: 40 [59968/60000 (100%)]\tLoss: 0.000301\n",
            "\n",
            "Test set: Average loss: 46.6337, Accuracy: 9739/10000 (97%)\n",
            "\n",
            "Train Epoch: 41 [59968/60000 (100%)]\tLoss: 0.000491\n",
            "\n",
            "Test set: Average loss: 44.6477, Accuracy: 9766/10000 (98%)\n",
            "\n",
            "Train Epoch: 42 [59968/60000 (100%)]\tLoss: 0.000131\n",
            "\n",
            "Test set: Average loss: 46.1386, Accuracy: 9768/10000 (98%)\n",
            "\n",
            "Train Epoch: 43 [59968/60000 (100%)]\tLoss: 0.000106\n",
            "\n",
            "Test set: Average loss: 47.9565, Accuracy: 9757/10000 (98%)\n",
            "\n",
            "Train Epoch: 44 [59968/60000 (100%)]\tLoss: 0.000004\n",
            "\n",
            "Test set: Average loss: 47.2361, Accuracy: 9758/10000 (98%)\n",
            "\n",
            "Train Epoch: 45 [59968/60000 (100%)]\tLoss: 0.000102\n",
            "\n",
            "Test set: Average loss: 44.9639, Accuracy: 9755/10000 (98%)\n",
            "\n",
            "Train Epoch: 46 [59968/60000 (100%)]\tLoss: 0.000004\n",
            "\n",
            "Test set: Average loss: 47.1967, Accuracy: 9763/10000 (98%)\n",
            "\n",
            "Train Epoch: 47 [59968/60000 (100%)]\tLoss: 0.000035\n",
            "\n",
            "Test set: Average loss: 51.0768, Accuracy: 9762/10000 (98%)\n",
            "\n",
            "Train Epoch: 48 [59968/60000 (100%)]\tLoss: 0.000064\n",
            "\n",
            "Test set: Average loss: 45.0365, Accuracy: 9779/10000 (98%)\n",
            "\n",
            "Train Epoch: 49 [59968/60000 (100%)]\tLoss: 0.000006\n",
            "\n",
            "Test set: Average loss: 46.8102, Accuracy: 9756/10000 (98%)\n",
            "\n",
            "Train Epoch: 50 [59968/60000 (100%)]\tLoss: 0.000053\n",
            "\n",
            "Test set: Average loss: 45.0140, Accuracy: 9780/10000 (98%)\n",
            "\n",
            "Train Epoch: 51 [59968/60000 (100%)]\tLoss: 0.000051\n",
            "\n",
            "Test set: Average loss: 51.7670, Accuracy: 9738/10000 (97%)\n",
            "\n",
            "Train Epoch: 52 [59968/60000 (100%)]\tLoss: 0.000045\n",
            "\n",
            "Test set: Average loss: 51.0249, Accuracy: 9756/10000 (98%)\n",
            "\n",
            "Train Epoch: 53 [59968/60000 (100%)]\tLoss: 0.000121\n",
            "\n",
            "Test set: Average loss: 45.6477, Accuracy: 9770/10000 (98%)\n",
            "\n",
            "Train Epoch: 54 [59968/60000 (100%)]\tLoss: 0.000039\n",
            "\n",
            "Test set: Average loss: 50.3667, Accuracy: 9757/10000 (98%)\n",
            "\n",
            "Train Epoch: 55 [59968/60000 (100%)]\tLoss: 0.000012\n",
            "\n",
            "Test set: Average loss: 43.0311, Accuracy: 9783/10000 (98%)\n",
            "\n",
            "Train Epoch: 56 [59968/60000 (100%)]\tLoss: 0.000050\n",
            "\n",
            "Test set: Average loss: 47.0979, Accuracy: 9779/10000 (98%)\n",
            "\n",
            "Train Epoch: 57 [59968/60000 (100%)]\tLoss: 0.000764\n",
            "\n",
            "Test set: Average loss: 43.1988, Accuracy: 9793/10000 (98%)\n",
            "\n",
            "Train Epoch: 58 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 46.0094, Accuracy: 9781/10000 (98%)\n",
            "\n",
            "Train Epoch: 59 [59968/60000 (100%)]\tLoss: 0.000176\n",
            "\n",
            "Test set: Average loss: 43.7621, Accuracy: 9785/10000 (98%)\n",
            "\n",
            "Train Epoch: 60 [59968/60000 (100%)]\tLoss: 0.000003\n",
            "\n",
            "Test set: Average loss: 46.8046, Accuracy: 9776/10000 (98%)\n",
            "\n",
            "Train Epoch: 61 [59968/60000 (100%)]\tLoss: 0.000003\n",
            "\n",
            "Test set: Average loss: 46.4586, Accuracy: 9782/10000 (98%)\n",
            "\n",
            "Train Epoch: 62 [59968/60000 (100%)]\tLoss: 0.000031\n",
            "\n",
            "Test set: Average loss: 46.4340, Accuracy: 9779/10000 (98%)\n",
            "\n",
            "Train Epoch: 63 [59968/60000 (100%)]\tLoss: 0.001142\n",
            "\n",
            "Test set: Average loss: 46.4052, Accuracy: 9790/10000 (98%)\n",
            "\n",
            "Train Epoch: 64 [59968/60000 (100%)]\tLoss: 0.000216\n",
            "\n",
            "Test set: Average loss: 43.3149, Accuracy: 9789/10000 (98%)\n",
            "\n",
            "Train Epoch: 65 [59968/60000 (100%)]\tLoss: 0.172812\n",
            "\n",
            "Test set: Average loss: 53.7039, Accuracy: 9741/10000 (97%)\n",
            "\n",
            "Train Epoch: 66 [59968/60000 (100%)]\tLoss: 0.000733\n",
            "\n",
            "Test set: Average loss: 44.8838, Accuracy: 9781/10000 (98%)\n",
            "\n",
            "Train Epoch: 67 [59968/60000 (100%)]\tLoss: 0.001175\n",
            "\n",
            "Test set: Average loss: 44.4628, Accuracy: 9770/10000 (98%)\n",
            "\n",
            "Train Epoch: 68 [59968/60000 (100%)]\tLoss: 0.000006\n",
            "\n",
            "Test set: Average loss: 43.3865, Accuracy: 9777/10000 (98%)\n",
            "\n",
            "Train Epoch: 69 [59968/60000 (100%)]\tLoss: 0.000214\n",
            "\n",
            "Test set: Average loss: 50.9224, Accuracy: 9754/10000 (98%)\n",
            "\n",
            "Train Epoch: 70 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 44.7331, Accuracy: 9783/10000 (98%)\n",
            "\n",
            "Train Epoch: 71 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 45.4213, Accuracy: 9785/10000 (98%)\n",
            "\n",
            "Train Epoch: 72 [59968/60000 (100%)]\tLoss: 0.000003\n",
            "\n",
            "Test set: Average loss: 43.5832, Accuracy: 9782/10000 (98%)\n",
            "\n",
            "Train Epoch: 73 [59968/60000 (100%)]\tLoss: 0.000203\n",
            "\n",
            "Test set: Average loss: 46.6623, Accuracy: 9793/10000 (98%)\n",
            "\n",
            "Train Epoch: 74 [59968/60000 (100%)]\tLoss: 0.057291\n",
            "\n",
            "Test set: Average loss: 57.0301, Accuracy: 9761/10000 (98%)\n",
            "\n",
            "Train Epoch: 75 [59968/60000 (100%)]\tLoss: 0.000136\n",
            "\n",
            "Test set: Average loss: 44.6696, Accuracy: 9797/10000 (98%)\n",
            "\n",
            "Train Epoch: 76 [59968/60000 (100%)]\tLoss: 0.000370\n",
            "\n",
            "Test set: Average loss: 45.3288, Accuracy: 9796/10000 (98%)\n",
            "\n",
            "Train Epoch: 77 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 49.1422, Accuracy: 9776/10000 (98%)\n",
            "\n",
            "Train Epoch: 78 [59968/60000 (100%)]\tLoss: 0.000020\n",
            "\n",
            "Test set: Average loss: 47.1862, Accuracy: 9769/10000 (98%)\n",
            "\n",
            "Train Epoch: 79 [59968/60000 (100%)]\tLoss: 0.000008\n",
            "\n",
            "Test set: Average loss: 47.1671, Accuracy: 9766/10000 (98%)\n",
            "\n",
            "Train Epoch: 80 [59968/60000 (100%)]\tLoss: 0.000006\n",
            "\n",
            "Test set: Average loss: 52.6365, Accuracy: 9768/10000 (98%)\n",
            "\n",
            "Train Epoch: 81 [59968/60000 (100%)]\tLoss: 0.002133\n",
            "\n",
            "Test set: Average loss: 45.7723, Accuracy: 9786/10000 (98%)\n",
            "\n",
            "Train Epoch: 82 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 52.8512, Accuracy: 9771/10000 (98%)\n",
            "\n",
            "Train Epoch: 83 [59968/60000 (100%)]\tLoss: 0.000010\n",
            "\n",
            "Test set: Average loss: 46.1019, Accuracy: 9794/10000 (98%)\n",
            "\n",
            "Train Epoch: 84 [59968/60000 (100%)]\tLoss: 0.000007\n",
            "\n",
            "Test set: Average loss: 51.4019, Accuracy: 9754/10000 (98%)\n",
            "\n",
            "Train Epoch: 85 [59968/60000 (100%)]\tLoss: 0.000012\n",
            "\n",
            "Test set: Average loss: 51.1521, Accuracy: 9775/10000 (98%)\n",
            "\n",
            "Train Epoch: 86 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 61.5597, Accuracy: 9745/10000 (97%)\n",
            "\n",
            "Train Epoch: 87 [59968/60000 (100%)]\tLoss: 0.000002\n",
            "\n",
            "Test set: Average loss: 49.6143, Accuracy: 9799/10000 (98%)\n",
            "\n",
            "Train Epoch: 88 [59968/60000 (100%)]\tLoss: 0.056039\n",
            "\n",
            "Test set: Average loss: 89.6589, Accuracy: 9637/10000 (96%)\n",
            "\n",
            "Train Epoch: 89 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 53.2301, Accuracy: 9776/10000 (98%)\n",
            "\n",
            "Train Epoch: 90 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 51.0677, Accuracy: 9788/10000 (98%)\n",
            "\n",
            "Train Epoch: 91 [59968/60000 (100%)]\tLoss: 0.000017\n",
            "\n",
            "Test set: Average loss: 50.9719, Accuracy: 9794/10000 (98%)\n",
            "\n",
            "Train Epoch: 92 [59968/60000 (100%)]\tLoss: 0.000021\n",
            "\n",
            "Test set: Average loss: 49.3661, Accuracy: 9797/10000 (98%)\n",
            "\n",
            "Train Epoch: 93 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 57.3739, Accuracy: 9773/10000 (98%)\n",
            "\n",
            "Train Epoch: 94 [59968/60000 (100%)]\tLoss: 0.000030\n",
            "\n",
            "Test set: Average loss: 49.9859, Accuracy: 9784/10000 (98%)\n",
            "\n",
            "Train Epoch: 95 [59968/60000 (100%)]\tLoss: 0.000008\n",
            "\n",
            "Test set: Average loss: 50.1756, Accuracy: 9794/10000 (98%)\n",
            "\n",
            "Train Epoch: 96 [59968/60000 (100%)]\tLoss: 0.000160\n",
            "\n",
            "Test set: Average loss: 48.4730, Accuracy: 9788/10000 (98%)\n",
            "\n",
            "Train Epoch: 97 [59968/60000 (100%)]\tLoss: 0.000037\n",
            "\n",
            "Test set: Average loss: 55.2973, Accuracy: 9763/10000 (98%)\n",
            "\n",
            "Train Epoch: 98 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 66.8305, Accuracy: 9707/10000 (97%)\n",
            "\n",
            "Train Epoch: 99 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 47.1309, Accuracy: 9804/10000 (98%)\n",
            "\n",
            "Train Epoch: 100 [59968/60000 (100%)]\tLoss: 0.000769\n",
            "\n",
            "Test set: Average loss: 54.8321, Accuracy: 9785/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoupled Learning network with soft ReLU"
      ],
      "metadata": {
        "id": "QbR1GwWNMORV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eps, beta = 0.1, 4"
      ],
      "metadata": {
        "id": "47StdQQ9NglY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftGate(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(SoftGate, self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(SoftGate, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def forward(self, x):\n",
        "        activation = (1 + eps)*F.sigmoid(beta*x)\n",
        "        return activation\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "class DecoupledLearning(nn.Module):\n",
        "  def __init__(self, depth, width):\n",
        "\n",
        "    super(DecoupledLearning, self).__init__()\n",
        "\n",
        "    self.depth = depth\n",
        "    self.width = width\n",
        "\n",
        "    setattr(self,'R1', nn.Linear(input_size, width))\n",
        "\n",
        "    setattr(self,'G1', nn.Linear(input_size, width))\n",
        "\n",
        "    for i in range(depth - 2):\n",
        "      setattr(self,'R{}'.format(i+2), nn.Linear(width, width))\n",
        "\n",
        "      setattr(self,'G{}'.format(i+2), nn.Linear(width, width))\n",
        "\n",
        "    self.outputs = nn.Linear(width, 10)\n",
        "      \n",
        "  def forward(self, x):\n",
        "    R1 = getattr(self,'R1')(x)\n",
        "    R1A = F.relu(R1)\n",
        "    A1 = SoftGate()(R1)\n",
        "\n",
        "    G1 = getattr(self,'G1')(x)\n",
        "    G1A = torch.mul(G1, A1)\n",
        "\n",
        "    for i in range(self.depth-2):\n",
        "      R1 = getattr(self,'R{}'.format(i+2))(R1A)\n",
        "      R1A = F.relu(R1)      \n",
        "      A1 = SoftGate()(R1)\n",
        "\n",
        "      G1 = getattr(self,'G{}'.format(i+2))(G1A)\n",
        "      G1A = torch.mul(G1, A1)\n",
        "\n",
        "    return self.outputs(G1A)"
      ],
      "metadata": {
        "id": "ojV2_6toQmEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DecoupledLearning(6, 128)\n",
        "print(model)\n",
        "print()\n",
        "lossFn = nn.CrossEntropyLoss()\n",
        "\n",
        "if args['cuda']:\n",
        "    model.cuda()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
        "\n",
        "for epoch in range(1, args['epochs'] + 1):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tliRdQhyRcvp",
        "outputId": "d0668b83-6a03-4de8-e635-bc6da4a74648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecoupledLearning(\n",
            "  (R1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (G1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (R2): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (G2): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (R3): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (G3): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (R4): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (G4): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (R5): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (G5): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (outputs): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [59968/60000 (100%)]\tLoss: 0.492965\n",
            "\n",
            "Test set: Average loss: 48.6603, Accuracy: 9543/10000 (95%)\n",
            "\n",
            "Train Epoch: 2 [59968/60000 (100%)]\tLoss: 0.067670\n",
            "\n",
            "Test set: Average loss: 34.8555, Accuracy: 9654/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [59968/60000 (100%)]\tLoss: 0.016897\n",
            "\n",
            "Test set: Average loss: 30.4128, Accuracy: 9695/10000 (97%)\n",
            "\n",
            "Train Epoch: 4 [59968/60000 (100%)]\tLoss: 0.019364\n",
            "\n",
            "Test set: Average loss: 28.0258, Accuracy: 9732/10000 (97%)\n",
            "\n",
            "Train Epoch: 5 [59968/60000 (100%)]\tLoss: 0.012244\n",
            "\n",
            "Test set: Average loss: 27.8741, Accuracy: 9731/10000 (97%)\n",
            "\n",
            "Train Epoch: 6 [59968/60000 (100%)]\tLoss: 0.003773\n",
            "\n",
            "Test set: Average loss: 28.9548, Accuracy: 9725/10000 (97%)\n",
            "\n",
            "Train Epoch: 7 [59968/60000 (100%)]\tLoss: 0.091763\n",
            "\n",
            "Test set: Average loss: 28.2029, Accuracy: 9745/10000 (97%)\n",
            "\n",
            "Train Epoch: 8 [59968/60000 (100%)]\tLoss: 0.003674\n",
            "\n",
            "Test set: Average loss: 26.8941, Accuracy: 9769/10000 (98%)\n",
            "\n",
            "Train Epoch: 9 [59968/60000 (100%)]\tLoss: 0.000701\n",
            "\n",
            "Test set: Average loss: 28.0652, Accuracy: 9756/10000 (98%)\n",
            "\n",
            "Train Epoch: 10 [59968/60000 (100%)]\tLoss: 0.000356\n",
            "\n",
            "Test set: Average loss: 30.6056, Accuracy: 9755/10000 (98%)\n",
            "\n",
            "Train Epoch: 11 [59968/60000 (100%)]\tLoss: 0.025469\n",
            "\n",
            "Test set: Average loss: 33.6213, Accuracy: 9756/10000 (98%)\n",
            "\n",
            "Train Epoch: 12 [59968/60000 (100%)]\tLoss: 0.001083\n",
            "\n",
            "Test set: Average loss: 28.8799, Accuracy: 9782/10000 (98%)\n",
            "\n",
            "Train Epoch: 13 [59968/60000 (100%)]\tLoss: 0.052770\n",
            "\n",
            "Test set: Average loss: 34.8603, Accuracy: 9739/10000 (97%)\n",
            "\n",
            "Train Epoch: 14 [59968/60000 (100%)]\tLoss: 0.000610\n",
            "\n",
            "Test set: Average loss: 35.1051, Accuracy: 9745/10000 (97%)\n",
            "\n",
            "Train Epoch: 15 [59968/60000 (100%)]\tLoss: 0.000684\n",
            "\n",
            "Test set: Average loss: 34.6132, Accuracy: 9768/10000 (98%)\n",
            "\n",
            "Train Epoch: 16 [59968/60000 (100%)]\tLoss: 0.000495\n",
            "\n",
            "Test set: Average loss: 38.7882, Accuracy: 9770/10000 (98%)\n",
            "\n",
            "Train Epoch: 17 [59968/60000 (100%)]\tLoss: 0.000051\n",
            "\n",
            "Test set: Average loss: 36.6550, Accuracy: 9786/10000 (98%)\n",
            "\n",
            "Train Epoch: 18 [59968/60000 (100%)]\tLoss: 0.000076\n",
            "\n",
            "Test set: Average loss: 36.8411, Accuracy: 9778/10000 (98%)\n",
            "\n",
            "Train Epoch: 19 [59968/60000 (100%)]\tLoss: 0.000568\n",
            "\n",
            "Test set: Average loss: 33.6986, Accuracy: 9798/10000 (98%)\n",
            "\n",
            "Train Epoch: 20 [59968/60000 (100%)]\tLoss: 0.000088\n",
            "\n",
            "Test set: Average loss: 40.2035, Accuracy: 9760/10000 (98%)\n",
            "\n",
            "Train Epoch: 21 [59968/60000 (100%)]\tLoss: 0.004720\n",
            "\n",
            "Test set: Average loss: 37.9871, Accuracy: 9766/10000 (98%)\n",
            "\n",
            "Train Epoch: 22 [59968/60000 (100%)]\tLoss: 0.008816\n",
            "\n",
            "Test set: Average loss: 32.9392, Accuracy: 9782/10000 (98%)\n",
            "\n",
            "Train Epoch: 23 [59968/60000 (100%)]\tLoss: 0.000009\n",
            "\n",
            "Test set: Average loss: 37.2118, Accuracy: 9772/10000 (98%)\n",
            "\n",
            "Train Epoch: 24 [59968/60000 (100%)]\tLoss: 0.000800\n",
            "\n",
            "Test set: Average loss: 33.7937, Accuracy: 9792/10000 (98%)\n",
            "\n",
            "Train Epoch: 25 [59968/60000 (100%)]\tLoss: 0.000502\n",
            "\n",
            "Test set: Average loss: 43.1400, Accuracy: 9773/10000 (98%)\n",
            "\n",
            "Train Epoch: 26 [59968/60000 (100%)]\tLoss: 0.013000\n",
            "\n",
            "Test set: Average loss: 43.0044, Accuracy: 9751/10000 (98%)\n",
            "\n",
            "Train Epoch: 27 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 36.8632, Accuracy: 9784/10000 (98%)\n",
            "\n",
            "Train Epoch: 28 [59968/60000 (100%)]\tLoss: 0.001203\n",
            "\n",
            "Test set: Average loss: 38.4815, Accuracy: 9790/10000 (98%)\n",
            "\n",
            "Train Epoch: 29 [59968/60000 (100%)]\tLoss: 0.024950\n",
            "\n",
            "Test set: Average loss: 37.5724, Accuracy: 9792/10000 (98%)\n",
            "\n",
            "Train Epoch: 30 [59968/60000 (100%)]\tLoss: 0.000696\n",
            "\n",
            "Test set: Average loss: 36.8385, Accuracy: 9796/10000 (98%)\n",
            "\n",
            "Train Epoch: 31 [59968/60000 (100%)]\tLoss: 0.000415\n",
            "\n",
            "Test set: Average loss: 34.8108, Accuracy: 9805/10000 (98%)\n",
            "\n",
            "Train Epoch: 32 [59968/60000 (100%)]\tLoss: 0.010554\n",
            "\n",
            "Test set: Average loss: 44.3532, Accuracy: 9763/10000 (98%)\n",
            "\n",
            "Train Epoch: 33 [59968/60000 (100%)]\tLoss: 0.000085\n",
            "\n",
            "Test set: Average loss: 36.2988, Accuracy: 9807/10000 (98%)\n",
            "\n",
            "Train Epoch: 34 [59968/60000 (100%)]\tLoss: 0.000003\n",
            "\n",
            "Test set: Average loss: 41.4619, Accuracy: 9790/10000 (98%)\n",
            "\n",
            "Train Epoch: 35 [59968/60000 (100%)]\tLoss: 0.000049\n",
            "\n",
            "Test set: Average loss: 41.9559, Accuracy: 9770/10000 (98%)\n",
            "\n",
            "Train Epoch: 36 [59968/60000 (100%)]\tLoss: 0.000002\n",
            "\n",
            "Test set: Average loss: 40.2522, Accuracy: 9804/10000 (98%)\n",
            "\n",
            "Train Epoch: 37 [59968/60000 (100%)]\tLoss: 0.000017\n",
            "\n",
            "Test set: Average loss: 42.7870, Accuracy: 9778/10000 (98%)\n",
            "\n",
            "Train Epoch: 38 [59968/60000 (100%)]\tLoss: 0.000074\n",
            "\n",
            "Test set: Average loss: 46.1791, Accuracy: 9771/10000 (98%)\n",
            "\n",
            "Train Epoch: 39 [59968/60000 (100%)]\tLoss: 0.000013\n",
            "\n",
            "Test set: Average loss: 42.2633, Accuracy: 9782/10000 (98%)\n",
            "\n",
            "Train Epoch: 40 [59968/60000 (100%)]\tLoss: 0.000028\n",
            "\n",
            "Test set: Average loss: 43.7879, Accuracy: 9791/10000 (98%)\n",
            "\n",
            "Train Epoch: 41 [59968/60000 (100%)]\tLoss: 0.001977\n",
            "\n",
            "Test set: Average loss: 40.5068, Accuracy: 9793/10000 (98%)\n",
            "\n",
            "Train Epoch: 42 [59968/60000 (100%)]\tLoss: 0.000589\n",
            "\n",
            "Test set: Average loss: 45.2593, Accuracy: 9771/10000 (98%)\n",
            "\n",
            "Train Epoch: 43 [59968/60000 (100%)]\tLoss: 0.000161\n",
            "\n",
            "Test set: Average loss: 37.4523, Accuracy: 9793/10000 (98%)\n",
            "\n",
            "Train Epoch: 44 [59968/60000 (100%)]\tLoss: 0.000002\n",
            "\n",
            "Test set: Average loss: 43.9034, Accuracy: 9793/10000 (98%)\n",
            "\n",
            "Train Epoch: 45 [59968/60000 (100%)]\tLoss: 0.000226\n",
            "\n",
            "Test set: Average loss: 38.4248, Accuracy: 9810/10000 (98%)\n",
            "\n",
            "Train Epoch: 46 [59968/60000 (100%)]\tLoss: 0.000042\n",
            "\n",
            "Test set: Average loss: 41.5095, Accuracy: 9803/10000 (98%)\n",
            "\n",
            "Train Epoch: 47 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 42.9735, Accuracy: 9807/10000 (98%)\n",
            "\n",
            "Train Epoch: 48 [59968/60000 (100%)]\tLoss: 0.000051\n",
            "\n",
            "Test set: Average loss: 40.8090, Accuracy: 9799/10000 (98%)\n",
            "\n",
            "Train Epoch: 49 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 39.9305, Accuracy: 9815/10000 (98%)\n",
            "\n",
            "Train Epoch: 50 [59968/60000 (100%)]\tLoss: 0.000004\n",
            "\n",
            "Test set: Average loss: 41.7344, Accuracy: 9792/10000 (98%)\n",
            "\n",
            "Train Epoch: 51 [59968/60000 (100%)]\tLoss: 0.000809\n",
            "\n",
            "Test set: Average loss: 39.3964, Accuracy: 9809/10000 (98%)\n",
            "\n",
            "Train Epoch: 52 [59968/60000 (100%)]\tLoss: 0.000005\n",
            "\n",
            "Test set: Average loss: 42.9312, Accuracy: 9796/10000 (98%)\n",
            "\n",
            "Train Epoch: 53 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 43.7099, Accuracy: 9787/10000 (98%)\n",
            "\n",
            "Train Epoch: 54 [59968/60000 (100%)]\tLoss: 0.000010\n",
            "\n",
            "Test set: Average loss: 38.0713, Accuracy: 9818/10000 (98%)\n",
            "\n",
            "Train Epoch: 55 [59968/60000 (100%)]\tLoss: 0.000053\n",
            "\n",
            "Test set: Average loss: 36.4153, Accuracy: 9810/10000 (98%)\n",
            "\n",
            "Train Epoch: 56 [59968/60000 (100%)]\tLoss: 0.000322\n",
            "\n",
            "Test set: Average loss: 36.3828, Accuracy: 9811/10000 (98%)\n",
            "\n",
            "Train Epoch: 57 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 46.5934, Accuracy: 9776/10000 (98%)\n",
            "\n",
            "Train Epoch: 58 [59968/60000 (100%)]\tLoss: 0.001782\n",
            "\n",
            "Test set: Average loss: 44.5010, Accuracy: 9767/10000 (98%)\n",
            "\n",
            "Train Epoch: 59 [59968/60000 (100%)]\tLoss: 0.012921\n",
            "\n",
            "Test set: Average loss: 38.8557, Accuracy: 9802/10000 (98%)\n",
            "\n",
            "Train Epoch: 60 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 42.6391, Accuracy: 9793/10000 (98%)\n",
            "\n",
            "Train Epoch: 61 [59968/60000 (100%)]\tLoss: 0.002396\n",
            "\n",
            "Test set: Average loss: 40.2115, Accuracy: 9805/10000 (98%)\n",
            "\n",
            "Train Epoch: 62 [59968/60000 (100%)]\tLoss: 0.000159\n",
            "\n",
            "Test set: Average loss: 40.4644, Accuracy: 9797/10000 (98%)\n",
            "\n",
            "Train Epoch: 63 [59968/60000 (100%)]\tLoss: 0.005604\n",
            "\n",
            "Test set: Average loss: 47.4042, Accuracy: 9772/10000 (98%)\n",
            "\n",
            "Train Epoch: 64 [59968/60000 (100%)]\tLoss: 0.000014\n",
            "\n",
            "Test set: Average loss: 41.6064, Accuracy: 9789/10000 (98%)\n",
            "\n",
            "Train Epoch: 65 [59968/60000 (100%)]\tLoss: 0.000002\n",
            "\n",
            "Test set: Average loss: 43.4574, Accuracy: 9794/10000 (98%)\n",
            "\n",
            "Train Epoch: 66 [59968/60000 (100%)]\tLoss: 0.000019\n",
            "\n",
            "Test set: Average loss: 49.1669, Accuracy: 9785/10000 (98%)\n",
            "\n",
            "Train Epoch: 67 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 49.4389, Accuracy: 9764/10000 (98%)\n",
            "\n",
            "Train Epoch: 68 [59968/60000 (100%)]\tLoss: 0.004071\n",
            "\n",
            "Test set: Average loss: 43.6049, Accuracy: 9810/10000 (98%)\n",
            "\n",
            "Train Epoch: 69 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 41.6836, Accuracy: 9799/10000 (98%)\n",
            "\n",
            "Train Epoch: 70 [59968/60000 (100%)]\tLoss: 0.000152\n",
            "\n",
            "Test set: Average loss: 42.8156, Accuracy: 9800/10000 (98%)\n",
            "\n",
            "Train Epoch: 71 [59968/60000 (100%)]\tLoss: 0.188118\n",
            "\n",
            "Test set: Average loss: 49.1790, Accuracy: 9785/10000 (98%)\n",
            "\n",
            "Train Epoch: 72 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 54.4650, Accuracy: 9779/10000 (98%)\n",
            "\n",
            "Train Epoch: 73 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 45.1838, Accuracy: 9783/10000 (98%)\n",
            "\n",
            "Train Epoch: 74 [59968/60000 (100%)]\tLoss: 0.000011\n",
            "\n",
            "Test set: Average loss: 44.3105, Accuracy: 9808/10000 (98%)\n",
            "\n",
            "Train Epoch: 75 [59968/60000 (100%)]\tLoss: 0.008436\n",
            "\n",
            "Test set: Average loss: 44.0741, Accuracy: 9794/10000 (98%)\n",
            "\n",
            "Train Epoch: 76 [59968/60000 (100%)]\tLoss: 0.000009\n",
            "\n",
            "Test set: Average loss: 46.2107, Accuracy: 9788/10000 (98%)\n",
            "\n",
            "Train Epoch: 77 [59968/60000 (100%)]\tLoss: 0.000165\n",
            "\n",
            "Test set: Average loss: 44.7015, Accuracy: 9792/10000 (98%)\n",
            "\n",
            "Train Epoch: 78 [59968/60000 (100%)]\tLoss: 0.002407\n",
            "\n",
            "Test set: Average loss: 47.9121, Accuracy: 9782/10000 (98%)\n",
            "\n",
            "Train Epoch: 79 [59968/60000 (100%)]\tLoss: 0.000078\n",
            "\n",
            "Test set: Average loss: 44.3269, Accuracy: 9800/10000 (98%)\n",
            "\n",
            "Train Epoch: 80 [59968/60000 (100%)]\tLoss: 0.000003\n",
            "\n",
            "Test set: Average loss: 47.5491, Accuracy: 9802/10000 (98%)\n",
            "\n",
            "Train Epoch: 81 [59968/60000 (100%)]\tLoss: 0.000103\n",
            "\n",
            "Test set: Average loss: 42.5565, Accuracy: 9815/10000 (98%)\n",
            "\n",
            "Train Epoch: 82 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 43.9749, Accuracy: 9803/10000 (98%)\n",
            "\n",
            "Train Epoch: 83 [59968/60000 (100%)]\tLoss: 0.000019\n",
            "\n",
            "Test set: Average loss: 45.9244, Accuracy: 9803/10000 (98%)\n",
            "\n",
            "Train Epoch: 84 [59968/60000 (100%)]\tLoss: 0.000056\n",
            "\n",
            "Test set: Average loss: 42.8129, Accuracy: 9787/10000 (98%)\n",
            "\n",
            "Train Epoch: 85 [59968/60000 (100%)]\tLoss: 0.000100\n",
            "\n",
            "Test set: Average loss: 50.0576, Accuracy: 9781/10000 (98%)\n",
            "\n",
            "Train Epoch: 86 [59968/60000 (100%)]\tLoss: 0.011302\n",
            "\n",
            "Test set: Average loss: 39.8581, Accuracy: 9818/10000 (98%)\n",
            "\n",
            "Train Epoch: 87 [59968/60000 (100%)]\tLoss: 0.000029\n",
            "\n",
            "Test set: Average loss: 47.3926, Accuracy: 9763/10000 (98%)\n",
            "\n",
            "Train Epoch: 88 [59968/60000 (100%)]\tLoss: 0.000049\n",
            "\n",
            "Test set: Average loss: 43.7003, Accuracy: 9789/10000 (98%)\n",
            "\n",
            "Train Epoch: 89 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 42.4235, Accuracy: 9804/10000 (98%)\n",
            "\n",
            "Train Epoch: 90 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 44.4059, Accuracy: 9793/10000 (98%)\n",
            "\n",
            "Train Epoch: 91 [59968/60000 (100%)]\tLoss: 0.000019\n",
            "\n",
            "Test set: Average loss: 43.0859, Accuracy: 9801/10000 (98%)\n",
            "\n",
            "Train Epoch: 92 [59968/60000 (100%)]\tLoss: 0.000026\n",
            "\n",
            "Test set: Average loss: 48.8206, Accuracy: 9794/10000 (98%)\n",
            "\n",
            "Train Epoch: 93 [59968/60000 (100%)]\tLoss: 0.000005\n",
            "\n",
            "Test set: Average loss: 49.2591, Accuracy: 9770/10000 (98%)\n",
            "\n",
            "Train Epoch: 94 [59968/60000 (100%)]\tLoss: 0.000170\n",
            "\n",
            "Test set: Average loss: 52.7824, Accuracy: 9783/10000 (98%)\n",
            "\n",
            "Train Epoch: 95 [59968/60000 (100%)]\tLoss: 0.000024\n",
            "\n",
            "Test set: Average loss: 45.7457, Accuracy: 9790/10000 (98%)\n",
            "\n",
            "Train Epoch: 96 [59968/60000 (100%)]\tLoss: 0.000006\n",
            "\n",
            "Test set: Average loss: 62.0165, Accuracy: 9761/10000 (98%)\n",
            "\n",
            "Train Epoch: 97 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 48.1390, Accuracy: 9784/10000 (98%)\n",
            "\n",
            "Train Epoch: 98 [59968/60000 (100%)]\tLoss: 0.000074\n",
            "\n",
            "Test set: Average loss: 41.7188, Accuracy: 9814/10000 (98%)\n",
            "\n",
            "Train Epoch: 99 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 38.2168, Accuracy: 9825/10000 (98%)\n",
            "\n",
            "Train Epoch: 100 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 39.9065, Accuracy: 9813/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoupled learning with hard ReLU activation"
      ],
      "metadata": {
        "id": "Az0TgEDQ-mPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SignGate(nn.Module):\n",
        "  def __init__(self, **kwargs):\n",
        "      super(SignGate, self).__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "      super(SignGate, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "  def forward(self, x):\n",
        "      output = torch.sign(F.relu(x))\n",
        "      return output\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "      return input_shape\n",
        "      \n",
        "  \n",
        "class Galu(nn.Module):\n",
        "\n",
        "  def __init__(self, depth, width):\n",
        "    super(Galu, self).__init__()\n",
        "\n",
        "    self.depth = depth\n",
        "    self.width = width\n",
        "\n",
        "    setattr(self,'R1', nn.Linear(input_size, width))\n",
        "\n",
        "    setattr(self,'G1', nn.Linear(input_size, width))\n",
        "\n",
        "    for i in range(depth - 2):\n",
        "      setattr(self,'R{}'.format(i+2), nn.Linear(width, width))\n",
        "\n",
        "      setattr(self,'G{}'.format(i+2), nn.Linear(width, width))\n",
        "\n",
        "    self.outputs = nn.Linear(width, 10)\n",
        "\n",
        "  def freezeLayerR(self):\n",
        "    for i in range(1,6):\n",
        "      getattr(self, 'R{}'.format(i)).requires_grad = False\n",
        "      \n",
        "  def forward(self, x):\n",
        "    R1 = F.relu(getattr(self,'R1')(x))\n",
        "    A1 = SignGate()(R1)\n",
        "\n",
        "    G1 = getattr(self,'G1')(x)\n",
        "    G1A = torch.mul(G1, A1)\n",
        "\n",
        "    for i in range(self.depth-2):\n",
        "      R1 = F.relu(getattr(self,'R{}'.format(i+2))(R1))      \n",
        "      A1 = SignGate()(R1)\n",
        "\n",
        "      G1 = getattr(self,'G{}'.format(i+2))(G1A)\n",
        "      G1A = torch.mul(G1, A1)\n",
        "\n",
        "    return self.outputs(G1A)\n"
      ],
      "metadata": {
        "id": "Jo9-D2F6chWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Galu(6, 128)\n",
        "print(model)\n",
        "print()\n",
        "lossFn = nn.CrossEntropyLoss()\n",
        "\n",
        "if args['cuda']:\n",
        "    model.cuda()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
        "\n",
        "for epoch in range(1, args['epochs'] + 1):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm_sbXRheWNq",
        "outputId": "e6174c4d-8eae-443a-9b2d-17ff54492e93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Galu(\n",
            "  (R1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (G1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (R2): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (G2): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (R3): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (G3): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (R4): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (G4): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (R5): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (G5): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (outputs): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n",
            "\n",
            "Train Epoch: 1 [59968/60000 (100%)]\tLoss: 0.336282\n",
            "\n",
            "Test set: Average loss: 109.5237, Accuracy: 8953/10000 (90%)\n",
            "\n",
            "Train Epoch: 2 [59968/60000 (100%)]\tLoss: 0.109688\n",
            "\n",
            "Test set: Average loss: 74.3465, Accuracy: 9266/10000 (93%)\n",
            "\n",
            "Train Epoch: 3 [59968/60000 (100%)]\tLoss: 0.263122\n",
            "\n",
            "Test set: Average loss: 61.4839, Accuracy: 9414/10000 (94%)\n",
            "\n",
            "Train Epoch: 4 [59968/60000 (100%)]\tLoss: 0.088312\n",
            "\n",
            "Test set: Average loss: 54.5181, Accuracy: 9471/10000 (95%)\n",
            "\n",
            "Train Epoch: 5 [59968/60000 (100%)]\tLoss: 0.045087\n",
            "\n",
            "Test set: Average loss: 51.7214, Accuracy: 9510/10000 (95%)\n",
            "\n",
            "Train Epoch: 6 [59968/60000 (100%)]\tLoss: 0.104333\n",
            "\n",
            "Test set: Average loss: 47.3336, Accuracy: 9547/10000 (95%)\n",
            "\n",
            "Train Epoch: 7 [59968/60000 (100%)]\tLoss: 0.210779\n",
            "\n",
            "Test set: Average loss: 46.6637, Accuracy: 9567/10000 (96%)\n",
            "\n",
            "Train Epoch: 8 [59968/60000 (100%)]\tLoss: 0.045957\n",
            "\n",
            "Test set: Average loss: 46.3798, Accuracy: 9565/10000 (96%)\n",
            "\n",
            "Train Epoch: 9 [59968/60000 (100%)]\tLoss: 0.104662\n",
            "\n",
            "Test set: Average loss: 47.5607, Accuracy: 9592/10000 (96%)\n",
            "\n",
            "Train Epoch: 10 [59968/60000 (100%)]\tLoss: 0.042058\n",
            "\n",
            "Test set: Average loss: 50.9023, Accuracy: 9568/10000 (96%)\n",
            "\n",
            "Train Epoch: 11 [59968/60000 (100%)]\tLoss: 0.005104\n",
            "\n",
            "Test set: Average loss: 51.7503, Accuracy: 9578/10000 (96%)\n",
            "\n",
            "Train Epoch: 12 [59968/60000 (100%)]\tLoss: 0.082654\n",
            "\n",
            "Test set: Average loss: 55.4600, Accuracy: 9586/10000 (96%)\n",
            "\n",
            "Train Epoch: 13 [59968/60000 (100%)]\tLoss: 0.119040\n",
            "\n",
            "Test set: Average loss: 59.2245, Accuracy: 9593/10000 (96%)\n",
            "\n",
            "Train Epoch: 14 [59968/60000 (100%)]\tLoss: 0.004735\n",
            "\n",
            "Test set: Average loss: 65.8120, Accuracy: 9572/10000 (96%)\n",
            "\n",
            "Train Epoch: 15 [59968/60000 (100%)]\tLoss: 0.005252\n",
            "\n",
            "Test set: Average loss: 70.1367, Accuracy: 9571/10000 (96%)\n",
            "\n",
            "Train Epoch: 16 [59968/60000 (100%)]\tLoss: 0.001299\n",
            "\n",
            "Test set: Average loss: 73.5413, Accuracy: 9577/10000 (96%)\n",
            "\n",
            "Train Epoch: 17 [59968/60000 (100%)]\tLoss: 0.002037\n",
            "\n",
            "Test set: Average loss: 79.6006, Accuracy: 9573/10000 (96%)\n",
            "\n",
            "Train Epoch: 18 [59968/60000 (100%)]\tLoss: 0.003600\n",
            "\n",
            "Test set: Average loss: 85.2431, Accuracy: 9576/10000 (96%)\n",
            "\n",
            "Train Epoch: 19 [59968/60000 (100%)]\tLoss: 0.004882\n",
            "\n",
            "Test set: Average loss: 87.6406, Accuracy: 9569/10000 (96%)\n",
            "\n",
            "Train Epoch: 20 [59968/60000 (100%)]\tLoss: 0.000081\n",
            "\n",
            "Test set: Average loss: 95.0368, Accuracy: 9568/10000 (96%)\n",
            "\n",
            "Train Epoch: 21 [59968/60000 (100%)]\tLoss: 0.000003\n",
            "\n",
            "Test set: Average loss: 101.3276, Accuracy: 9555/10000 (96%)\n",
            "\n",
            "Train Epoch: 22 [59968/60000 (100%)]\tLoss: 0.000125\n",
            "\n",
            "Test set: Average loss: 101.1615, Accuracy: 9563/10000 (96%)\n",
            "\n",
            "Train Epoch: 23 [59968/60000 (100%)]\tLoss: 0.000168\n",
            "\n",
            "Test set: Average loss: 105.0585, Accuracy: 9582/10000 (96%)\n",
            "\n",
            "Train Epoch: 24 [59968/60000 (100%)]\tLoss: 0.039164\n",
            "\n",
            "Test set: Average loss: 122.0137, Accuracy: 9522/10000 (95%)\n",
            "\n",
            "Train Epoch: 25 [59968/60000 (100%)]\tLoss: 0.000797\n",
            "\n",
            "Test set: Average loss: 110.9791, Accuracy: 9568/10000 (96%)\n",
            "\n",
            "Train Epoch: 26 [59968/60000 (100%)]\tLoss: 0.000037\n",
            "\n",
            "Test set: Average loss: 118.2475, Accuracy: 9563/10000 (96%)\n",
            "\n",
            "Train Epoch: 27 [59968/60000 (100%)]\tLoss: 0.000180\n",
            "\n",
            "Test set: Average loss: 119.1602, Accuracy: 9560/10000 (96%)\n",
            "\n",
            "Train Epoch: 28 [59968/60000 (100%)]\tLoss: 0.000467\n",
            "\n",
            "Test set: Average loss: 114.1835, Accuracy: 9581/10000 (96%)\n",
            "\n",
            "Train Epoch: 29 [59968/60000 (100%)]\tLoss: 0.000054\n",
            "\n",
            "Test set: Average loss: 117.5410, Accuracy: 9587/10000 (96%)\n",
            "\n",
            "Train Epoch: 30 [59968/60000 (100%)]\tLoss: 0.000034\n",
            "\n",
            "Test set: Average loss: 145.3412, Accuracy: 9509/10000 (95%)\n",
            "\n",
            "Train Epoch: 31 [59968/60000 (100%)]\tLoss: 0.000045\n",
            "\n",
            "Test set: Average loss: 135.1157, Accuracy: 9573/10000 (96%)\n",
            "\n",
            "Train Epoch: 32 [59968/60000 (100%)]\tLoss: 0.000021\n",
            "\n",
            "Test set: Average loss: 126.4639, Accuracy: 9583/10000 (96%)\n",
            "\n",
            "Train Epoch: 33 [59968/60000 (100%)]\tLoss: 0.073453\n",
            "\n",
            "Test set: Average loss: 143.9493, Accuracy: 9537/10000 (95%)\n",
            "\n",
            "Train Epoch: 34 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 132.0763, Accuracy: 9572/10000 (96%)\n",
            "\n",
            "Train Epoch: 35 [59968/60000 (100%)]\tLoss: 0.013558\n",
            "\n",
            "Test set: Average loss: 130.2727, Accuracy: 9575/10000 (96%)\n",
            "\n",
            "Train Epoch: 36 [59968/60000 (100%)]\tLoss: 0.002581\n",
            "\n",
            "Test set: Average loss: 138.3691, Accuracy: 9558/10000 (96%)\n",
            "\n",
            "Train Epoch: 37 [59968/60000 (100%)]\tLoss: 0.000715\n",
            "\n",
            "Test set: Average loss: 136.7419, Accuracy: 9547/10000 (95%)\n",
            "\n",
            "Train Epoch: 38 [59968/60000 (100%)]\tLoss: 0.000077\n",
            "\n",
            "Test set: Average loss: 131.8337, Accuracy: 9565/10000 (96%)\n",
            "\n",
            "Train Epoch: 39 [59968/60000 (100%)]\tLoss: 0.000058\n",
            "\n",
            "Test set: Average loss: 139.2105, Accuracy: 9573/10000 (96%)\n",
            "\n",
            "Train Epoch: 40 [59968/60000 (100%)]\tLoss: 0.038649\n",
            "\n",
            "Test set: Average loss: 139.4857, Accuracy: 9576/10000 (96%)\n",
            "\n",
            "Train Epoch: 41 [59968/60000 (100%)]\tLoss: 0.000830\n",
            "\n",
            "Test set: Average loss: 140.1498, Accuracy: 9564/10000 (96%)\n",
            "\n",
            "Train Epoch: 42 [59968/60000 (100%)]\tLoss: 0.000610\n",
            "\n",
            "Test set: Average loss: 139.6360, Accuracy: 9576/10000 (96%)\n",
            "\n",
            "Train Epoch: 43 [59968/60000 (100%)]\tLoss: 0.000100\n",
            "\n",
            "Test set: Average loss: 146.6437, Accuracy: 9565/10000 (96%)\n",
            "\n",
            "Train Epoch: 44 [59968/60000 (100%)]\tLoss: 0.000318\n",
            "\n",
            "Test set: Average loss: 144.1397, Accuracy: 9547/10000 (95%)\n",
            "\n",
            "Train Epoch: 45 [59968/60000 (100%)]\tLoss: 0.000033\n",
            "\n",
            "Test set: Average loss: 144.0821, Accuracy: 9575/10000 (96%)\n",
            "\n",
            "Train Epoch: 46 [59968/60000 (100%)]\tLoss: 0.000033\n",
            "\n",
            "Test set: Average loss: 141.4512, Accuracy: 9587/10000 (96%)\n",
            "\n",
            "Train Epoch: 47 [59968/60000 (100%)]\tLoss: 0.000173\n",
            "\n",
            "Test set: Average loss: 152.1199, Accuracy: 9553/10000 (96%)\n",
            "\n",
            "Train Epoch: 48 [59968/60000 (100%)]\tLoss: 0.000101\n",
            "\n",
            "Test set: Average loss: 147.3669, Accuracy: 9587/10000 (96%)\n",
            "\n",
            "Train Epoch: 49 [59968/60000 (100%)]\tLoss: 0.000252\n",
            "\n",
            "Test set: Average loss: 144.5213, Accuracy: 9584/10000 (96%)\n",
            "\n",
            "Train Epoch: 50 [59968/60000 (100%)]\tLoss: 0.001327\n",
            "\n",
            "Test set: Average loss: 150.6866, Accuracy: 9567/10000 (96%)\n",
            "\n",
            "Train Epoch: 51 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 143.5999, Accuracy: 9572/10000 (96%)\n",
            "\n",
            "Train Epoch: 52 [59968/60000 (100%)]\tLoss: 0.000045\n",
            "\n",
            "Test set: Average loss: 155.0595, Accuracy: 9567/10000 (96%)\n",
            "\n",
            "Train Epoch: 53 [59968/60000 (100%)]\tLoss: 0.000090\n",
            "\n",
            "Test set: Average loss: 150.6054, Accuracy: 9567/10000 (96%)\n",
            "\n",
            "Train Epoch: 54 [59968/60000 (100%)]\tLoss: 0.000665\n",
            "\n",
            "Test set: Average loss: 161.6560, Accuracy: 9542/10000 (95%)\n",
            "\n",
            "Train Epoch: 55 [59968/60000 (100%)]\tLoss: 0.000274\n",
            "\n",
            "Test set: Average loss: 154.9580, Accuracy: 9568/10000 (96%)\n",
            "\n",
            "Train Epoch: 56 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 155.2987, Accuracy: 9560/10000 (96%)\n",
            "\n",
            "Train Epoch: 57 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 147.1522, Accuracy: 9579/10000 (96%)\n",
            "\n",
            "Train Epoch: 58 [59968/60000 (100%)]\tLoss: 0.000587\n",
            "\n",
            "Test set: Average loss: 162.9555, Accuracy: 9550/10000 (96%)\n",
            "\n",
            "Train Epoch: 59 [59968/60000 (100%)]\tLoss: 0.000013\n",
            "\n",
            "Test set: Average loss: 157.9427, Accuracy: 9571/10000 (96%)\n",
            "\n",
            "Train Epoch: 60 [59968/60000 (100%)]\tLoss: 0.000086\n",
            "\n",
            "Test set: Average loss: 151.9131, Accuracy: 9596/10000 (96%)\n",
            "\n",
            "Train Epoch: 61 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 163.8711, Accuracy: 9581/10000 (96%)\n",
            "\n",
            "Train Epoch: 62 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 151.2803, Accuracy: 9594/10000 (96%)\n",
            "\n",
            "Train Epoch: 63 [59968/60000 (100%)]\tLoss: 0.003717\n",
            "\n",
            "Test set: Average loss: 164.4241, Accuracy: 9555/10000 (96%)\n",
            "\n",
            "Train Epoch: 64 [59968/60000 (100%)]\tLoss: 0.000013\n",
            "\n",
            "Test set: Average loss: 155.0854, Accuracy: 9584/10000 (96%)\n",
            "\n",
            "Train Epoch: 65 [59968/60000 (100%)]\tLoss: 0.000004\n",
            "\n",
            "Test set: Average loss: 153.6265, Accuracy: 9595/10000 (96%)\n",
            "\n",
            "Train Epoch: 66 [59968/60000 (100%)]\tLoss: 0.000004\n",
            "\n",
            "Test set: Average loss: 155.0858, Accuracy: 9582/10000 (96%)\n",
            "\n",
            "Train Epoch: 67 [59968/60000 (100%)]\tLoss: 0.002052\n",
            "\n",
            "Test set: Average loss: 177.3800, Accuracy: 9557/10000 (96%)\n",
            "\n",
            "Train Epoch: 68 [59968/60000 (100%)]\tLoss: 0.000010\n",
            "\n",
            "Test set: Average loss: 173.2858, Accuracy: 9556/10000 (96%)\n",
            "\n",
            "Train Epoch: 69 [59968/60000 (100%)]\tLoss: 0.000236\n",
            "\n",
            "Test set: Average loss: 167.8627, Accuracy: 9568/10000 (96%)\n",
            "\n",
            "Train Epoch: 70 [59968/60000 (100%)]\tLoss: 0.000003\n",
            "\n",
            "Test set: Average loss: 160.5976, Accuracy: 9564/10000 (96%)\n",
            "\n",
            "Train Epoch: 71 [59968/60000 (100%)]\tLoss: 0.000480\n",
            "\n",
            "Test set: Average loss: 164.3410, Accuracy: 9597/10000 (96%)\n",
            "\n",
            "Train Epoch: 72 [59968/60000 (100%)]\tLoss: 0.001491\n",
            "\n",
            "Test set: Average loss: 173.4995, Accuracy: 9566/10000 (96%)\n",
            "\n",
            "Train Epoch: 73 [59968/60000 (100%)]\tLoss: 0.001878\n",
            "\n",
            "Test set: Average loss: 167.3827, Accuracy: 9578/10000 (96%)\n",
            "\n",
            "Train Epoch: 74 [59968/60000 (100%)]\tLoss: 0.002215\n",
            "\n",
            "Test set: Average loss: 165.6025, Accuracy: 9596/10000 (96%)\n",
            "\n",
            "Train Epoch: 75 [59968/60000 (100%)]\tLoss: 0.000014\n",
            "\n",
            "Test set: Average loss: 165.4439, Accuracy: 9594/10000 (96%)\n",
            "\n",
            "Train Epoch: 76 [59968/60000 (100%)]\tLoss: 0.007752\n",
            "\n",
            "Test set: Average loss: 174.3397, Accuracy: 9568/10000 (96%)\n",
            "\n",
            "Train Epoch: 77 [59968/60000 (100%)]\tLoss: 0.000005\n",
            "\n",
            "Test set: Average loss: 165.8405, Accuracy: 9592/10000 (96%)\n",
            "\n",
            "Train Epoch: 78 [59968/60000 (100%)]\tLoss: 0.000004\n",
            "\n",
            "Test set: Average loss: 165.0429, Accuracy: 9594/10000 (96%)\n",
            "\n",
            "Train Epoch: 79 [59968/60000 (100%)]\tLoss: 0.000006\n",
            "\n",
            "Test set: Average loss: 165.9562, Accuracy: 9598/10000 (96%)\n",
            "\n",
            "Train Epoch: 80 [59968/60000 (100%)]\tLoss: 0.000029\n",
            "\n",
            "Test set: Average loss: 167.0175, Accuracy: 9598/10000 (96%)\n",
            "\n",
            "Train Epoch: 81 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 171.2021, Accuracy: 9602/10000 (96%)\n",
            "\n",
            "Train Epoch: 82 [59968/60000 (100%)]\tLoss: 0.000027\n",
            "\n",
            "Test set: Average loss: 175.1729, Accuracy: 9599/10000 (96%)\n",
            "\n",
            "Train Epoch: 83 [59968/60000 (100%)]\tLoss: 0.000002\n",
            "\n",
            "Test set: Average loss: 180.0591, Accuracy: 9583/10000 (96%)\n",
            "\n",
            "Train Epoch: 84 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 178.3661, Accuracy: 9582/10000 (96%)\n",
            "\n",
            "Train Epoch: 85 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 183.4110, Accuracy: 9588/10000 (96%)\n",
            "\n",
            "Train Epoch: 86 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 180.5817, Accuracy: 9593/10000 (96%)\n",
            "\n",
            "Train Epoch: 87 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 202.5747, Accuracy: 9558/10000 (96%)\n",
            "\n",
            "Train Epoch: 88 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 183.4391, Accuracy: 9592/10000 (96%)\n",
            "\n",
            "Train Epoch: 89 [59968/60000 (100%)]\tLoss: 0.000003\n",
            "\n",
            "Test set: Average loss: 183.2217, Accuracy: 9563/10000 (96%)\n",
            "\n",
            "Train Epoch: 90 [59968/60000 (100%)]\tLoss: 0.000002\n",
            "\n",
            "Test set: Average loss: 180.5977, Accuracy: 9580/10000 (96%)\n",
            "\n",
            "Train Epoch: 91 [59968/60000 (100%)]\tLoss: 0.000004\n",
            "\n",
            "Test set: Average loss: 187.5756, Accuracy: 9567/10000 (96%)\n",
            "\n",
            "Train Epoch: 92 [59968/60000 (100%)]\tLoss: 0.000175\n",
            "\n",
            "Test set: Average loss: 182.6451, Accuracy: 9565/10000 (96%)\n",
            "\n",
            "Train Epoch: 93 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 182.2712, Accuracy: 9581/10000 (96%)\n",
            "\n",
            "Train Epoch: 94 [59968/60000 (100%)]\tLoss: 0.000005\n",
            "\n",
            "Test set: Average loss: 186.2385, Accuracy: 9579/10000 (96%)\n",
            "\n",
            "Train Epoch: 95 [59968/60000 (100%)]\tLoss: 0.000133\n",
            "\n",
            "Test set: Average loss: 178.2180, Accuracy: 9577/10000 (96%)\n",
            "\n",
            "Train Epoch: 96 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 176.8156, Accuracy: 9582/10000 (96%)\n",
            "\n",
            "Train Epoch: 97 [59968/60000 (100%)]\tLoss: 0.000003\n",
            "\n",
            "Test set: Average loss: 186.4648, Accuracy: 9572/10000 (96%)\n",
            "\n",
            "Train Epoch: 98 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 183.5804, Accuracy: 9578/10000 (96%)\n",
            "\n",
            "Train Epoch: 99 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 180.8443, Accuracy: 9580/10000 (96%)\n",
            "\n",
            "Train Epoch: 100 [59968/60000 (100%)]\tLoss: 0.000019\n",
            "\n",
            "Test set: Average loss: 200.7365, Accuracy: 9555/10000 (96%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FRNPF(II)"
      ],
      "metadata": {
        "id": "w5sisU34cidh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Galu(6, 128)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uh7wYOsLfEAQ",
        "outputId": "6f4e6df8-35c2-49d4-d374-6b185e9924b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Galu(\n",
              "  (R1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (G1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (R2): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (G2): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (R3): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (G3): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (R4): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (G4): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (R5): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (G5): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (outputs): Linear(in_features=128, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if name[0]=='R':\n",
        "        param.requires_grad = False"
      ],
      "metadata": {
        "id": "gu4skZJ3sTn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "      print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6S9iFdYesURF",
        "outputId": "e46246fd-f61e-49d2-d962-4c9c5dcb3c82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G1.weight\n",
            "G1.bias\n",
            "G2.weight\n",
            "G2.bias\n",
            "G3.weight\n",
            "G3.bias\n",
            "G4.weight\n",
            "G4.bias\n",
            "G5.weight\n",
            "G5.bias\n",
            "outputs.weight\n",
            "outputs.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lossFn = nn.CrossEntropyLoss()\n",
        "\n",
        "if args['cuda']:\n",
        "    model.cuda()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
        "\n",
        "for epoch in range(1, args['epochs'] + 1):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viKltmoMuML-",
        "outputId": "16e0e3ff-0eb5-4312-e964-fb799ea54647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [59968/60000 (100%)]\tLoss: 0.429225\n",
            "\n",
            "Test set: Average loss: 108.8231, Accuracy: 8940/10000 (89%)\n",
            "\n",
            "Train Epoch: 2 [59968/60000 (100%)]\tLoss: 0.071385\n",
            "\n",
            "Test set: Average loss: 74.0679, Accuracy: 9274/10000 (93%)\n",
            "\n",
            "Train Epoch: 3 [59968/60000 (100%)]\tLoss: 0.252288\n",
            "\n",
            "Test set: Average loss: 61.1804, Accuracy: 9401/10000 (94%)\n",
            "\n",
            "Train Epoch: 4 [59968/60000 (100%)]\tLoss: 0.308776\n",
            "\n",
            "Test set: Average loss: 53.8918, Accuracy: 9460/10000 (95%)\n",
            "\n",
            "Train Epoch: 5 [59968/60000 (100%)]\tLoss: 0.164819\n",
            "\n",
            "Test set: Average loss: 49.0676, Accuracy: 9488/10000 (95%)\n",
            "\n",
            "Train Epoch: 6 [59968/60000 (100%)]\tLoss: 0.014278\n",
            "\n",
            "Test set: Average loss: 50.4661, Accuracy: 9501/10000 (95%)\n",
            "\n",
            "Train Epoch: 7 [59968/60000 (100%)]\tLoss: 0.092907\n",
            "\n",
            "Test set: Average loss: 46.1685, Accuracy: 9546/10000 (95%)\n",
            "\n",
            "Train Epoch: 8 [59968/60000 (100%)]\tLoss: 0.063042\n",
            "\n",
            "Test set: Average loss: 46.6205, Accuracy: 9554/10000 (96%)\n",
            "\n",
            "Train Epoch: 9 [59968/60000 (100%)]\tLoss: 0.021147\n",
            "\n",
            "Test set: Average loss: 49.1171, Accuracy: 9545/10000 (95%)\n",
            "\n",
            "Train Epoch: 10 [59968/60000 (100%)]\tLoss: 0.036575\n",
            "\n",
            "Test set: Average loss: 50.1360, Accuracy: 9550/10000 (96%)\n",
            "\n",
            "Train Epoch: 11 [59968/60000 (100%)]\tLoss: 0.016516\n",
            "\n",
            "Test set: Average loss: 52.1300, Accuracy: 9547/10000 (95%)\n",
            "\n",
            "Train Epoch: 12 [59968/60000 (100%)]\tLoss: 0.013179\n",
            "\n",
            "Test set: Average loss: 56.4638, Accuracy: 9555/10000 (96%)\n",
            "\n",
            "Train Epoch: 13 [59968/60000 (100%)]\tLoss: 0.024152\n",
            "\n",
            "Test set: Average loss: 58.3816, Accuracy: 9552/10000 (96%)\n",
            "\n",
            "Train Epoch: 14 [59968/60000 (100%)]\tLoss: 0.007031\n",
            "\n",
            "Test set: Average loss: 59.4909, Accuracy: 9587/10000 (96%)\n",
            "\n",
            "Train Epoch: 15 [59968/60000 (100%)]\tLoss: 0.002313\n",
            "\n",
            "Test set: Average loss: 66.6675, Accuracy: 9551/10000 (96%)\n",
            "\n",
            "Train Epoch: 16 [59968/60000 (100%)]\tLoss: 0.000295\n",
            "\n",
            "Test set: Average loss: 70.7227, Accuracy: 9556/10000 (96%)\n",
            "\n",
            "Train Epoch: 17 [59968/60000 (100%)]\tLoss: 0.003352\n",
            "\n",
            "Test set: Average loss: 79.9487, Accuracy: 9544/10000 (95%)\n",
            "\n",
            "Train Epoch: 18 [59968/60000 (100%)]\tLoss: 0.001781\n",
            "\n",
            "Test set: Average loss: 85.8225, Accuracy: 9531/10000 (95%)\n",
            "\n",
            "Train Epoch: 19 [59968/60000 (100%)]\tLoss: 0.000214\n",
            "\n",
            "Test set: Average loss: 89.8334, Accuracy: 9547/10000 (95%)\n",
            "\n",
            "Train Epoch: 20 [59968/60000 (100%)]\tLoss: 0.005534\n",
            "\n",
            "Test set: Average loss: 93.8420, Accuracy: 9553/10000 (96%)\n",
            "\n",
            "Train Epoch: 21 [59968/60000 (100%)]\tLoss: 0.000058\n",
            "\n",
            "Test set: Average loss: 94.6283, Accuracy: 9535/10000 (95%)\n",
            "\n",
            "Train Epoch: 22 [59968/60000 (100%)]\tLoss: 0.000699\n",
            "\n",
            "Test set: Average loss: 98.4026, Accuracy: 9568/10000 (96%)\n",
            "\n",
            "Train Epoch: 23 [59968/60000 (100%)]\tLoss: 0.000273\n",
            "\n",
            "Test set: Average loss: 102.2325, Accuracy: 9570/10000 (96%)\n",
            "\n",
            "Train Epoch: 24 [59968/60000 (100%)]\tLoss: 0.000377\n",
            "\n",
            "Test set: Average loss: 115.5320, Accuracy: 9531/10000 (95%)\n",
            "\n",
            "Train Epoch: 25 [59968/60000 (100%)]\tLoss: 0.000071\n",
            "\n",
            "Test set: Average loss: 107.7990, Accuracy: 9542/10000 (95%)\n",
            "\n",
            "Train Epoch: 26 [59968/60000 (100%)]\tLoss: 0.001303\n",
            "\n",
            "Test set: Average loss: 109.3178, Accuracy: 9549/10000 (95%)\n",
            "\n",
            "Train Epoch: 27 [59968/60000 (100%)]\tLoss: 0.003044\n",
            "\n",
            "Test set: Average loss: 112.8822, Accuracy: 9546/10000 (95%)\n",
            "\n",
            "Train Epoch: 28 [59968/60000 (100%)]\tLoss: 0.000007\n",
            "\n",
            "Test set: Average loss: 108.1246, Accuracy: 9575/10000 (96%)\n",
            "\n",
            "Train Epoch: 29 [59968/60000 (100%)]\tLoss: 0.000815\n",
            "\n",
            "Test set: Average loss: 114.0364, Accuracy: 9560/10000 (96%)\n",
            "\n",
            "Train Epoch: 30 [59968/60000 (100%)]\tLoss: 0.000385\n",
            "\n",
            "Test set: Average loss: 117.2918, Accuracy: 9543/10000 (95%)\n",
            "\n",
            "Train Epoch: 31 [59968/60000 (100%)]\tLoss: 0.000046\n",
            "\n",
            "Test set: Average loss: 114.3241, Accuracy: 9578/10000 (96%)\n",
            "\n",
            "Train Epoch: 32 [59968/60000 (100%)]\tLoss: 0.010091\n",
            "\n",
            "Test set: Average loss: 127.9699, Accuracy: 9545/10000 (95%)\n",
            "\n",
            "Train Epoch: 33 [59968/60000 (100%)]\tLoss: 0.000182\n",
            "\n",
            "Test set: Average loss: 117.0338, Accuracy: 9547/10000 (95%)\n",
            "\n",
            "Train Epoch: 34 [59968/60000 (100%)]\tLoss: 0.000083\n",
            "\n",
            "Test set: Average loss: 117.5257, Accuracy: 9557/10000 (96%)\n",
            "\n",
            "Train Epoch: 35 [59968/60000 (100%)]\tLoss: 0.010039\n",
            "\n",
            "Test set: Average loss: 118.9147, Accuracy: 9565/10000 (96%)\n",
            "\n",
            "Train Epoch: 36 [59968/60000 (100%)]\tLoss: 0.000002\n",
            "\n",
            "Test set: Average loss: 123.3948, Accuracy: 9545/10000 (95%)\n",
            "\n",
            "Train Epoch: 37 [59968/60000 (100%)]\tLoss: 0.000624\n",
            "\n",
            "Test set: Average loss: 129.0699, Accuracy: 9558/10000 (96%)\n",
            "\n",
            "Train Epoch: 38 [59968/60000 (100%)]\tLoss: 0.000019\n",
            "\n",
            "Test set: Average loss: 121.9508, Accuracy: 9585/10000 (96%)\n",
            "\n",
            "Train Epoch: 39 [59968/60000 (100%)]\tLoss: 0.000002\n",
            "\n",
            "Test set: Average loss: 152.7237, Accuracy: 9500/10000 (95%)\n",
            "\n",
            "Train Epoch: 40 [59968/60000 (100%)]\tLoss: 0.000003\n",
            "\n",
            "Test set: Average loss: 127.2366, Accuracy: 9563/10000 (96%)\n",
            "\n",
            "Train Epoch: 41 [59968/60000 (100%)]\tLoss: 0.000009\n",
            "\n",
            "Test set: Average loss: 141.6564, Accuracy: 9502/10000 (95%)\n",
            "\n",
            "Train Epoch: 42 [59968/60000 (100%)]\tLoss: 0.000005\n",
            "\n",
            "Test set: Average loss: 139.3666, Accuracy: 9550/10000 (96%)\n",
            "\n",
            "Train Epoch: 43 [59968/60000 (100%)]\tLoss: 0.001088\n",
            "\n",
            "Test set: Average loss: 137.3584, Accuracy: 9535/10000 (95%)\n",
            "\n",
            "Train Epoch: 44 [59968/60000 (100%)]\tLoss: 0.000002\n",
            "\n",
            "Test set: Average loss: 135.5111, Accuracy: 9540/10000 (95%)\n",
            "\n",
            "Train Epoch: 45 [59968/60000 (100%)]\tLoss: 0.000010\n",
            "\n",
            "Test set: Average loss: 137.1500, Accuracy: 9559/10000 (96%)\n",
            "\n",
            "Train Epoch: 46 [59968/60000 (100%)]\tLoss: 0.001174\n",
            "\n",
            "Test set: Average loss: 151.3096, Accuracy: 9524/10000 (95%)\n",
            "\n",
            "Train Epoch: 47 [59968/60000 (100%)]\tLoss: 0.057034\n",
            "\n",
            "Test set: Average loss: 143.9655, Accuracy: 9551/10000 (96%)\n",
            "\n",
            "Train Epoch: 48 [59968/60000 (100%)]\tLoss: 0.000029\n",
            "\n",
            "Test set: Average loss: 135.6888, Accuracy: 9570/10000 (96%)\n",
            "\n",
            "Train Epoch: 49 [59968/60000 (100%)]\tLoss: 0.000002\n",
            "\n",
            "Test set: Average loss: 147.1459, Accuracy: 9565/10000 (96%)\n",
            "\n",
            "Train Epoch: 50 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 141.8572, Accuracy: 9530/10000 (95%)\n",
            "\n",
            "Train Epoch: 51 [59968/60000 (100%)]\tLoss: 0.006621\n",
            "\n",
            "Test set: Average loss: 145.7440, Accuracy: 9555/10000 (96%)\n",
            "\n",
            "Train Epoch: 52 [59968/60000 (100%)]\tLoss: 0.000140\n",
            "\n",
            "Test set: Average loss: 135.7951, Accuracy: 9560/10000 (96%)\n",
            "\n",
            "Train Epoch: 53 [59968/60000 (100%)]\tLoss: 0.000014\n",
            "\n",
            "Test set: Average loss: 148.4115, Accuracy: 9560/10000 (96%)\n",
            "\n",
            "Train Epoch: 54 [59968/60000 (100%)]\tLoss: 0.000103\n",
            "\n",
            "Test set: Average loss: 144.8824, Accuracy: 9573/10000 (96%)\n",
            "\n",
            "Train Epoch: 55 [59968/60000 (100%)]\tLoss: 0.000084\n",
            "\n",
            "Test set: Average loss: 150.7109, Accuracy: 9531/10000 (95%)\n",
            "\n",
            "Train Epoch: 56 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 140.5209, Accuracy: 9562/10000 (96%)\n",
            "\n",
            "Train Epoch: 57 [59968/60000 (100%)]\tLoss: 0.002763\n",
            "\n",
            "Test set: Average loss: 149.6131, Accuracy: 9539/10000 (95%)\n",
            "\n",
            "Train Epoch: 58 [59968/60000 (100%)]\tLoss: 0.000154\n",
            "\n",
            "Test set: Average loss: 155.8570, Accuracy: 9553/10000 (96%)\n",
            "\n",
            "Train Epoch: 59 [59968/60000 (100%)]\tLoss: 0.000064\n",
            "\n",
            "Test set: Average loss: 149.0424, Accuracy: 9565/10000 (96%)\n",
            "\n",
            "Train Epoch: 60 [59968/60000 (100%)]\tLoss: 0.000003\n",
            "\n",
            "Test set: Average loss: 150.0036, Accuracy: 9558/10000 (96%)\n",
            "\n",
            "Train Epoch: 61 [59968/60000 (100%)]\tLoss: 0.000021\n",
            "\n",
            "Test set: Average loss: 148.5170, Accuracy: 9557/10000 (96%)\n",
            "\n",
            "Train Epoch: 62 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 149.3627, Accuracy: 9565/10000 (96%)\n",
            "\n",
            "Train Epoch: 63 [59968/60000 (100%)]\tLoss: 0.000005\n",
            "\n",
            "Test set: Average loss: 152.8466, Accuracy: 9553/10000 (96%)\n",
            "\n",
            "Train Epoch: 64 [59968/60000 (100%)]\tLoss: 0.000018\n",
            "\n",
            "Test set: Average loss: 158.7882, Accuracy: 9537/10000 (95%)\n",
            "\n",
            "Train Epoch: 65 [59968/60000 (100%)]\tLoss: 0.000004\n",
            "\n",
            "Test set: Average loss: 149.7760, Accuracy: 9561/10000 (96%)\n",
            "\n",
            "Train Epoch: 66 [59968/60000 (100%)]\tLoss: 0.000771\n",
            "\n",
            "Test set: Average loss: 157.2093, Accuracy: 9567/10000 (96%)\n",
            "\n",
            "Train Epoch: 67 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 149.0723, Accuracy: 9581/10000 (96%)\n",
            "\n",
            "Train Epoch: 68 [59968/60000 (100%)]\tLoss: 0.000002\n",
            "\n",
            "Test set: Average loss: 152.2447, Accuracy: 9567/10000 (96%)\n",
            "\n",
            "Train Epoch: 69 [59968/60000 (100%)]\tLoss: 0.000005\n",
            "\n",
            "Test set: Average loss: 152.9717, Accuracy: 9561/10000 (96%)\n",
            "\n",
            "Train Epoch: 70 [59968/60000 (100%)]\tLoss: 0.000038\n",
            "\n",
            "Test set: Average loss: 155.1909, Accuracy: 9565/10000 (96%)\n",
            "\n",
            "Train Epoch: 71 [59968/60000 (100%)]\tLoss: 0.000362\n",
            "\n",
            "Test set: Average loss: 156.3348, Accuracy: 9539/10000 (95%)\n",
            "\n",
            "Train Epoch: 72 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 157.6590, Accuracy: 9549/10000 (95%)\n",
            "\n",
            "Train Epoch: 73 [59968/60000 (100%)]\tLoss: 0.000002\n",
            "\n",
            "Test set: Average loss: 155.6373, Accuracy: 9556/10000 (96%)\n",
            "\n",
            "Train Epoch: 74 [59968/60000 (100%)]\tLoss: 0.000095\n",
            "\n",
            "Test set: Average loss: 154.1083, Accuracy: 9566/10000 (96%)\n",
            "\n",
            "Train Epoch: 75 [59968/60000 (100%)]\tLoss: 0.000179\n",
            "\n",
            "Test set: Average loss: 163.7223, Accuracy: 9550/10000 (96%)\n",
            "\n",
            "Train Epoch: 76 [59968/60000 (100%)]\tLoss: 0.000224\n",
            "\n",
            "Test set: Average loss: 159.7481, Accuracy: 9553/10000 (96%)\n",
            "\n",
            "Train Epoch: 77 [59968/60000 (100%)]\tLoss: 0.003021\n",
            "\n",
            "Test set: Average loss: 155.6357, Accuracy: 9553/10000 (96%)\n",
            "\n",
            "Train Epoch: 78 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 167.9891, Accuracy: 9549/10000 (95%)\n",
            "\n",
            "Train Epoch: 79 [59968/60000 (100%)]\tLoss: 0.000004\n",
            "\n",
            "Test set: Average loss: 163.0823, Accuracy: 9532/10000 (95%)\n",
            "\n",
            "Train Epoch: 80 [59968/60000 (100%)]\tLoss: 0.000040\n",
            "\n",
            "Test set: Average loss: 160.9456, Accuracy: 9570/10000 (96%)\n",
            "\n",
            "Train Epoch: 81 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 166.3769, Accuracy: 9539/10000 (95%)\n",
            "\n",
            "Train Epoch: 82 [59968/60000 (100%)]\tLoss: 0.000027\n",
            "\n",
            "Test set: Average loss: 162.0898, Accuracy: 9552/10000 (96%)\n",
            "\n",
            "Train Epoch: 83 [59968/60000 (100%)]\tLoss: 0.000004\n",
            "\n",
            "Test set: Average loss: 161.4283, Accuracy: 9582/10000 (96%)\n",
            "\n",
            "Train Epoch: 84 [59968/60000 (100%)]\tLoss: 0.000122\n",
            "\n",
            "Test set: Average loss: 168.1165, Accuracy: 9536/10000 (95%)\n",
            "\n",
            "Train Epoch: 85 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 169.1142, Accuracy: 9531/10000 (95%)\n",
            "\n",
            "Train Epoch: 86 [59968/60000 (100%)]\tLoss: 0.057450\n",
            "\n",
            "Test set: Average loss: 167.8971, Accuracy: 9578/10000 (96%)\n",
            "\n",
            "Train Epoch: 87 [59968/60000 (100%)]\tLoss: 0.000210\n",
            "\n",
            "Test set: Average loss: 163.2237, Accuracy: 9558/10000 (96%)\n",
            "\n",
            "Train Epoch: 88 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 160.6386, Accuracy: 9558/10000 (96%)\n",
            "\n",
            "Train Epoch: 89 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 159.2643, Accuracy: 9568/10000 (96%)\n",
            "\n",
            "Train Epoch: 90 [59968/60000 (100%)]\tLoss: 0.000011\n",
            "\n",
            "Test set: Average loss: 158.7010, Accuracy: 9566/10000 (96%)\n",
            "\n",
            "Train Epoch: 91 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 158.8804, Accuracy: 9573/10000 (96%)\n",
            "\n",
            "Train Epoch: 92 [59968/60000 (100%)]\tLoss: 0.000004\n",
            "\n",
            "Test set: Average loss: 159.8302, Accuracy: 9571/10000 (96%)\n",
            "\n",
            "Train Epoch: 93 [59968/60000 (100%)]\tLoss: 0.000017\n",
            "\n",
            "Test set: Average loss: 197.9723, Accuracy: 9505/10000 (95%)\n",
            "\n",
            "Train Epoch: 94 [59968/60000 (100%)]\tLoss: 0.000015\n",
            "\n",
            "Test set: Average loss: 175.8540, Accuracy: 9563/10000 (96%)\n",
            "\n",
            "Train Epoch: 95 [59968/60000 (100%)]\tLoss: 0.000279\n",
            "\n",
            "Test set: Average loss: 174.5651, Accuracy: 9564/10000 (96%)\n",
            "\n",
            "Train Epoch: 96 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 164.0195, Accuracy: 9574/10000 (96%)\n",
            "\n",
            "Train Epoch: 97 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 169.8658, Accuracy: 9569/10000 (96%)\n",
            "\n",
            "Train Epoch: 98 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 168.8677, Accuracy: 9564/10000 (96%)\n",
            "\n",
            "Train Epoch: 99 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 169.5930, Accuracy: 9553/10000 (96%)\n",
            "\n",
            "Train Epoch: 100 [59968/60000 (100%)]\tLoss: 0.000006\n",
            "\n",
            "Test set: Average loss: 176.3904, Accuracy: 9545/10000 (95%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FRNPF(DI)"
      ],
      "metadata": {
        "id": "9jN9gsQx4I6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Galu(6, 128)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35EqSMSC4F-p",
        "outputId": "9f1929c9-f679-4ead-fb34-f2a2deb5e313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Galu(\n",
              "  (R1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (G1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (R2): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (G2): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (R3): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (G3): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (R4): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (G4): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (R5): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (G5): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (outputs): Linear(in_features=128, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for p in model.parameters():\n",
        "  print(p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMtlczbg5pLU",
        "outputId": "76c87c9a-4fcf-43db-cc82-0375fabfc15d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0175,  0.0339, -0.0281,  ...,  0.0258, -0.0037,  0.0338],\n",
            "        [ 0.0236,  0.0342,  0.0038,  ..., -0.0260, -0.0349,  0.0076],\n",
            "        [-0.0275, -0.0221, -0.0112,  ...,  0.0019,  0.0136,  0.0007],\n",
            "        ...,\n",
            "        [ 0.0115,  0.0184,  0.0234,  ...,  0.0205,  0.0050,  0.0284],\n",
            "        [-0.0114, -0.0229,  0.0006,  ...,  0.0051, -0.0103,  0.0282],\n",
            "        [-0.0113,  0.0345, -0.0118,  ...,  0.0204,  0.0284, -0.0018]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0225, -0.0062,  0.0227,  0.0352,  0.0107, -0.0216, -0.0232, -0.0265,\n",
            "        -0.0234, -0.0093,  0.0210, -0.0132, -0.0127, -0.0029, -0.0132,  0.0042,\n",
            "        -0.0021,  0.0261, -0.0014, -0.0122,  0.0184,  0.0115,  0.0082,  0.0310,\n",
            "        -0.0116,  0.0003,  0.0146, -0.0232, -0.0065, -0.0240,  0.0178, -0.0343,\n",
            "        -0.0024,  0.0196, -0.0353,  0.0123, -0.0129, -0.0095,  0.0153, -0.0166,\n",
            "         0.0172, -0.0236, -0.0061, -0.0190,  0.0245,  0.0040, -0.0090, -0.0273,\n",
            "        -0.0350,  0.0117, -0.0295, -0.0294, -0.0159,  0.0223, -0.0181,  0.0219,\n",
            "         0.0323,  0.0070, -0.0289, -0.0158, -0.0199, -0.0291,  0.0015,  0.0140,\n",
            "         0.0344, -0.0088, -0.0038, -0.0113, -0.0161,  0.0027,  0.0111,  0.0004,\n",
            "        -0.0043,  0.0008,  0.0267,  0.0069,  0.0277,  0.0016,  0.0189,  0.0281,\n",
            "         0.0173,  0.0090,  0.0221, -0.0070,  0.0044,  0.0042,  0.0176,  0.0162,\n",
            "        -0.0222, -0.0321,  0.0040,  0.0231, -0.0003, -0.0334, -0.0201,  0.0339,\n",
            "         0.0190, -0.0230,  0.0026, -0.0343, -0.0251,  0.0134,  0.0330,  0.0343,\n",
            "        -0.0339, -0.0271, -0.0251,  0.0346, -0.0083, -0.0216, -0.0286,  0.0002,\n",
            "         0.0001, -0.0214, -0.0071,  0.0178, -0.0313, -0.0264, -0.0164, -0.0058,\n",
            "         0.0320,  0.0011, -0.0256, -0.0310,  0.0004,  0.0193,  0.0271, -0.0194],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.0268, -0.0019, -0.0206,  ..., -0.0338, -0.0258, -0.0098],\n",
            "        [ 0.0146,  0.0321,  0.0136,  ...,  0.0295,  0.0227, -0.0296],\n",
            "        [ 0.0325, -0.0236,  0.0151,  ...,  0.0265, -0.0236, -0.0078],\n",
            "        ...,\n",
            "        [-0.0221,  0.0159, -0.0223,  ..., -0.0282, -0.0025, -0.0354],\n",
            "        [ 0.0207, -0.0067, -0.0311,  ..., -0.0025,  0.0296,  0.0335],\n",
            "        [ 0.0189,  0.0232,  0.0314,  ...,  0.0111,  0.0220,  0.0248]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0109, -0.0037, -0.0197,  0.0098,  0.0161,  0.0081,  0.0201, -0.0259,\n",
            "         0.0262,  0.0200, -0.0032, -0.0346,  0.0186,  0.0045, -0.0066, -0.0170,\n",
            "        -0.0325, -0.0039, -0.0323,  0.0112,  0.0258, -0.0253,  0.0236, -0.0068,\n",
            "         0.0116, -0.0208, -0.0221, -0.0124, -0.0246,  0.0110, -0.0018, -0.0332,\n",
            "         0.0353,  0.0035, -0.0262, -0.0352, -0.0287,  0.0284, -0.0221, -0.0053,\n",
            "         0.0057, -0.0105,  0.0224,  0.0350, -0.0206, -0.0168,  0.0286, -0.0139,\n",
            "        -0.0102, -0.0147, -0.0267,  0.0244, -0.0098,  0.0014,  0.0048, -0.0192,\n",
            "         0.0002,  0.0318,  0.0202,  0.0064,  0.0209,  0.0107, -0.0349, -0.0255,\n",
            "        -0.0190,  0.0314,  0.0021,  0.0129,  0.0045, -0.0177,  0.0122, -0.0157,\n",
            "         0.0108, -0.0228, -0.0151,  0.0131,  0.0315, -0.0067, -0.0024, -0.0056,\n",
            "        -0.0313,  0.0236, -0.0322, -0.0191,  0.0335,  0.0171, -0.0326, -0.0089,\n",
            "        -0.0320,  0.0144,  0.0258, -0.0355,  0.0097,  0.0203,  0.0199,  0.0337,\n",
            "         0.0040,  0.0138,  0.0086, -0.0144, -0.0120, -0.0289, -0.0185, -0.0058,\n",
            "        -0.0279, -0.0300,  0.0337,  0.0112,  0.0257, -0.0193, -0.0143,  0.0336,\n",
            "         0.0190,  0.0290,  0.0230,  0.0174, -0.0257, -0.0067, -0.0108, -0.0261,\n",
            "         0.0217, -0.0346, -0.0340,  0.0155,  0.0175, -0.0223,  0.0191, -0.0344],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.0128, -0.0240, -0.0327,  ...,  0.0568,  0.0647, -0.0726],\n",
            "        [-0.0425,  0.0183, -0.0265,  ...,  0.0377,  0.0762, -0.0324],\n",
            "        [-0.0389,  0.0697, -0.0168,  ...,  0.0129,  0.0047, -0.0866],\n",
            "        ...,\n",
            "        [-0.0055, -0.0207, -0.0682,  ...,  0.0159, -0.0382, -0.0650],\n",
            "        [ 0.0754, -0.0255, -0.0631,  ..., -0.0411,  0.0323, -0.0181],\n",
            "        [ 0.0013, -0.0496, -0.0421,  ...,  0.0015, -0.0482, -0.0477]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0501,  0.0820, -0.0566,  0.0030,  0.0581, -0.0617,  0.0302,  0.0137,\n",
            "        -0.0149, -0.0726,  0.0823,  0.0340,  0.0102,  0.0335, -0.0360, -0.0450,\n",
            "         0.0877, -0.0847,  0.0590,  0.0558, -0.0027, -0.0777,  0.0826, -0.0729,\n",
            "        -0.0201,  0.0123,  0.0072,  0.0560,  0.0005, -0.0803,  0.0664, -0.0673,\n",
            "        -0.0038,  0.0127,  0.0654,  0.0147,  0.0816, -0.0605,  0.0756, -0.0102,\n",
            "        -0.0263, -0.0273, -0.0830, -0.0539, -0.0607,  0.0540,  0.0379,  0.0375,\n",
            "         0.0207, -0.0032, -0.0081,  0.0606, -0.0122, -0.0337,  0.0471,  0.0806,\n",
            "         0.0070, -0.0078, -0.0235,  0.0556,  0.0434,  0.0648,  0.0769,  0.0479,\n",
            "        -0.0625, -0.0298,  0.0643,  0.0130,  0.0311, -0.0859,  0.0365,  0.0334,\n",
            "         0.0502, -0.0568,  0.0734, -0.0293, -0.0415, -0.0004,  0.0373,  0.0453,\n",
            "        -0.0380,  0.0615,  0.0581, -0.0067,  0.0633,  0.0618,  0.0740, -0.0838,\n",
            "         0.0374, -0.0354,  0.0447,  0.0796, -0.0516, -0.0300,  0.0468, -0.0633,\n",
            "         0.0660, -0.0594,  0.0111,  0.0820, -0.0578,  0.0043, -0.0114, -0.0040,\n",
            "         0.0434, -0.0071, -0.0085,  0.0231, -0.0435,  0.0213, -0.0052,  0.0527,\n",
            "         0.0586,  0.0268, -0.0743, -0.0837,  0.0075, -0.0340, -0.0116, -0.0353,\n",
            "        -0.0273,  0.0344,  0.0861, -0.0503,  0.0212,  0.0497, -0.0121,  0.0256],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-2.7148e-02, -5.8318e-02, -7.4931e-02,  ...,  4.2491e-02,\n",
            "          6.8583e-02, -8.6301e-02],\n",
            "        [-1.1330e-02,  3.5931e-02, -1.7478e-02,  ...,  3.7756e-02,\n",
            "          8.2321e-02, -2.0705e-02],\n",
            "        [-3.9860e-02, -4.5067e-02, -2.8850e-02,  ..., -3.9120e-02,\n",
            "         -1.2580e-02,  6.9068e-02],\n",
            "        ...,\n",
            "        [ 3.3588e-02,  3.0216e-02, -4.3204e-02,  ...,  2.1864e-02,\n",
            "         -2.9441e-02,  5.4997e-02],\n",
            "        [ 2.3493e-02,  8.5081e-02,  2.2001e-05,  ..., -4.1074e-02,\n",
            "         -4.1911e-02, -2.8649e-02],\n",
            "        [-7.6234e-02,  1.4695e-02,  3.1123e-02,  ...,  3.2278e-02,\n",
            "          1.4299e-02, -4.2295e-02]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0736,  0.0776,  0.0522,  0.0753, -0.0287, -0.0137, -0.0740,  0.0762,\n",
            "        -0.0794,  0.0148, -0.0504, -0.0869, -0.0295,  0.0649,  0.0474,  0.0415,\n",
            "         0.0647, -0.0191,  0.0576,  0.0876,  0.0208,  0.0336,  0.0307,  0.0087,\n",
            "         0.0662, -0.0869, -0.0562,  0.0756, -0.0486, -0.0313,  0.0411, -0.0440,\n",
            "         0.0467, -0.0618, -0.0195,  0.0518,  0.0578, -0.0374, -0.0774, -0.0183,\n",
            "        -0.0456,  0.0396, -0.0571,  0.0575,  0.0197, -0.0240,  0.0840,  0.0145,\n",
            "        -0.0136, -0.0292, -0.0686,  0.0233,  0.0156,  0.0088, -0.0647, -0.0087,\n",
            "         0.0593, -0.0402, -0.0604, -0.0589,  0.0024, -0.0509, -0.0633,  0.0308,\n",
            "        -0.0599, -0.0258,  0.0005,  0.0710, -0.0148,  0.0336, -0.0235, -0.0253,\n",
            "         0.0415,  0.0691,  0.0282, -0.0262, -0.0301, -0.0218,  0.0293, -0.0716,\n",
            "        -0.0210, -0.0186, -0.0658,  0.0421,  0.0356, -0.0437,  0.0279, -0.0243,\n",
            "         0.0830,  0.0739, -0.0327,  0.0499, -0.0822,  0.0098, -0.0086, -0.0267,\n",
            "        -0.0454,  0.0272,  0.0064,  0.0039, -0.0390, -0.0519,  0.0133,  0.0003,\n",
            "        -0.0562,  0.0381, -0.0101,  0.0448, -0.0229, -0.0699,  0.0048, -0.0260,\n",
            "        -0.0438,  0.0509, -0.0031,  0.0196,  0.0229,  0.0762, -0.0033, -0.0556,\n",
            "        -0.0255, -0.0775, -0.0652, -0.0615, -0.0302, -0.0515,  0.0306,  0.0516],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.0408,  0.0322, -0.0554,  ..., -0.0850,  0.0834,  0.0358],\n",
            "        [ 0.0028,  0.0808,  0.0241,  ..., -0.0130,  0.0093,  0.0133],\n",
            "        [-0.0868, -0.0797,  0.0394,  ..., -0.0697,  0.0731,  0.0697],\n",
            "        ...,\n",
            "        [-0.0005,  0.0252,  0.0214,  ...,  0.0610, -0.0674,  0.0036],\n",
            "        [-0.0367,  0.0067,  0.0620,  ...,  0.0342,  0.0689,  0.0201],\n",
            "        [-0.0688, -0.0153, -0.0873,  ..., -0.0795,  0.0380, -0.0026]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0806,  0.0244, -0.0425,  0.0838, -0.0570,  0.0092,  0.0429,  0.0607,\n",
            "        -0.0463, -0.0147, -0.0437, -0.0268,  0.0340, -0.0564, -0.0086,  0.0697,\n",
            "        -0.0273,  0.0199, -0.0182,  0.0004,  0.0641,  0.0138, -0.0157,  0.0478,\n",
            "         0.0723, -0.0318,  0.0503,  0.0717, -0.0643,  0.0437,  0.0342,  0.0106,\n",
            "         0.0875,  0.0373,  0.0857,  0.0650, -0.0191,  0.0526, -0.0748, -0.0755,\n",
            "        -0.0704,  0.0705, -0.0558,  0.0683, -0.0182,  0.0286,  0.0611, -0.0740,\n",
            "         0.0277, -0.0131,  0.0144,  0.0835,  0.0261, -0.0802, -0.0495,  0.0329,\n",
            "        -0.0312,  0.0400,  0.0662, -0.0456, -0.0615, -0.0562, -0.0171,  0.0378,\n",
            "         0.0325, -0.0272, -0.0657,  0.0699,  0.0364, -0.0601,  0.0530,  0.0064,\n",
            "        -0.0830,  0.0607,  0.0370, -0.0055, -0.0306, -0.0401,  0.0510,  0.0500,\n",
            "         0.0695, -0.0421, -0.0008,  0.0412,  0.0677, -0.0780,  0.0701,  0.0147,\n",
            "        -0.0486, -0.0662, -0.0198,  0.0589, -0.0559,  0.0526, -0.0827, -0.0604,\n",
            "         0.0151,  0.0491, -0.0533,  0.0201,  0.0227, -0.0540,  0.0692, -0.0235,\n",
            "         0.0742, -0.0854, -0.0530,  0.0547,  0.0575, -0.0815, -0.0328, -0.0488,\n",
            "         0.0372,  0.0373,  0.0489,  0.0259,  0.0646, -0.0682,  0.0451,  0.0240,\n",
            "         0.0508,  0.0298, -0.0676,  0.0610,  0.0774,  0.0199, -0.0388, -0.0427],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.0728,  0.0634, -0.0851,  ..., -0.0173,  0.0635, -0.0096],\n",
            "        [ 0.0695,  0.0353,  0.0544,  ..., -0.0601,  0.0819,  0.0852],\n",
            "        [ 0.0410,  0.0162, -0.0139,  ...,  0.0188,  0.0815, -0.0678],\n",
            "        ...,\n",
            "        [-0.0355, -0.0588,  0.0513,  ...,  0.0134,  0.0809, -0.0117],\n",
            "        [ 0.0820,  0.0413, -0.0212,  ...,  0.0205,  0.0035,  0.0574],\n",
            "        [-0.0800,  0.0324,  0.0238,  ...,  0.0201,  0.0765, -0.0256]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0573,  0.0381,  0.0467, -0.0692,  0.0199,  0.0675,  0.0397, -0.0439,\n",
            "        -0.0200, -0.0772, -0.0392,  0.0871, -0.0134,  0.0094, -0.0437,  0.0715,\n",
            "        -0.0429,  0.0294, -0.0804,  0.0641, -0.0529,  0.0879, -0.0360,  0.0629,\n",
            "         0.0071,  0.0006,  0.0235,  0.0688,  0.0874,  0.0100,  0.0763,  0.0238,\n",
            "        -0.0243,  0.0125, -0.0011, -0.0716, -0.0774,  0.0663,  0.0201,  0.0458,\n",
            "         0.0800,  0.0127, -0.0266,  0.0046,  0.0189, -0.0079, -0.0410,  0.0269,\n",
            "         0.0133, -0.0679, -0.0037,  0.0577,  0.0725, -0.0318, -0.0427, -0.0120,\n",
            "         0.0110,  0.0811,  0.0419, -0.0092, -0.0784,  0.0654,  0.0697, -0.0670,\n",
            "        -0.0823, -0.0359,  0.0493,  0.0328, -0.0095, -0.0232, -0.0042, -0.0780,\n",
            "         0.0858, -0.0413,  0.0284,  0.0490,  0.0089, -0.0216,  0.0066,  0.0644,\n",
            "         0.0606, -0.0649, -0.0426,  0.0389, -0.0759,  0.0461,  0.0293, -0.0265,\n",
            "         0.0508, -0.0564,  0.0829, -0.0451,  0.0474,  0.0059, -0.0330, -0.0101,\n",
            "        -0.0404,  0.0839,  0.0576,  0.0614, -0.0317,  0.0694,  0.0016,  0.0320,\n",
            "        -0.0450,  0.0663,  0.0162,  0.0758, -0.0050, -0.0024,  0.0334, -0.0298,\n",
            "         0.0299, -0.0760, -0.0449,  0.0790, -0.0095, -0.0647,  0.0812, -0.0508,\n",
            "         0.0733, -0.0199,  0.0851,  0.0227,  0.0444,  0.0150,  0.0129,  0.0457],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.0672,  0.0861,  0.0560,  ...,  0.0424,  0.0771,  0.0550],\n",
            "        [-0.0017, -0.0655, -0.0081,  ...,  0.0870,  0.0188,  0.0736],\n",
            "        [-0.0334, -0.0180,  0.0365,  ..., -0.0439,  0.0416, -0.0406],\n",
            "        ...,\n",
            "        [ 0.0080, -0.0833,  0.0175,  ..., -0.0548,  0.0455, -0.0757],\n",
            "        [ 0.0171, -0.0091,  0.0772,  ..., -0.0775,  0.0434,  0.0881],\n",
            "        [-0.0008, -0.0354,  0.0836,  ..., -0.0188, -0.0828,  0.0286]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-2.0636e-02,  2.4237e-03, -4.7104e-02, -4.9104e-02, -3.6439e-02,\n",
            "         4.3090e-02, -8.0420e-02, -3.0438e-02, -6.8298e-03, -7.8080e-02,\n",
            "         8.0111e-02, -5.7742e-04, -3.0210e-02,  6.7007e-02, -7.1246e-02,\n",
            "        -5.7048e-02, -4.0973e-02,  4.0453e-02, -7.7899e-02, -4.0138e-02,\n",
            "        -2.1432e-02, -2.7565e-02, -7.3480e-02,  4.7000e-02,  4.5017e-03,\n",
            "        -1.4517e-02, -2.1087e-02, -4.1883e-02, -7.1726e-02,  1.8051e-02,\n",
            "         1.0630e-02,  7.2361e-02, -6.7731e-02, -7.2484e-02, -4.7880e-02,\n",
            "        -2.8049e-02,  7.9208e-02, -3.9580e-02,  7.1238e-02,  3.2604e-02,\n",
            "        -7.4670e-02, -4.1844e-02,  4.4570e-02,  3.2517e-02, -6.0978e-02,\n",
            "        -5.5229e-02, -9.4840e-03,  8.4527e-02,  6.3209e-02, -7.6026e-02,\n",
            "        -5.4974e-02,  2.3544e-02,  8.3336e-02, -1.6856e-02,  5.2935e-02,\n",
            "         4.0427e-02,  6.8732e-02, -5.3991e-02, -4.6727e-02, -3.5684e-02,\n",
            "        -8.2908e-02, -5.0395e-02, -4.7040e-02,  8.3243e-02,  4.5315e-02,\n",
            "         1.6893e-02,  6.7710e-02,  4.6377e-02,  8.6811e-02,  1.0526e-02,\n",
            "         1.5290e-02, -5.2327e-02,  8.3878e-02,  6.9532e-02,  5.5486e-05,\n",
            "        -4.3075e-02, -8.7238e-02,  2.7323e-02,  3.7976e-02,  6.4509e-02,\n",
            "         1.1365e-02,  3.3209e-02,  6.1090e-02, -6.4224e-02, -3.9313e-02,\n",
            "         6.4309e-02, -3.7499e-02,  5.7311e-02,  2.6998e-02, -4.2985e-02,\n",
            "         6.9589e-02,  1.6833e-03,  6.0343e-02,  4.4052e-02,  2.5569e-02,\n",
            "        -3.3303e-02,  2.8341e-02, -5.8712e-02, -8.5097e-03,  1.9453e-02,\n",
            "        -5.0596e-02, -1.1348e-02,  7.3621e-02, -1.8639e-02, -1.4376e-02,\n",
            "        -5.3432e-02, -2.6950e-02,  5.3824e-02,  5.3415e-02,  7.2985e-02,\n",
            "        -5.1353e-03,  7.6151e-02, -4.7714e-02, -4.2279e-02, -4.4811e-02,\n",
            "        -7.8533e-02, -1.6286e-02, -2.7951e-02,  3.9269e-02,  5.5587e-02,\n",
            "         7.3606e-02, -4.3493e-02, -1.6648e-02,  6.6029e-02,  4.5123e-02,\n",
            "        -6.2261e-04,  6.7479e-02, -2.2800e-02], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.0832,  0.0331, -0.0112,  ...,  0.0361,  0.0266, -0.0395],\n",
            "        [-0.0550, -0.0553,  0.0057,  ...,  0.0801, -0.0381, -0.0307],\n",
            "        [-0.0490,  0.0421, -0.0340,  ...,  0.0805,  0.0675,  0.0226],\n",
            "        ...,\n",
            "        [ 0.0277, -0.0866,  0.0390,  ...,  0.0333,  0.0133, -0.0417],\n",
            "        [ 0.0448,  0.0343, -0.0795,  ...,  0.0599,  0.0052, -0.0206],\n",
            "        [-0.0262,  0.0360,  0.0407,  ..., -0.0303,  0.0790, -0.0626]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0130, -0.0204, -0.0752,  0.0666,  0.0307,  0.0616, -0.0524,  0.0386,\n",
            "        -0.0594, -0.0614,  0.0305, -0.0478,  0.0732, -0.0842, -0.0010,  0.0172,\n",
            "        -0.0690,  0.0561, -0.0040,  0.0876, -0.0129, -0.0436, -0.0006,  0.0213,\n",
            "        -0.0016,  0.0459,  0.0420,  0.0582,  0.0204, -0.0386,  0.0128, -0.0247,\n",
            "        -0.0552,  0.0579, -0.0740, -0.0711,  0.0676,  0.0771,  0.0217, -0.0739,\n",
            "        -0.0360, -0.0599, -0.0013, -0.0141, -0.0201,  0.0832, -0.0736, -0.0805,\n",
            "        -0.0394,  0.0447,  0.0018,  0.0796,  0.0243,  0.0389,  0.0344, -0.0099,\n",
            "        -0.0647, -0.0635, -0.0396,  0.0351,  0.0861,  0.0548,  0.0523,  0.0875,\n",
            "         0.0037,  0.0362,  0.0652, -0.0049, -0.0606, -0.0080,  0.0445, -0.0093,\n",
            "        -0.0305,  0.0040, -0.0486, -0.0093,  0.0383, -0.0771,  0.0616,  0.0745,\n",
            "        -0.0741, -0.0289,  0.0574,  0.0826,  0.0254, -0.0833,  0.0710,  0.0181,\n",
            "         0.0648, -0.0831, -0.0206,  0.0618,  0.0325,  0.0174, -0.0431, -0.0119,\n",
            "        -0.0660,  0.0738,  0.0806, -0.0440,  0.0050,  0.0480,  0.0218,  0.0381,\n",
            "        -0.0137,  0.0201, -0.0203,  0.0481, -0.0470, -0.0089, -0.0781,  0.0533,\n",
            "        -0.0798, -0.0575, -0.0039, -0.0824, -0.0180, -0.0760,  0.0716,  0.0880,\n",
            "         0.0525,  0.0515,  0.0858,  0.0475, -0.0305,  0.0020,  0.0400, -0.0426],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 5.5301e-02,  6.3122e-02, -3.0580e-03,  ..., -8.6223e-02,\n",
            "          2.9737e-02,  1.1229e-02],\n",
            "        [-6.4905e-02,  3.7100e-02, -1.8299e-02,  ..., -3.4341e-02,\n",
            "         -6.9808e-02, -6.3732e-02],\n",
            "        [-8.4980e-02, -4.8381e-02,  3.8227e-05,  ..., -5.0989e-03,\n",
            "         -7.7342e-02,  5.0518e-02],\n",
            "        ...,\n",
            "        [-5.7925e-02, -7.3013e-02, -7.8835e-02,  ..., -4.5654e-03,\n",
            "         -7.3153e-02, -2.7139e-02],\n",
            "        [-4.9941e-03, -7.1227e-02, -3.5379e-02,  ...,  8.5392e-02,\n",
            "          7.4753e-02, -2.1550e-02],\n",
            "        [ 4.7478e-02, -6.2006e-02, -7.5526e-02,  ..., -5.5007e-02,\n",
            "         -7.3400e-02,  2.0054e-02]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-5.9743e-02,  6.7917e-02,  6.8763e-03, -7.5240e-02, -7.4398e-02,\n",
            "        -1.0117e-02,  2.8360e-02,  5.4576e-02,  3.7263e-02, -1.1415e-02,\n",
            "        -7.5110e-02,  7.5441e-02,  6.0055e-02, -2.0259e-02, -2.2057e-02,\n",
            "        -2.3856e-02, -7.4424e-02, -1.9583e-03, -8.5406e-02,  7.7116e-02,\n",
            "        -5.2585e-02, -6.3060e-02,  5.5765e-02,  1.6831e-02,  5.3361e-02,\n",
            "        -5.7437e-02,  5.8391e-02,  2.2764e-02, -5.7752e-02,  1.3658e-02,\n",
            "        -7.2738e-02,  4.1908e-02, -2.8547e-02,  4.0446e-03,  5.7776e-02,\n",
            "         2.2999e-03, -2.9640e-02, -5.8496e-02,  5.4409e-02,  1.4298e-02,\n",
            "         2.6504e-02, -7.6503e-02,  5.3924e-02,  2.2195e-02, -6.1302e-02,\n",
            "         4.2892e-02,  6.1641e-02, -3.3649e-02, -7.2869e-02, -9.9375e-03,\n",
            "         4.1746e-02, -6.5874e-02,  5.1166e-02,  5.2873e-02,  5.6215e-02,\n",
            "        -3.5097e-02,  3.5435e-02,  4.1309e-02, -3.8279e-03,  1.6410e-02,\n",
            "         5.6393e-03,  5.1335e-02,  3.9268e-02,  2.1433e-02, -5.8953e-02,\n",
            "        -6.6166e-02, -3.2559e-02,  5.7192e-02, -6.3617e-02, -2.5706e-02,\n",
            "         2.5861e-02, -5.1605e-03,  4.4853e-02, -5.3562e-02,  8.4087e-02,\n",
            "         3.4059e-02, -7.6696e-02,  7.3610e-02,  3.3295e-03, -5.6325e-02,\n",
            "         6.5978e-02,  7.0572e-02,  1.1879e-03,  2.9620e-02,  2.2505e-02,\n",
            "        -6.2038e-02, -6.5433e-02,  8.6868e-02, -1.7624e-02,  7.1224e-02,\n",
            "         4.1828e-02, -1.0270e-02,  1.6497e-02, -1.9380e-02, -3.4780e-02,\n",
            "         5.7446e-02, -5.4731e-02, -4.0201e-02,  1.1924e-02,  7.6451e-02,\n",
            "         3.8925e-02,  8.3819e-03, -6.8032e-02, -2.3152e-02, -2.0841e-02,\n",
            "        -9.6498e-03,  5.7699e-02, -3.4537e-03,  4.9115e-02, -3.0332e-02,\n",
            "        -1.5451e-02,  4.6214e-02, -6.6497e-03, -3.3133e-02,  6.9089e-02,\n",
            "         6.0712e-02, -5.0555e-03,  5.8413e-02, -7.9845e-02,  6.9997e-02,\n",
            "        -4.6172e-05,  2.1131e-02,  3.5466e-02, -8.3012e-02, -1.1918e-02,\n",
            "        -8.6765e-02, -1.5183e-03,  7.5080e-02], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.0663,  0.0347, -0.0383,  ..., -0.0211, -0.0676,  0.0302],\n",
            "        [ 0.0069,  0.0053, -0.0306,  ...,  0.0530, -0.0525, -0.0880],\n",
            "        [ 0.0558,  0.0761, -0.0759,  ...,  0.0109,  0.0790,  0.0699],\n",
            "        ...,\n",
            "        [-0.0201, -0.0467,  0.0670,  ...,  0.0717, -0.0129, -0.0816],\n",
            "        [ 0.0455,  0.0880, -0.0735,  ...,  0.0788, -0.0324, -0.0152],\n",
            "        [-0.0726,  0.0672,  0.0720,  ...,  0.0314, -0.0283,  0.0398]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0823,  0.0381,  0.0634,  0.0349, -0.0072,  0.0727,  0.0679,  0.0771,\n",
            "         0.0332, -0.0326, -0.0126,  0.0618, -0.0709, -0.0457,  0.0434, -0.0536,\n",
            "        -0.0538, -0.0014, -0.0772,  0.0864, -0.0667,  0.0215, -0.0621, -0.0189,\n",
            "         0.0569,  0.0348, -0.0016, -0.0048, -0.0607,  0.0377,  0.0661,  0.0330,\n",
            "        -0.0605,  0.0780, -0.0382,  0.0260, -0.0248, -0.0495,  0.0821, -0.0232,\n",
            "        -0.0088, -0.0091, -0.0759, -0.0629, -0.0523, -0.0842,  0.0826, -0.0282,\n",
            "         0.0733, -0.0810, -0.0458,  0.0686,  0.0065, -0.0809, -0.0491, -0.0757,\n",
            "        -0.0757,  0.0091,  0.0521,  0.0756,  0.0425,  0.0072, -0.0492, -0.0276,\n",
            "         0.0048, -0.0806, -0.0145,  0.0539,  0.0596, -0.0134,  0.0793, -0.0237,\n",
            "         0.0112, -0.0196,  0.0578,  0.0169,  0.0326,  0.0606,  0.0417,  0.0776,\n",
            "        -0.0255,  0.0445, -0.0146, -0.0276,  0.0289,  0.0422, -0.0216, -0.0338,\n",
            "        -0.0789, -0.0757,  0.0258,  0.0117, -0.0864,  0.0410,  0.0160,  0.0052,\n",
            "         0.0644,  0.0052, -0.0558,  0.0389, -0.0788,  0.0085,  0.0037,  0.0748,\n",
            "         0.0100,  0.0235, -0.0621,  0.0627, -0.0503, -0.0803,  0.0483,  0.0018,\n",
            "        -0.0380,  0.0362, -0.0212, -0.0592, -0.0531, -0.0614,  0.0416,  0.0624,\n",
            "        -0.0785, -0.0765,  0.0027, -0.0699, -0.0353, -0.0435, -0.0784,  0.0520],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 1.5955e-02,  2.0806e-02,  1.8439e-02,  ..., -1.2642e-02,\n",
            "         -4.2856e-02, -3.3964e-02],\n",
            "        [-6.9004e-02, -5.0802e-02, -5.7926e-02,  ..., -2.1648e-02,\n",
            "          4.6429e-02, -3.6011e-02],\n",
            "        [-6.1387e-02, -6.5814e-04, -6.1413e-02,  ...,  8.2825e-02,\n",
            "         -3.2423e-02,  5.1470e-02],\n",
            "        ...,\n",
            "        [ 4.5572e-02, -5.2368e-02,  5.2782e-02,  ...,  7.1816e-02,\n",
            "         -3.5513e-02, -5.1322e-02],\n",
            "        [-6.3474e-02,  2.8514e-02, -6.5065e-02,  ..., -6.7556e-02,\n",
            "          1.8747e-02,  8.5370e-02],\n",
            "        [ 6.0564e-03, -9.1543e-05, -3.4080e-02,  ..., -4.5288e-02,\n",
            "         -4.2643e-02, -5.8453e-02]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0320,  0.0868, -0.0496, -0.0566,  0.0370, -0.0365, -0.0181,  0.0722,\n",
            "         0.0238, -0.0369], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.state_dict().get(\"G1.weight\").data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwvQL79s40w_",
        "outputId": "cceead16-1fb4-43b0-fa42-c6d409442925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0268, -0.0019, -0.0206,  ..., -0.0338, -0.0258, -0.0098],\n",
              "        [ 0.0146,  0.0321,  0.0136,  ...,  0.0295,  0.0227, -0.0296],\n",
              "        [ 0.0325, -0.0236,  0.0151,  ...,  0.0265, -0.0236, -0.0078],\n",
              "        ...,\n",
              "        [-0.0221,  0.0159, -0.0223,  ..., -0.0282, -0.0025, -0.0354],\n",
              "        [ 0.0207, -0.0067, -0.0311,  ..., -0.0025,  0.0296,  0.0335],\n",
              "        [ 0.0189,  0.0232,  0.0314,  ...,  0.0111,  0.0220,  0.0248]])"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if name[0]=='R':\n",
        "      paramName = \"G\"+name[1:]\n",
        "      param.data = model.state_dict().get(paramName).data\n",
        "      param.requires_grad = False\n",
        "    print(name, param)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBudZVO54dnb",
        "outputId": "f8677320-7c43-40a1-8325-9f1a97eb8e3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R1.weight Parameter containing:\n",
            "tensor([[-0.0268, -0.0019, -0.0206,  ..., -0.0338, -0.0258, -0.0098],\n",
            "        [ 0.0146,  0.0321,  0.0136,  ...,  0.0295,  0.0227, -0.0296],\n",
            "        [ 0.0325, -0.0236,  0.0151,  ...,  0.0265, -0.0236, -0.0078],\n",
            "        ...,\n",
            "        [-0.0221,  0.0159, -0.0223,  ..., -0.0282, -0.0025, -0.0354],\n",
            "        [ 0.0207, -0.0067, -0.0311,  ..., -0.0025,  0.0296,  0.0335],\n",
            "        [ 0.0189,  0.0232,  0.0314,  ...,  0.0111,  0.0220,  0.0248]])\n",
            "R1.bias Parameter containing:\n",
            "tensor([ 0.0109, -0.0037, -0.0197,  0.0098,  0.0161,  0.0081,  0.0201, -0.0259,\n",
            "         0.0262,  0.0200, -0.0032, -0.0346,  0.0186,  0.0045, -0.0066, -0.0170,\n",
            "        -0.0325, -0.0039, -0.0323,  0.0112,  0.0258, -0.0253,  0.0236, -0.0068,\n",
            "         0.0116, -0.0208, -0.0221, -0.0124, -0.0246,  0.0110, -0.0018, -0.0332,\n",
            "         0.0353,  0.0035, -0.0262, -0.0352, -0.0287,  0.0284, -0.0221, -0.0053,\n",
            "         0.0057, -0.0105,  0.0224,  0.0350, -0.0206, -0.0168,  0.0286, -0.0139,\n",
            "        -0.0102, -0.0147, -0.0267,  0.0244, -0.0098,  0.0014,  0.0048, -0.0192,\n",
            "         0.0002,  0.0318,  0.0202,  0.0064,  0.0209,  0.0107, -0.0349, -0.0255,\n",
            "        -0.0190,  0.0314,  0.0021,  0.0129,  0.0045, -0.0177,  0.0122, -0.0157,\n",
            "         0.0108, -0.0228, -0.0151,  0.0131,  0.0315, -0.0067, -0.0024, -0.0056,\n",
            "        -0.0313,  0.0236, -0.0322, -0.0191,  0.0335,  0.0171, -0.0326, -0.0089,\n",
            "        -0.0320,  0.0144,  0.0258, -0.0355,  0.0097,  0.0203,  0.0199,  0.0337,\n",
            "         0.0040,  0.0138,  0.0086, -0.0144, -0.0120, -0.0289, -0.0185, -0.0058,\n",
            "        -0.0279, -0.0300,  0.0337,  0.0112,  0.0257, -0.0193, -0.0143,  0.0336,\n",
            "         0.0190,  0.0290,  0.0230,  0.0174, -0.0257, -0.0067, -0.0108, -0.0261,\n",
            "         0.0217, -0.0346, -0.0340,  0.0155,  0.0175, -0.0223,  0.0191, -0.0344])\n",
            "G1.weight Parameter containing:\n",
            "tensor([[-0.0268, -0.0019, -0.0206,  ..., -0.0338, -0.0258, -0.0098],\n",
            "        [ 0.0146,  0.0321,  0.0136,  ...,  0.0295,  0.0227, -0.0296],\n",
            "        [ 0.0325, -0.0236,  0.0151,  ...,  0.0265, -0.0236, -0.0078],\n",
            "        ...,\n",
            "        [-0.0221,  0.0159, -0.0223,  ..., -0.0282, -0.0025, -0.0354],\n",
            "        [ 0.0207, -0.0067, -0.0311,  ..., -0.0025,  0.0296,  0.0335],\n",
            "        [ 0.0189,  0.0232,  0.0314,  ...,  0.0111,  0.0220,  0.0248]],\n",
            "       requires_grad=True)\n",
            "G1.bias Parameter containing:\n",
            "tensor([ 0.0109, -0.0037, -0.0197,  0.0098,  0.0161,  0.0081,  0.0201, -0.0259,\n",
            "         0.0262,  0.0200, -0.0032, -0.0346,  0.0186,  0.0045, -0.0066, -0.0170,\n",
            "        -0.0325, -0.0039, -0.0323,  0.0112,  0.0258, -0.0253,  0.0236, -0.0068,\n",
            "         0.0116, -0.0208, -0.0221, -0.0124, -0.0246,  0.0110, -0.0018, -0.0332,\n",
            "         0.0353,  0.0035, -0.0262, -0.0352, -0.0287,  0.0284, -0.0221, -0.0053,\n",
            "         0.0057, -0.0105,  0.0224,  0.0350, -0.0206, -0.0168,  0.0286, -0.0139,\n",
            "        -0.0102, -0.0147, -0.0267,  0.0244, -0.0098,  0.0014,  0.0048, -0.0192,\n",
            "         0.0002,  0.0318,  0.0202,  0.0064,  0.0209,  0.0107, -0.0349, -0.0255,\n",
            "        -0.0190,  0.0314,  0.0021,  0.0129,  0.0045, -0.0177,  0.0122, -0.0157,\n",
            "         0.0108, -0.0228, -0.0151,  0.0131,  0.0315, -0.0067, -0.0024, -0.0056,\n",
            "        -0.0313,  0.0236, -0.0322, -0.0191,  0.0335,  0.0171, -0.0326, -0.0089,\n",
            "        -0.0320,  0.0144,  0.0258, -0.0355,  0.0097,  0.0203,  0.0199,  0.0337,\n",
            "         0.0040,  0.0138,  0.0086, -0.0144, -0.0120, -0.0289, -0.0185, -0.0058,\n",
            "        -0.0279, -0.0300,  0.0337,  0.0112,  0.0257, -0.0193, -0.0143,  0.0336,\n",
            "         0.0190,  0.0290,  0.0230,  0.0174, -0.0257, -0.0067, -0.0108, -0.0261,\n",
            "         0.0217, -0.0346, -0.0340,  0.0155,  0.0175, -0.0223,  0.0191, -0.0344],\n",
            "       requires_grad=True)\n",
            "R2.weight Parameter containing:\n",
            "tensor([[-2.7148e-02, -5.8318e-02, -7.4931e-02,  ...,  4.2491e-02,\n",
            "          6.8583e-02, -8.6301e-02],\n",
            "        [-1.1330e-02,  3.5931e-02, -1.7478e-02,  ...,  3.7756e-02,\n",
            "          8.2321e-02, -2.0705e-02],\n",
            "        [-3.9860e-02, -4.5067e-02, -2.8850e-02,  ..., -3.9120e-02,\n",
            "         -1.2580e-02,  6.9068e-02],\n",
            "        ...,\n",
            "        [ 3.3588e-02,  3.0216e-02, -4.3204e-02,  ...,  2.1864e-02,\n",
            "         -2.9441e-02,  5.4997e-02],\n",
            "        [ 2.3493e-02,  8.5081e-02,  2.2001e-05,  ..., -4.1074e-02,\n",
            "         -4.1911e-02, -2.8649e-02],\n",
            "        [-7.6234e-02,  1.4695e-02,  3.1123e-02,  ...,  3.2278e-02,\n",
            "          1.4299e-02, -4.2295e-02]])\n",
            "R2.bias Parameter containing:\n",
            "tensor([ 0.0736,  0.0776,  0.0522,  0.0753, -0.0287, -0.0137, -0.0740,  0.0762,\n",
            "        -0.0794,  0.0148, -0.0504, -0.0869, -0.0295,  0.0649,  0.0474,  0.0415,\n",
            "         0.0647, -0.0191,  0.0576,  0.0876,  0.0208,  0.0336,  0.0307,  0.0087,\n",
            "         0.0662, -0.0869, -0.0562,  0.0756, -0.0486, -0.0313,  0.0411, -0.0440,\n",
            "         0.0467, -0.0618, -0.0195,  0.0518,  0.0578, -0.0374, -0.0774, -0.0183,\n",
            "        -0.0456,  0.0396, -0.0571,  0.0575,  0.0197, -0.0240,  0.0840,  0.0145,\n",
            "        -0.0136, -0.0292, -0.0686,  0.0233,  0.0156,  0.0088, -0.0647, -0.0087,\n",
            "         0.0593, -0.0402, -0.0604, -0.0589,  0.0024, -0.0509, -0.0633,  0.0308,\n",
            "        -0.0599, -0.0258,  0.0005,  0.0710, -0.0148,  0.0336, -0.0235, -0.0253,\n",
            "         0.0415,  0.0691,  0.0282, -0.0262, -0.0301, -0.0218,  0.0293, -0.0716,\n",
            "        -0.0210, -0.0186, -0.0658,  0.0421,  0.0356, -0.0437,  0.0279, -0.0243,\n",
            "         0.0830,  0.0739, -0.0327,  0.0499, -0.0822,  0.0098, -0.0086, -0.0267,\n",
            "        -0.0454,  0.0272,  0.0064,  0.0039, -0.0390, -0.0519,  0.0133,  0.0003,\n",
            "        -0.0562,  0.0381, -0.0101,  0.0448, -0.0229, -0.0699,  0.0048, -0.0260,\n",
            "        -0.0438,  0.0509, -0.0031,  0.0196,  0.0229,  0.0762, -0.0033, -0.0556,\n",
            "        -0.0255, -0.0775, -0.0652, -0.0615, -0.0302, -0.0515,  0.0306,  0.0516])\n",
            "G2.weight Parameter containing:\n",
            "tensor([[-2.7148e-02, -5.8318e-02, -7.4931e-02,  ...,  4.2491e-02,\n",
            "          6.8583e-02, -8.6301e-02],\n",
            "        [-1.1330e-02,  3.5931e-02, -1.7478e-02,  ...,  3.7756e-02,\n",
            "          8.2321e-02, -2.0705e-02],\n",
            "        [-3.9860e-02, -4.5067e-02, -2.8850e-02,  ..., -3.9120e-02,\n",
            "         -1.2580e-02,  6.9068e-02],\n",
            "        ...,\n",
            "        [ 3.3588e-02,  3.0216e-02, -4.3204e-02,  ...,  2.1864e-02,\n",
            "         -2.9441e-02,  5.4997e-02],\n",
            "        [ 2.3493e-02,  8.5081e-02,  2.2001e-05,  ..., -4.1074e-02,\n",
            "         -4.1911e-02, -2.8649e-02],\n",
            "        [-7.6234e-02,  1.4695e-02,  3.1123e-02,  ...,  3.2278e-02,\n",
            "          1.4299e-02, -4.2295e-02]], requires_grad=True)\n",
            "G2.bias Parameter containing:\n",
            "tensor([ 0.0736,  0.0776,  0.0522,  0.0753, -0.0287, -0.0137, -0.0740,  0.0762,\n",
            "        -0.0794,  0.0148, -0.0504, -0.0869, -0.0295,  0.0649,  0.0474,  0.0415,\n",
            "         0.0647, -0.0191,  0.0576,  0.0876,  0.0208,  0.0336,  0.0307,  0.0087,\n",
            "         0.0662, -0.0869, -0.0562,  0.0756, -0.0486, -0.0313,  0.0411, -0.0440,\n",
            "         0.0467, -0.0618, -0.0195,  0.0518,  0.0578, -0.0374, -0.0774, -0.0183,\n",
            "        -0.0456,  0.0396, -0.0571,  0.0575,  0.0197, -0.0240,  0.0840,  0.0145,\n",
            "        -0.0136, -0.0292, -0.0686,  0.0233,  0.0156,  0.0088, -0.0647, -0.0087,\n",
            "         0.0593, -0.0402, -0.0604, -0.0589,  0.0024, -0.0509, -0.0633,  0.0308,\n",
            "        -0.0599, -0.0258,  0.0005,  0.0710, -0.0148,  0.0336, -0.0235, -0.0253,\n",
            "         0.0415,  0.0691,  0.0282, -0.0262, -0.0301, -0.0218,  0.0293, -0.0716,\n",
            "        -0.0210, -0.0186, -0.0658,  0.0421,  0.0356, -0.0437,  0.0279, -0.0243,\n",
            "         0.0830,  0.0739, -0.0327,  0.0499, -0.0822,  0.0098, -0.0086, -0.0267,\n",
            "        -0.0454,  0.0272,  0.0064,  0.0039, -0.0390, -0.0519,  0.0133,  0.0003,\n",
            "        -0.0562,  0.0381, -0.0101,  0.0448, -0.0229, -0.0699,  0.0048, -0.0260,\n",
            "        -0.0438,  0.0509, -0.0031,  0.0196,  0.0229,  0.0762, -0.0033, -0.0556,\n",
            "        -0.0255, -0.0775, -0.0652, -0.0615, -0.0302, -0.0515,  0.0306,  0.0516],\n",
            "       requires_grad=True)\n",
            "R3.weight Parameter containing:\n",
            "tensor([[ 0.0728,  0.0634, -0.0851,  ..., -0.0173,  0.0635, -0.0096],\n",
            "        [ 0.0695,  0.0353,  0.0544,  ..., -0.0601,  0.0819,  0.0852],\n",
            "        [ 0.0410,  0.0162, -0.0139,  ...,  0.0188,  0.0815, -0.0678],\n",
            "        ...,\n",
            "        [-0.0355, -0.0588,  0.0513,  ...,  0.0134,  0.0809, -0.0117],\n",
            "        [ 0.0820,  0.0413, -0.0212,  ...,  0.0205,  0.0035,  0.0574],\n",
            "        [-0.0800,  0.0324,  0.0238,  ...,  0.0201,  0.0765, -0.0256]])\n",
            "R3.bias Parameter containing:\n",
            "tensor([ 0.0573,  0.0381,  0.0467, -0.0692,  0.0199,  0.0675,  0.0397, -0.0439,\n",
            "        -0.0200, -0.0772, -0.0392,  0.0871, -0.0134,  0.0094, -0.0437,  0.0715,\n",
            "        -0.0429,  0.0294, -0.0804,  0.0641, -0.0529,  0.0879, -0.0360,  0.0629,\n",
            "         0.0071,  0.0006,  0.0235,  0.0688,  0.0874,  0.0100,  0.0763,  0.0238,\n",
            "        -0.0243,  0.0125, -0.0011, -0.0716, -0.0774,  0.0663,  0.0201,  0.0458,\n",
            "         0.0800,  0.0127, -0.0266,  0.0046,  0.0189, -0.0079, -0.0410,  0.0269,\n",
            "         0.0133, -0.0679, -0.0037,  0.0577,  0.0725, -0.0318, -0.0427, -0.0120,\n",
            "         0.0110,  0.0811,  0.0419, -0.0092, -0.0784,  0.0654,  0.0697, -0.0670,\n",
            "        -0.0823, -0.0359,  0.0493,  0.0328, -0.0095, -0.0232, -0.0042, -0.0780,\n",
            "         0.0858, -0.0413,  0.0284,  0.0490,  0.0089, -0.0216,  0.0066,  0.0644,\n",
            "         0.0606, -0.0649, -0.0426,  0.0389, -0.0759,  0.0461,  0.0293, -0.0265,\n",
            "         0.0508, -0.0564,  0.0829, -0.0451,  0.0474,  0.0059, -0.0330, -0.0101,\n",
            "        -0.0404,  0.0839,  0.0576,  0.0614, -0.0317,  0.0694,  0.0016,  0.0320,\n",
            "        -0.0450,  0.0663,  0.0162,  0.0758, -0.0050, -0.0024,  0.0334, -0.0298,\n",
            "         0.0299, -0.0760, -0.0449,  0.0790, -0.0095, -0.0647,  0.0812, -0.0508,\n",
            "         0.0733, -0.0199,  0.0851,  0.0227,  0.0444,  0.0150,  0.0129,  0.0457])\n",
            "G3.weight Parameter containing:\n",
            "tensor([[ 0.0728,  0.0634, -0.0851,  ..., -0.0173,  0.0635, -0.0096],\n",
            "        [ 0.0695,  0.0353,  0.0544,  ..., -0.0601,  0.0819,  0.0852],\n",
            "        [ 0.0410,  0.0162, -0.0139,  ...,  0.0188,  0.0815, -0.0678],\n",
            "        ...,\n",
            "        [-0.0355, -0.0588,  0.0513,  ...,  0.0134,  0.0809, -0.0117],\n",
            "        [ 0.0820,  0.0413, -0.0212,  ...,  0.0205,  0.0035,  0.0574],\n",
            "        [-0.0800,  0.0324,  0.0238,  ...,  0.0201,  0.0765, -0.0256]],\n",
            "       requires_grad=True)\n",
            "G3.bias Parameter containing:\n",
            "tensor([ 0.0573,  0.0381,  0.0467, -0.0692,  0.0199,  0.0675,  0.0397, -0.0439,\n",
            "        -0.0200, -0.0772, -0.0392,  0.0871, -0.0134,  0.0094, -0.0437,  0.0715,\n",
            "        -0.0429,  0.0294, -0.0804,  0.0641, -0.0529,  0.0879, -0.0360,  0.0629,\n",
            "         0.0071,  0.0006,  0.0235,  0.0688,  0.0874,  0.0100,  0.0763,  0.0238,\n",
            "        -0.0243,  0.0125, -0.0011, -0.0716, -0.0774,  0.0663,  0.0201,  0.0458,\n",
            "         0.0800,  0.0127, -0.0266,  0.0046,  0.0189, -0.0079, -0.0410,  0.0269,\n",
            "         0.0133, -0.0679, -0.0037,  0.0577,  0.0725, -0.0318, -0.0427, -0.0120,\n",
            "         0.0110,  0.0811,  0.0419, -0.0092, -0.0784,  0.0654,  0.0697, -0.0670,\n",
            "        -0.0823, -0.0359,  0.0493,  0.0328, -0.0095, -0.0232, -0.0042, -0.0780,\n",
            "         0.0858, -0.0413,  0.0284,  0.0490,  0.0089, -0.0216,  0.0066,  0.0644,\n",
            "         0.0606, -0.0649, -0.0426,  0.0389, -0.0759,  0.0461,  0.0293, -0.0265,\n",
            "         0.0508, -0.0564,  0.0829, -0.0451,  0.0474,  0.0059, -0.0330, -0.0101,\n",
            "        -0.0404,  0.0839,  0.0576,  0.0614, -0.0317,  0.0694,  0.0016,  0.0320,\n",
            "        -0.0450,  0.0663,  0.0162,  0.0758, -0.0050, -0.0024,  0.0334, -0.0298,\n",
            "         0.0299, -0.0760, -0.0449,  0.0790, -0.0095, -0.0647,  0.0812, -0.0508,\n",
            "         0.0733, -0.0199,  0.0851,  0.0227,  0.0444,  0.0150,  0.0129,  0.0457],\n",
            "       requires_grad=True)\n",
            "R4.weight Parameter containing:\n",
            "tensor([[ 0.0832,  0.0331, -0.0112,  ...,  0.0361,  0.0266, -0.0395],\n",
            "        [-0.0550, -0.0553,  0.0057,  ...,  0.0801, -0.0381, -0.0307],\n",
            "        [-0.0490,  0.0421, -0.0340,  ...,  0.0805,  0.0675,  0.0226],\n",
            "        ...,\n",
            "        [ 0.0277, -0.0866,  0.0390,  ...,  0.0333,  0.0133, -0.0417],\n",
            "        [ 0.0448,  0.0343, -0.0795,  ...,  0.0599,  0.0052, -0.0206],\n",
            "        [-0.0262,  0.0360,  0.0407,  ..., -0.0303,  0.0790, -0.0626]])\n",
            "R4.bias Parameter containing:\n",
            "tensor([ 0.0130, -0.0204, -0.0752,  0.0666,  0.0307,  0.0616, -0.0524,  0.0386,\n",
            "        -0.0594, -0.0614,  0.0305, -0.0478,  0.0732, -0.0842, -0.0010,  0.0172,\n",
            "        -0.0690,  0.0561, -0.0040,  0.0876, -0.0129, -0.0436, -0.0006,  0.0213,\n",
            "        -0.0016,  0.0459,  0.0420,  0.0582,  0.0204, -0.0386,  0.0128, -0.0247,\n",
            "        -0.0552,  0.0579, -0.0740, -0.0711,  0.0676,  0.0771,  0.0217, -0.0739,\n",
            "        -0.0360, -0.0599, -0.0013, -0.0141, -0.0201,  0.0832, -0.0736, -0.0805,\n",
            "        -0.0394,  0.0447,  0.0018,  0.0796,  0.0243,  0.0389,  0.0344, -0.0099,\n",
            "        -0.0647, -0.0635, -0.0396,  0.0351,  0.0861,  0.0548,  0.0523,  0.0875,\n",
            "         0.0037,  0.0362,  0.0652, -0.0049, -0.0606, -0.0080,  0.0445, -0.0093,\n",
            "        -0.0305,  0.0040, -0.0486, -0.0093,  0.0383, -0.0771,  0.0616,  0.0745,\n",
            "        -0.0741, -0.0289,  0.0574,  0.0826,  0.0254, -0.0833,  0.0710,  0.0181,\n",
            "         0.0648, -0.0831, -0.0206,  0.0618,  0.0325,  0.0174, -0.0431, -0.0119,\n",
            "        -0.0660,  0.0738,  0.0806, -0.0440,  0.0050,  0.0480,  0.0218,  0.0381,\n",
            "        -0.0137,  0.0201, -0.0203,  0.0481, -0.0470, -0.0089, -0.0781,  0.0533,\n",
            "        -0.0798, -0.0575, -0.0039, -0.0824, -0.0180, -0.0760,  0.0716,  0.0880,\n",
            "         0.0525,  0.0515,  0.0858,  0.0475, -0.0305,  0.0020,  0.0400, -0.0426])\n",
            "G4.weight Parameter containing:\n",
            "tensor([[ 0.0832,  0.0331, -0.0112,  ...,  0.0361,  0.0266, -0.0395],\n",
            "        [-0.0550, -0.0553,  0.0057,  ...,  0.0801, -0.0381, -0.0307],\n",
            "        [-0.0490,  0.0421, -0.0340,  ...,  0.0805,  0.0675,  0.0226],\n",
            "        ...,\n",
            "        [ 0.0277, -0.0866,  0.0390,  ...,  0.0333,  0.0133, -0.0417],\n",
            "        [ 0.0448,  0.0343, -0.0795,  ...,  0.0599,  0.0052, -0.0206],\n",
            "        [-0.0262,  0.0360,  0.0407,  ..., -0.0303,  0.0790, -0.0626]],\n",
            "       requires_grad=True)\n",
            "G4.bias Parameter containing:\n",
            "tensor([ 0.0130, -0.0204, -0.0752,  0.0666,  0.0307,  0.0616, -0.0524,  0.0386,\n",
            "        -0.0594, -0.0614,  0.0305, -0.0478,  0.0732, -0.0842, -0.0010,  0.0172,\n",
            "        -0.0690,  0.0561, -0.0040,  0.0876, -0.0129, -0.0436, -0.0006,  0.0213,\n",
            "        -0.0016,  0.0459,  0.0420,  0.0582,  0.0204, -0.0386,  0.0128, -0.0247,\n",
            "        -0.0552,  0.0579, -0.0740, -0.0711,  0.0676,  0.0771,  0.0217, -0.0739,\n",
            "        -0.0360, -0.0599, -0.0013, -0.0141, -0.0201,  0.0832, -0.0736, -0.0805,\n",
            "        -0.0394,  0.0447,  0.0018,  0.0796,  0.0243,  0.0389,  0.0344, -0.0099,\n",
            "        -0.0647, -0.0635, -0.0396,  0.0351,  0.0861,  0.0548,  0.0523,  0.0875,\n",
            "         0.0037,  0.0362,  0.0652, -0.0049, -0.0606, -0.0080,  0.0445, -0.0093,\n",
            "        -0.0305,  0.0040, -0.0486, -0.0093,  0.0383, -0.0771,  0.0616,  0.0745,\n",
            "        -0.0741, -0.0289,  0.0574,  0.0826,  0.0254, -0.0833,  0.0710,  0.0181,\n",
            "         0.0648, -0.0831, -0.0206,  0.0618,  0.0325,  0.0174, -0.0431, -0.0119,\n",
            "        -0.0660,  0.0738,  0.0806, -0.0440,  0.0050,  0.0480,  0.0218,  0.0381,\n",
            "        -0.0137,  0.0201, -0.0203,  0.0481, -0.0470, -0.0089, -0.0781,  0.0533,\n",
            "        -0.0798, -0.0575, -0.0039, -0.0824, -0.0180, -0.0760,  0.0716,  0.0880,\n",
            "         0.0525,  0.0515,  0.0858,  0.0475, -0.0305,  0.0020,  0.0400, -0.0426],\n",
            "       requires_grad=True)\n",
            "R5.weight Parameter containing:\n",
            "tensor([[ 0.0663,  0.0347, -0.0383,  ..., -0.0211, -0.0676,  0.0302],\n",
            "        [ 0.0069,  0.0053, -0.0306,  ...,  0.0530, -0.0525, -0.0880],\n",
            "        [ 0.0558,  0.0761, -0.0759,  ...,  0.0109,  0.0790,  0.0699],\n",
            "        ...,\n",
            "        [-0.0201, -0.0467,  0.0670,  ...,  0.0717, -0.0129, -0.0816],\n",
            "        [ 0.0455,  0.0880, -0.0735,  ...,  0.0788, -0.0324, -0.0152],\n",
            "        [-0.0726,  0.0672,  0.0720,  ...,  0.0314, -0.0283,  0.0398]])\n",
            "R5.bias Parameter containing:\n",
            "tensor([-0.0823,  0.0381,  0.0634,  0.0349, -0.0072,  0.0727,  0.0679,  0.0771,\n",
            "         0.0332, -0.0326, -0.0126,  0.0618, -0.0709, -0.0457,  0.0434, -0.0536,\n",
            "        -0.0538, -0.0014, -0.0772,  0.0864, -0.0667,  0.0215, -0.0621, -0.0189,\n",
            "         0.0569,  0.0348, -0.0016, -0.0048, -0.0607,  0.0377,  0.0661,  0.0330,\n",
            "        -0.0605,  0.0780, -0.0382,  0.0260, -0.0248, -0.0495,  0.0821, -0.0232,\n",
            "        -0.0088, -0.0091, -0.0759, -0.0629, -0.0523, -0.0842,  0.0826, -0.0282,\n",
            "         0.0733, -0.0810, -0.0458,  0.0686,  0.0065, -0.0809, -0.0491, -0.0757,\n",
            "        -0.0757,  0.0091,  0.0521,  0.0756,  0.0425,  0.0072, -0.0492, -0.0276,\n",
            "         0.0048, -0.0806, -0.0145,  0.0539,  0.0596, -0.0134,  0.0793, -0.0237,\n",
            "         0.0112, -0.0196,  0.0578,  0.0169,  0.0326,  0.0606,  0.0417,  0.0776,\n",
            "        -0.0255,  0.0445, -0.0146, -0.0276,  0.0289,  0.0422, -0.0216, -0.0338,\n",
            "        -0.0789, -0.0757,  0.0258,  0.0117, -0.0864,  0.0410,  0.0160,  0.0052,\n",
            "         0.0644,  0.0052, -0.0558,  0.0389, -0.0788,  0.0085,  0.0037,  0.0748,\n",
            "         0.0100,  0.0235, -0.0621,  0.0627, -0.0503, -0.0803,  0.0483,  0.0018,\n",
            "        -0.0380,  0.0362, -0.0212, -0.0592, -0.0531, -0.0614,  0.0416,  0.0624,\n",
            "        -0.0785, -0.0765,  0.0027, -0.0699, -0.0353, -0.0435, -0.0784,  0.0520])\n",
            "G5.weight Parameter containing:\n",
            "tensor([[ 0.0663,  0.0347, -0.0383,  ..., -0.0211, -0.0676,  0.0302],\n",
            "        [ 0.0069,  0.0053, -0.0306,  ...,  0.0530, -0.0525, -0.0880],\n",
            "        [ 0.0558,  0.0761, -0.0759,  ...,  0.0109,  0.0790,  0.0699],\n",
            "        ...,\n",
            "        [-0.0201, -0.0467,  0.0670,  ...,  0.0717, -0.0129, -0.0816],\n",
            "        [ 0.0455,  0.0880, -0.0735,  ...,  0.0788, -0.0324, -0.0152],\n",
            "        [-0.0726,  0.0672,  0.0720,  ...,  0.0314, -0.0283,  0.0398]],\n",
            "       requires_grad=True)\n",
            "G5.bias Parameter containing:\n",
            "tensor([-0.0823,  0.0381,  0.0634,  0.0349, -0.0072,  0.0727,  0.0679,  0.0771,\n",
            "         0.0332, -0.0326, -0.0126,  0.0618, -0.0709, -0.0457,  0.0434, -0.0536,\n",
            "        -0.0538, -0.0014, -0.0772,  0.0864, -0.0667,  0.0215, -0.0621, -0.0189,\n",
            "         0.0569,  0.0348, -0.0016, -0.0048, -0.0607,  0.0377,  0.0661,  0.0330,\n",
            "        -0.0605,  0.0780, -0.0382,  0.0260, -0.0248, -0.0495,  0.0821, -0.0232,\n",
            "        -0.0088, -0.0091, -0.0759, -0.0629, -0.0523, -0.0842,  0.0826, -0.0282,\n",
            "         0.0733, -0.0810, -0.0458,  0.0686,  0.0065, -0.0809, -0.0491, -0.0757,\n",
            "        -0.0757,  0.0091,  0.0521,  0.0756,  0.0425,  0.0072, -0.0492, -0.0276,\n",
            "         0.0048, -0.0806, -0.0145,  0.0539,  0.0596, -0.0134,  0.0793, -0.0237,\n",
            "         0.0112, -0.0196,  0.0578,  0.0169,  0.0326,  0.0606,  0.0417,  0.0776,\n",
            "        -0.0255,  0.0445, -0.0146, -0.0276,  0.0289,  0.0422, -0.0216, -0.0338,\n",
            "        -0.0789, -0.0757,  0.0258,  0.0117, -0.0864,  0.0410,  0.0160,  0.0052,\n",
            "         0.0644,  0.0052, -0.0558,  0.0389, -0.0788,  0.0085,  0.0037,  0.0748,\n",
            "         0.0100,  0.0235, -0.0621,  0.0627, -0.0503, -0.0803,  0.0483,  0.0018,\n",
            "        -0.0380,  0.0362, -0.0212, -0.0592, -0.0531, -0.0614,  0.0416,  0.0624,\n",
            "        -0.0785, -0.0765,  0.0027, -0.0699, -0.0353, -0.0435, -0.0784,  0.0520],\n",
            "       requires_grad=True)\n",
            "outputs.weight Parameter containing:\n",
            "tensor([[ 1.5955e-02,  2.0806e-02,  1.8439e-02,  ..., -1.2642e-02,\n",
            "         -4.2856e-02, -3.3964e-02],\n",
            "        [-6.9004e-02, -5.0802e-02, -5.7926e-02,  ..., -2.1648e-02,\n",
            "          4.6429e-02, -3.6011e-02],\n",
            "        [-6.1387e-02, -6.5814e-04, -6.1413e-02,  ...,  8.2825e-02,\n",
            "         -3.2423e-02,  5.1470e-02],\n",
            "        ...,\n",
            "        [ 4.5572e-02, -5.2368e-02,  5.2782e-02,  ...,  7.1816e-02,\n",
            "         -3.5513e-02, -5.1322e-02],\n",
            "        [-6.3474e-02,  2.8514e-02, -6.5065e-02,  ..., -6.7556e-02,\n",
            "          1.8747e-02,  8.5370e-02],\n",
            "        [ 6.0564e-03, -9.1543e-05, -3.4080e-02,  ..., -4.5288e-02,\n",
            "         -4.2643e-02, -5.8453e-02]], requires_grad=True)\n",
            "outputs.bias Parameter containing:\n",
            "tensor([-0.0320,  0.0868, -0.0496, -0.0566,  0.0370, -0.0365, -0.0181,  0.0722,\n",
            "         0.0238, -0.0369], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "      print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71TsXOlm54O5",
        "outputId": "7041704b-dd7f-48ed-f2f4-d2ea83c43f49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G1.weight\n",
            "G1.bias\n",
            "G2.weight\n",
            "G2.bias\n",
            "G3.weight\n",
            "G3.bias\n",
            "G4.weight\n",
            "G4.bias\n",
            "G5.weight\n",
            "G5.bias\n",
            "outputs.weight\n",
            "outputs.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lossFn = nn.CrossEntropyLoss()\n",
        "\n",
        "if args['cuda']:\n",
        "    model.cuda()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
        "\n",
        "for epoch in range(1, args['epochs'] + 1):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18MWWpd76Ta4",
        "outputId": "a34c0522-71a4-4728-9b8c-c7b9becd3662"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [59968/60000 (100%)]\tLoss: 0.059813\n",
            "\n",
            "Test set: Average loss: 86.2581, Accuracy: 9186/10000 (92%)\n",
            "\n",
            "Train Epoch: 2 [59968/60000 (100%)]\tLoss: 0.320187\n",
            "\n",
            "Test set: Average loss: 60.0275, Accuracy: 9408/10000 (94%)\n",
            "\n",
            "Train Epoch: 3 [59968/60000 (100%)]\tLoss: 0.033653\n",
            "\n",
            "Test set: Average loss: 47.8090, Accuracy: 9542/10000 (95%)\n",
            "\n",
            "Train Epoch: 4 [59968/60000 (100%)]\tLoss: 0.046542\n",
            "\n",
            "Test set: Average loss: 47.8092, Accuracy: 9550/10000 (96%)\n",
            "\n",
            "Train Epoch: 5 [59968/60000 (100%)]\tLoss: 0.009930\n",
            "\n",
            "Test set: Average loss: 36.7361, Accuracy: 9636/10000 (96%)\n",
            "\n",
            "Train Epoch: 6 [59968/60000 (100%)]\tLoss: 0.011075\n",
            "\n",
            "Test set: Average loss: 31.5736, Accuracy: 9695/10000 (97%)\n",
            "\n",
            "Train Epoch: 7 [59968/60000 (100%)]\tLoss: 0.063425\n",
            "\n",
            "Test set: Average loss: 33.4118, Accuracy: 9666/10000 (97%)\n",
            "\n",
            "Train Epoch: 8 [59968/60000 (100%)]\tLoss: 0.101278\n",
            "\n",
            "Test set: Average loss: 29.8511, Accuracy: 9717/10000 (97%)\n",
            "\n",
            "Train Epoch: 9 [59968/60000 (100%)]\tLoss: 0.015318\n",
            "\n",
            "Test set: Average loss: 27.8059, Accuracy: 9731/10000 (97%)\n",
            "\n",
            "Train Epoch: 10 [59968/60000 (100%)]\tLoss: 0.028132\n",
            "\n",
            "Test set: Average loss: 27.7840, Accuracy: 9729/10000 (97%)\n",
            "\n",
            "Train Epoch: 11 [59968/60000 (100%)]\tLoss: 0.004768\n",
            "\n",
            "Test set: Average loss: 28.2188, Accuracy: 9711/10000 (97%)\n",
            "\n",
            "Train Epoch: 12 [59968/60000 (100%)]\tLoss: 0.013916\n",
            "\n",
            "Test set: Average loss: 28.4380, Accuracy: 9730/10000 (97%)\n",
            "\n",
            "Train Epoch: 13 [59968/60000 (100%)]\tLoss: 0.061564\n",
            "\n",
            "Test set: Average loss: 26.5601, Accuracy: 9764/10000 (98%)\n",
            "\n",
            "Train Epoch: 14 [59968/60000 (100%)]\tLoss: 0.127838\n",
            "\n",
            "Test set: Average loss: 30.8478, Accuracy: 9731/10000 (97%)\n",
            "\n",
            "Train Epoch: 15 [59968/60000 (100%)]\tLoss: 0.000809\n",
            "\n",
            "Test set: Average loss: 27.8115, Accuracy: 9749/10000 (97%)\n",
            "\n",
            "Train Epoch: 16 [59968/60000 (100%)]\tLoss: 0.009303\n",
            "\n",
            "Test set: Average loss: 32.0959, Accuracy: 9733/10000 (97%)\n",
            "\n",
            "Train Epoch: 17 [59968/60000 (100%)]\tLoss: 0.001447\n",
            "\n",
            "Test set: Average loss: 30.9775, Accuracy: 9747/10000 (97%)\n",
            "\n",
            "Train Epoch: 18 [59968/60000 (100%)]\tLoss: 0.000097\n",
            "\n",
            "Test set: Average loss: 29.6775, Accuracy: 9769/10000 (98%)\n",
            "\n",
            "Train Epoch: 19 [59968/60000 (100%)]\tLoss: 0.002828\n",
            "\n",
            "Test set: Average loss: 31.5533, Accuracy: 9757/10000 (98%)\n",
            "\n",
            "Train Epoch: 20 [59968/60000 (100%)]\tLoss: 0.001767\n",
            "\n",
            "Test set: Average loss: 31.9509, Accuracy: 9777/10000 (98%)\n",
            "\n",
            "Train Epoch: 21 [59968/60000 (100%)]\tLoss: 0.010458\n",
            "\n",
            "Test set: Average loss: 34.9319, Accuracy: 9741/10000 (97%)\n",
            "\n",
            "Train Epoch: 22 [59968/60000 (100%)]\tLoss: 0.001837\n",
            "\n",
            "Test set: Average loss: 35.9230, Accuracy: 9724/10000 (97%)\n",
            "\n",
            "Train Epoch: 23 [59968/60000 (100%)]\tLoss: 0.029252\n",
            "\n",
            "Test set: Average loss: 31.9186, Accuracy: 9752/10000 (98%)\n",
            "\n",
            "Train Epoch: 24 [59968/60000 (100%)]\tLoss: 0.000034\n",
            "\n",
            "Test set: Average loss: 33.8856, Accuracy: 9774/10000 (98%)\n",
            "\n",
            "Train Epoch: 25 [59968/60000 (100%)]\tLoss: 0.000503\n",
            "\n",
            "Test set: Average loss: 33.6857, Accuracy: 9769/10000 (98%)\n",
            "\n",
            "Train Epoch: 26 [59968/60000 (100%)]\tLoss: 0.000382\n",
            "\n",
            "Test set: Average loss: 39.6237, Accuracy: 9735/10000 (97%)\n",
            "\n",
            "Train Epoch: 27 [59968/60000 (100%)]\tLoss: 0.000005\n",
            "\n",
            "Test set: Average loss: 35.5176, Accuracy: 9769/10000 (98%)\n",
            "\n",
            "Train Epoch: 28 [59968/60000 (100%)]\tLoss: 0.004059\n",
            "\n",
            "Test set: Average loss: 36.1655, Accuracy: 9766/10000 (98%)\n",
            "\n",
            "Train Epoch: 29 [59968/60000 (100%)]\tLoss: 0.009230\n",
            "\n",
            "Test set: Average loss: 35.8889, Accuracy: 9768/10000 (98%)\n",
            "\n",
            "Train Epoch: 30 [59968/60000 (100%)]\tLoss: 0.003842\n",
            "\n",
            "Test set: Average loss: 35.7308, Accuracy: 9762/10000 (98%)\n",
            "\n",
            "Train Epoch: 31 [59968/60000 (100%)]\tLoss: 0.000918\n",
            "\n",
            "Test set: Average loss: 49.7461, Accuracy: 9707/10000 (97%)\n",
            "\n",
            "Train Epoch: 32 [59968/60000 (100%)]\tLoss: 0.000478\n",
            "\n",
            "Test set: Average loss: 39.8962, Accuracy: 9774/10000 (98%)\n",
            "\n",
            "Train Epoch: 33 [59968/60000 (100%)]\tLoss: 0.000710\n",
            "\n",
            "Test set: Average loss: 39.3894, Accuracy: 9756/10000 (98%)\n",
            "\n",
            "Train Epoch: 34 [59968/60000 (100%)]\tLoss: 0.000827\n",
            "\n",
            "Test set: Average loss: 37.9636, Accuracy: 9766/10000 (98%)\n",
            "\n",
            "Train Epoch: 35 [59968/60000 (100%)]\tLoss: 0.001009\n",
            "\n",
            "Test set: Average loss: 40.3518, Accuracy: 9760/10000 (98%)\n",
            "\n",
            "Train Epoch: 36 [59968/60000 (100%)]\tLoss: 0.001309\n",
            "\n",
            "Test set: Average loss: 45.4215, Accuracy: 9737/10000 (97%)\n",
            "\n",
            "Train Epoch: 37 [59968/60000 (100%)]\tLoss: 0.000390\n",
            "\n",
            "Test set: Average loss: 38.2759, Accuracy: 9771/10000 (98%)\n",
            "\n",
            "Train Epoch: 38 [59968/60000 (100%)]\tLoss: 0.001284\n",
            "\n",
            "Test set: Average loss: 39.8840, Accuracy: 9767/10000 (98%)\n",
            "\n",
            "Train Epoch: 39 [59968/60000 (100%)]\tLoss: 0.000028\n",
            "\n",
            "Test set: Average loss: 39.9399, Accuracy: 9774/10000 (98%)\n",
            "\n",
            "Train Epoch: 40 [59968/60000 (100%)]\tLoss: 0.001305\n",
            "\n",
            "Test set: Average loss: 43.9484, Accuracy: 9760/10000 (98%)\n",
            "\n",
            "Train Epoch: 41 [59968/60000 (100%)]\tLoss: 0.000084\n",
            "\n",
            "Test set: Average loss: 41.2918, Accuracy: 9768/10000 (98%)\n",
            "\n",
            "Train Epoch: 42 [59968/60000 (100%)]\tLoss: 0.000002\n",
            "\n",
            "Test set: Average loss: 46.9404, Accuracy: 9742/10000 (97%)\n",
            "\n",
            "Train Epoch: 43 [59968/60000 (100%)]\tLoss: 0.000045\n",
            "\n",
            "Test set: Average loss: 43.8581, Accuracy: 9755/10000 (98%)\n",
            "\n",
            "Train Epoch: 44 [59968/60000 (100%)]\tLoss: 0.002132\n",
            "\n",
            "Test set: Average loss: 47.6282, Accuracy: 9758/10000 (98%)\n",
            "\n",
            "Train Epoch: 45 [59968/60000 (100%)]\tLoss: 0.004349\n",
            "\n",
            "Test set: Average loss: 40.9793, Accuracy: 9781/10000 (98%)\n",
            "\n",
            "Train Epoch: 46 [59968/60000 (100%)]\tLoss: 0.003586\n",
            "\n",
            "Test set: Average loss: 48.4030, Accuracy: 9749/10000 (97%)\n",
            "\n",
            "Train Epoch: 47 [59968/60000 (100%)]\tLoss: 0.000002\n",
            "\n",
            "Test set: Average loss: 44.2936, Accuracy: 9775/10000 (98%)\n",
            "\n",
            "Train Epoch: 48 [59968/60000 (100%)]\tLoss: 0.000161\n",
            "\n",
            "Test set: Average loss: 44.7904, Accuracy: 9750/10000 (98%)\n",
            "\n",
            "Train Epoch: 49 [59968/60000 (100%)]\tLoss: 0.000086\n",
            "\n",
            "Test set: Average loss: 43.7394, Accuracy: 9765/10000 (98%)\n",
            "\n",
            "Train Epoch: 50 [59968/60000 (100%)]\tLoss: 0.004671\n",
            "\n",
            "Test set: Average loss: 44.5514, Accuracy: 9767/10000 (98%)\n",
            "\n",
            "Train Epoch: 51 [59968/60000 (100%)]\tLoss: 0.001253\n",
            "\n",
            "Test set: Average loss: 46.0943, Accuracy: 9750/10000 (98%)\n",
            "\n",
            "Train Epoch: 52 [59968/60000 (100%)]\tLoss: 0.000058\n",
            "\n",
            "Test set: Average loss: 46.8234, Accuracy: 9750/10000 (98%)\n",
            "\n",
            "Train Epoch: 53 [59968/60000 (100%)]\tLoss: 0.000097\n",
            "\n",
            "Test set: Average loss: 40.4076, Accuracy: 9777/10000 (98%)\n",
            "\n",
            "Train Epoch: 54 [59968/60000 (100%)]\tLoss: 0.003278\n",
            "\n",
            "Test set: Average loss: 44.1614, Accuracy: 9775/10000 (98%)\n",
            "\n",
            "Train Epoch: 55 [59968/60000 (100%)]\tLoss: 0.000532\n",
            "\n",
            "Test set: Average loss: 45.1231, Accuracy: 9770/10000 (98%)\n",
            "\n",
            "Train Epoch: 56 [59968/60000 (100%)]\tLoss: 0.000004\n",
            "\n",
            "Test set: Average loss: 44.1310, Accuracy: 9763/10000 (98%)\n",
            "\n",
            "Train Epoch: 57 [59968/60000 (100%)]\tLoss: 0.000079\n",
            "\n",
            "Test set: Average loss: 45.5619, Accuracy: 9766/10000 (98%)\n",
            "\n",
            "Train Epoch: 58 [59968/60000 (100%)]\tLoss: 0.000003\n",
            "\n",
            "Test set: Average loss: 52.4132, Accuracy: 9741/10000 (97%)\n",
            "\n",
            "Train Epoch: 59 [59968/60000 (100%)]\tLoss: 0.000018\n",
            "\n",
            "Test set: Average loss: 45.7186, Accuracy: 9776/10000 (98%)\n",
            "\n",
            "Train Epoch: 60 [59968/60000 (100%)]\tLoss: 0.000689\n",
            "\n",
            "Test set: Average loss: 47.8898, Accuracy: 9756/10000 (98%)\n",
            "\n",
            "Train Epoch: 61 [59968/60000 (100%)]\tLoss: 0.000025\n",
            "\n",
            "Test set: Average loss: 45.0564, Accuracy: 9785/10000 (98%)\n",
            "\n",
            "Train Epoch: 62 [59968/60000 (100%)]\tLoss: 0.000092\n",
            "\n",
            "Test set: Average loss: 48.8406, Accuracy: 9735/10000 (97%)\n",
            "\n",
            "Train Epoch: 63 [59968/60000 (100%)]\tLoss: 0.006096\n",
            "\n",
            "Test set: Average loss: 47.0695, Accuracy: 9767/10000 (98%)\n",
            "\n",
            "Train Epoch: 64 [59968/60000 (100%)]\tLoss: 0.000025\n",
            "\n",
            "Test set: Average loss: 47.2520, Accuracy: 9757/10000 (98%)\n",
            "\n",
            "Train Epoch: 65 [59968/60000 (100%)]\tLoss: 0.000019\n",
            "\n",
            "Test set: Average loss: 49.7387, Accuracy: 9762/10000 (98%)\n",
            "\n",
            "Train Epoch: 66 [59968/60000 (100%)]\tLoss: 0.084705\n",
            "\n",
            "Test set: Average loss: 49.9374, Accuracy: 9764/10000 (98%)\n",
            "\n",
            "Train Epoch: 67 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 42.3064, Accuracy: 9777/10000 (98%)\n",
            "\n",
            "Train Epoch: 68 [59968/60000 (100%)]\tLoss: 0.000700\n",
            "\n",
            "Test set: Average loss: 49.1475, Accuracy: 9761/10000 (98%)\n",
            "\n",
            "Train Epoch: 69 [59968/60000 (100%)]\tLoss: 0.002732\n",
            "\n",
            "Test set: Average loss: 50.6228, Accuracy: 9752/10000 (98%)\n",
            "\n",
            "Train Epoch: 70 [59968/60000 (100%)]\tLoss: 0.000119\n",
            "\n",
            "Test set: Average loss: 44.9251, Accuracy: 9762/10000 (98%)\n",
            "\n",
            "Train Epoch: 71 [59968/60000 (100%)]\tLoss: 0.000006\n",
            "\n",
            "Test set: Average loss: 42.4062, Accuracy: 9782/10000 (98%)\n",
            "\n",
            "Train Epoch: 72 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 46.5248, Accuracy: 9773/10000 (98%)\n",
            "\n",
            "Train Epoch: 73 [59968/60000 (100%)]\tLoss: 0.000021\n",
            "\n",
            "Test set: Average loss: 46.4288, Accuracy: 9774/10000 (98%)\n",
            "\n",
            "Train Epoch: 74 [59968/60000 (100%)]\tLoss: 0.000243\n",
            "\n",
            "Test set: Average loss: 44.6913, Accuracy: 9765/10000 (98%)\n",
            "\n",
            "Train Epoch: 75 [59968/60000 (100%)]\tLoss: 0.000006\n",
            "\n",
            "Test set: Average loss: 43.9487, Accuracy: 9782/10000 (98%)\n",
            "\n",
            "Train Epoch: 76 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 43.5997, Accuracy: 9783/10000 (98%)\n",
            "\n",
            "Train Epoch: 77 [59968/60000 (100%)]\tLoss: 0.000421\n",
            "\n",
            "Test set: Average loss: 49.3007, Accuracy: 9771/10000 (98%)\n",
            "\n",
            "Train Epoch: 78 [59968/60000 (100%)]\tLoss: 0.000232\n",
            "\n",
            "Test set: Average loss: 39.5263, Accuracy: 9796/10000 (98%)\n",
            "\n",
            "Train Epoch: 79 [59968/60000 (100%)]\tLoss: 0.005090\n",
            "\n",
            "Test set: Average loss: 45.5707, Accuracy: 9782/10000 (98%)\n",
            "\n",
            "Train Epoch: 80 [59968/60000 (100%)]\tLoss: 0.025837\n",
            "\n",
            "Test set: Average loss: 50.0096, Accuracy: 9762/10000 (98%)\n",
            "\n",
            "Train Epoch: 81 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 42.5961, Accuracy: 9799/10000 (98%)\n",
            "\n",
            "Train Epoch: 82 [59968/60000 (100%)]\tLoss: 0.000005\n",
            "\n",
            "Test set: Average loss: 43.6899, Accuracy: 9787/10000 (98%)\n",
            "\n",
            "Train Epoch: 83 [59968/60000 (100%)]\tLoss: 0.000007\n",
            "\n",
            "Test set: Average loss: 44.2281, Accuracy: 9798/10000 (98%)\n",
            "\n",
            "Train Epoch: 84 [59968/60000 (100%)]\tLoss: 0.000011\n",
            "\n",
            "Test set: Average loss: 45.7270, Accuracy: 9797/10000 (98%)\n",
            "\n",
            "Train Epoch: 85 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 48.8308, Accuracy: 9800/10000 (98%)\n",
            "\n",
            "Train Epoch: 86 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 51.3120, Accuracy: 9806/10000 (98%)\n",
            "\n",
            "Train Epoch: 87 [59968/60000 (100%)]\tLoss: 0.000002\n",
            "\n",
            "Test set: Average loss: 56.0717, Accuracy: 9801/10000 (98%)\n",
            "\n",
            "Train Epoch: 88 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 50.0791, Accuracy: 9778/10000 (98%)\n",
            "\n",
            "Train Epoch: 89 [59968/60000 (100%)]\tLoss: 0.005687\n",
            "\n",
            "Test set: Average loss: 47.8759, Accuracy: 9789/10000 (98%)\n",
            "\n",
            "Train Epoch: 90 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 52.0838, Accuracy: 9774/10000 (98%)\n",
            "\n",
            "Train Epoch: 91 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 42.6145, Accuracy: 9782/10000 (98%)\n",
            "\n",
            "Train Epoch: 92 [59968/60000 (100%)]\tLoss: 0.000017\n",
            "\n",
            "Test set: Average loss: 49.3121, Accuracy: 9769/10000 (98%)\n",
            "\n",
            "Train Epoch: 93 [59968/60000 (100%)]\tLoss: 0.000002\n",
            "\n",
            "Test set: Average loss: 45.9934, Accuracy: 9776/10000 (98%)\n",
            "\n",
            "Train Epoch: 94 [59968/60000 (100%)]\tLoss: 0.005951\n",
            "\n",
            "Test set: Average loss: 51.2278, Accuracy: 9776/10000 (98%)\n",
            "\n",
            "Train Epoch: 95 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 45.3882, Accuracy: 9784/10000 (98%)\n",
            "\n",
            "Train Epoch: 96 [59968/60000 (100%)]\tLoss: 0.000009\n",
            "\n",
            "Test set: Average loss: 46.5663, Accuracy: 9777/10000 (98%)\n",
            "\n",
            "Train Epoch: 97 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 48.2562, Accuracy: 9787/10000 (98%)\n",
            "\n",
            "Train Epoch: 98 [59968/60000 (100%)]\tLoss: 0.000041\n",
            "\n",
            "Test set: Average loss: 46.0858, Accuracy: 9770/10000 (98%)\n",
            "\n",
            "Train Epoch: 99 [59968/60000 (100%)]\tLoss: 0.037831\n",
            "\n",
            "Test set: Average loss: 49.3194, Accuracy: 9773/10000 (98%)\n",
            "\n",
            "Train Epoch: 100 [59968/60000 (100%)]\tLoss: 0.000038\n",
            "\n",
            "Test set: Average loss: 54.6576, Accuracy: 9753/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FLNPF"
      ],
      "metadata": {
        "id": "wqhBvJvhGYby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Relu(6, 128)\n",
        "lossFn = nn.CrossEntropyLoss()\n",
        "\n",
        "if args['cuda']:\n",
        "    model.cuda()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
        "\n",
        "for epoch in range(1, args['epochs'] + 1):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8HfeM_H-WyT",
        "outputId": "d7ebb9bf-67bd-46a5-ac37-35805018f277"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [59968/60000 (100%)]\tLoss: 0.234286\n",
            "\n",
            "Test set: Average loss: 80.1784, Accuracy: 9221/10000 (92%)\n",
            "\n",
            "Train Epoch: 2 [59968/60000 (100%)]\tLoss: 0.513418\n",
            "\n",
            "Test set: Average loss: 55.7940, Accuracy: 9456/10000 (95%)\n",
            "\n",
            "Train Epoch: 3 [59968/60000 (100%)]\tLoss: 0.068985\n",
            "\n",
            "Test set: Average loss: 47.8331, Accuracy: 9529/10000 (95%)\n",
            "\n",
            "Train Epoch: 4 [59968/60000 (100%)]\tLoss: 0.030178\n",
            "\n",
            "Test set: Average loss: 42.0575, Accuracy: 9590/10000 (96%)\n",
            "\n",
            "Train Epoch: 5 [59968/60000 (100%)]\tLoss: 0.212829\n",
            "\n",
            "Test set: Average loss: 37.2943, Accuracy: 9646/10000 (96%)\n",
            "\n",
            "Train Epoch: 6 [59968/60000 (100%)]\tLoss: 0.204465\n",
            "\n",
            "Test set: Average loss: 35.7969, Accuracy: 9655/10000 (97%)\n",
            "\n",
            "Train Epoch: 7 [59968/60000 (100%)]\tLoss: 0.159987\n",
            "\n",
            "Test set: Average loss: 29.9535, Accuracy: 9701/10000 (97%)\n",
            "\n",
            "Train Epoch: 8 [59968/60000 (100%)]\tLoss: 0.011684\n",
            "\n",
            "Test set: Average loss: 29.3101, Accuracy: 9700/10000 (97%)\n",
            "\n",
            "Train Epoch: 9 [59968/60000 (100%)]\tLoss: 0.091108\n",
            "\n",
            "Test set: Average loss: 33.3716, Accuracy: 9686/10000 (97%)\n",
            "\n",
            "Train Epoch: 10 [59968/60000 (100%)]\tLoss: 0.013546\n",
            "\n",
            "Test set: Average loss: 29.0407, Accuracy: 9720/10000 (97%)\n",
            "\n",
            "Train Epoch: 11 [59968/60000 (100%)]\tLoss: 0.025746\n",
            "\n",
            "Test set: Average loss: 31.0041, Accuracy: 9715/10000 (97%)\n",
            "\n",
            "Train Epoch: 12 [59968/60000 (100%)]\tLoss: 0.044204\n",
            "\n",
            "Test set: Average loss: 27.4407, Accuracy: 9743/10000 (97%)\n",
            "\n",
            "Train Epoch: 13 [59968/60000 (100%)]\tLoss: 0.039107\n",
            "\n",
            "Test set: Average loss: 26.9859, Accuracy: 9744/10000 (97%)\n",
            "\n",
            "Train Epoch: 14 [59968/60000 (100%)]\tLoss: 0.096523\n",
            "\n",
            "Test set: Average loss: 29.1395, Accuracy: 9744/10000 (97%)\n",
            "\n",
            "Train Epoch: 15 [59968/60000 (100%)]\tLoss: 0.004246\n",
            "\n",
            "Test set: Average loss: 28.6023, Accuracy: 9748/10000 (97%)\n",
            "\n",
            "Train Epoch: 16 [59968/60000 (100%)]\tLoss: 0.024749\n",
            "\n",
            "Test set: Average loss: 31.9790, Accuracy: 9715/10000 (97%)\n",
            "\n",
            "Train Epoch: 17 [59968/60000 (100%)]\tLoss: 0.009419\n",
            "\n",
            "Test set: Average loss: 28.9695, Accuracy: 9780/10000 (98%)\n",
            "\n",
            "Train Epoch: 18 [59968/60000 (100%)]\tLoss: 0.003777\n",
            "\n",
            "Test set: Average loss: 31.4390, Accuracy: 9741/10000 (97%)\n",
            "\n",
            "Train Epoch: 19 [59968/60000 (100%)]\tLoss: 0.006911\n",
            "\n",
            "Test set: Average loss: 32.2564, Accuracy: 9752/10000 (98%)\n",
            "\n",
            "Train Epoch: 20 [59968/60000 (100%)]\tLoss: 0.036565\n",
            "\n",
            "Test set: Average loss: 33.9818, Accuracy: 9747/10000 (97%)\n",
            "\n",
            "Train Epoch: 21 [59968/60000 (100%)]\tLoss: 0.009898\n",
            "\n",
            "Test set: Average loss: 33.8948, Accuracy: 9744/10000 (97%)\n",
            "\n",
            "Train Epoch: 22 [59968/60000 (100%)]\tLoss: 0.007612\n",
            "\n",
            "Test set: Average loss: 32.9427, Accuracy: 9766/10000 (98%)\n",
            "\n",
            "Train Epoch: 23 [59968/60000 (100%)]\tLoss: 0.000038\n",
            "\n",
            "Test set: Average loss: 36.0798, Accuracy: 9730/10000 (97%)\n",
            "\n",
            "Train Epoch: 24 [59968/60000 (100%)]\tLoss: 0.001282\n",
            "\n",
            "Test set: Average loss: 43.4959, Accuracy: 9705/10000 (97%)\n",
            "\n",
            "Train Epoch: 25 [59968/60000 (100%)]\tLoss: 0.013406\n",
            "\n",
            "Test set: Average loss: 39.0962, Accuracy: 9725/10000 (97%)\n",
            "\n",
            "Train Epoch: 26 [59968/60000 (100%)]\tLoss: 0.000019\n",
            "\n",
            "Test set: Average loss: 39.5098, Accuracy: 9747/10000 (97%)\n",
            "\n",
            "Train Epoch: 27 [59968/60000 (100%)]\tLoss: 0.013191\n",
            "\n",
            "Test set: Average loss: 39.8701, Accuracy: 9746/10000 (97%)\n",
            "\n",
            "Train Epoch: 28 [59968/60000 (100%)]\tLoss: 0.000214\n",
            "\n",
            "Test set: Average loss: 36.5872, Accuracy: 9779/10000 (98%)\n",
            "\n",
            "Train Epoch: 29 [59968/60000 (100%)]\tLoss: 0.004174\n",
            "\n",
            "Test set: Average loss: 35.6150, Accuracy: 9765/10000 (98%)\n",
            "\n",
            "Train Epoch: 30 [59968/60000 (100%)]\tLoss: 0.023740\n",
            "\n",
            "Test set: Average loss: 39.8156, Accuracy: 9767/10000 (98%)\n",
            "\n",
            "Train Epoch: 31 [59968/60000 (100%)]\tLoss: 0.000258\n",
            "\n",
            "Test set: Average loss: 40.3431, Accuracy: 9769/10000 (98%)\n",
            "\n",
            "Train Epoch: 32 [59968/60000 (100%)]\tLoss: 0.000790\n",
            "\n",
            "Test set: Average loss: 36.8238, Accuracy: 9772/10000 (98%)\n",
            "\n",
            "Train Epoch: 33 [59968/60000 (100%)]\tLoss: 0.015975\n",
            "\n",
            "Test set: Average loss: 35.9631, Accuracy: 9778/10000 (98%)\n",
            "\n",
            "Train Epoch: 34 [59968/60000 (100%)]\tLoss: 0.000101\n",
            "\n",
            "Test set: Average loss: 38.7486, Accuracy: 9778/10000 (98%)\n",
            "\n",
            "Train Epoch: 35 [59968/60000 (100%)]\tLoss: 0.000005\n",
            "\n",
            "Test set: Average loss: 40.3505, Accuracy: 9750/10000 (98%)\n",
            "\n",
            "Train Epoch: 36 [59968/60000 (100%)]\tLoss: 0.000031\n",
            "\n",
            "Test set: Average loss: 40.4963, Accuracy: 9752/10000 (98%)\n",
            "\n",
            "Train Epoch: 37 [59968/60000 (100%)]\tLoss: 0.000652\n",
            "\n",
            "Test set: Average loss: 41.5171, Accuracy: 9749/10000 (97%)\n",
            "\n",
            "Train Epoch: 38 [59968/60000 (100%)]\tLoss: 0.003002\n",
            "\n",
            "Test set: Average loss: 43.8368, Accuracy: 9752/10000 (98%)\n",
            "\n",
            "Train Epoch: 39 [59968/60000 (100%)]\tLoss: 0.004451\n",
            "\n",
            "Test set: Average loss: 45.6326, Accuracy: 9750/10000 (98%)\n",
            "\n",
            "Train Epoch: 40 [59968/60000 (100%)]\tLoss: 0.000032\n",
            "\n",
            "Test set: Average loss: 45.5225, Accuracy: 9764/10000 (98%)\n",
            "\n",
            "Train Epoch: 41 [59968/60000 (100%)]\tLoss: 0.000140\n",
            "\n",
            "Test set: Average loss: 45.6327, Accuracy: 9753/10000 (98%)\n",
            "\n",
            "Train Epoch: 42 [59968/60000 (100%)]\tLoss: 0.000292\n",
            "\n",
            "Test set: Average loss: 38.9635, Accuracy: 9780/10000 (98%)\n",
            "\n",
            "Train Epoch: 43 [59968/60000 (100%)]\tLoss: 0.001665\n",
            "\n",
            "Test set: Average loss: 42.0736, Accuracy: 9763/10000 (98%)\n",
            "\n",
            "Train Epoch: 44 [59968/60000 (100%)]\tLoss: 0.003513\n",
            "\n",
            "Test set: Average loss: 43.4016, Accuracy: 9760/10000 (98%)\n",
            "\n",
            "Train Epoch: 45 [59968/60000 (100%)]\tLoss: 0.000013\n",
            "\n",
            "Test set: Average loss: 38.4331, Accuracy: 9790/10000 (98%)\n",
            "\n",
            "Train Epoch: 46 [59968/60000 (100%)]\tLoss: 0.000757\n",
            "\n",
            "Test set: Average loss: 45.2057, Accuracy: 9747/10000 (97%)\n",
            "\n",
            "Train Epoch: 47 [59968/60000 (100%)]\tLoss: 0.000002\n",
            "\n",
            "Test set: Average loss: 51.3336, Accuracy: 9741/10000 (97%)\n",
            "\n",
            "Train Epoch: 48 [59968/60000 (100%)]\tLoss: 0.000289\n",
            "\n",
            "Test set: Average loss: 43.4488, Accuracy: 9771/10000 (98%)\n",
            "\n",
            "Train Epoch: 49 [59968/60000 (100%)]\tLoss: 0.030064\n",
            "\n",
            "Test set: Average loss: 45.6802, Accuracy: 9762/10000 (98%)\n",
            "\n",
            "Train Epoch: 50 [59968/60000 (100%)]\tLoss: 0.000625\n",
            "\n",
            "Test set: Average loss: 40.9933, Accuracy: 9784/10000 (98%)\n",
            "\n",
            "Train Epoch: 51 [59968/60000 (100%)]\tLoss: 0.000055\n",
            "\n",
            "Test set: Average loss: 40.0822, Accuracy: 9791/10000 (98%)\n",
            "\n",
            "Train Epoch: 52 [59968/60000 (100%)]\tLoss: 0.000315\n",
            "\n",
            "Test set: Average loss: 45.7914, Accuracy: 9759/10000 (98%)\n",
            "\n",
            "Train Epoch: 53 [59968/60000 (100%)]\tLoss: 0.002513\n",
            "\n",
            "Test set: Average loss: 48.9314, Accuracy: 9754/10000 (98%)\n",
            "\n",
            "Train Epoch: 54 [59968/60000 (100%)]\tLoss: 0.000273\n",
            "\n",
            "Test set: Average loss: 49.5113, Accuracy: 9760/10000 (98%)\n",
            "\n",
            "Train Epoch: 55 [59968/60000 (100%)]\tLoss: 0.000003\n",
            "\n",
            "Test set: Average loss: 43.0321, Accuracy: 9789/10000 (98%)\n",
            "\n",
            "Train Epoch: 56 [59968/60000 (100%)]\tLoss: 0.040753\n",
            "\n",
            "Test set: Average loss: 49.9744, Accuracy: 9764/10000 (98%)\n",
            "\n",
            "Train Epoch: 57 [59968/60000 (100%)]\tLoss: 0.000005\n",
            "\n",
            "Test set: Average loss: 40.7515, Accuracy: 9781/10000 (98%)\n",
            "\n",
            "Train Epoch: 58 [59968/60000 (100%)]\tLoss: 0.003412\n",
            "\n",
            "Test set: Average loss: 47.3079, Accuracy: 9772/10000 (98%)\n",
            "\n",
            "Train Epoch: 59 [59968/60000 (100%)]\tLoss: 0.000151\n",
            "\n",
            "Test set: Average loss: 41.4491, Accuracy: 9805/10000 (98%)\n",
            "\n",
            "Train Epoch: 60 [59968/60000 (100%)]\tLoss: 0.000899\n",
            "\n",
            "Test set: Average loss: 45.8728, Accuracy: 9760/10000 (98%)\n",
            "\n",
            "Train Epoch: 61 [59968/60000 (100%)]\tLoss: 0.001496\n",
            "\n",
            "Test set: Average loss: 47.1435, Accuracy: 9785/10000 (98%)\n",
            "\n",
            "Train Epoch: 62 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 41.6102, Accuracy: 9782/10000 (98%)\n",
            "\n",
            "Train Epoch: 63 [59968/60000 (100%)]\tLoss: 0.000034\n",
            "\n",
            "Test set: Average loss: 48.9337, Accuracy: 9755/10000 (98%)\n",
            "\n",
            "Train Epoch: 64 [59968/60000 (100%)]\tLoss: 0.034994\n",
            "\n",
            "Test set: Average loss: 45.4656, Accuracy: 9780/10000 (98%)\n",
            "\n",
            "Train Epoch: 65 [59968/60000 (100%)]\tLoss: 0.000640\n",
            "\n",
            "Test set: Average loss: 51.0708, Accuracy: 9753/10000 (98%)\n",
            "\n",
            "Train Epoch: 66 [59968/60000 (100%)]\tLoss: 0.001084\n",
            "\n",
            "Test set: Average loss: 45.6605, Accuracy: 9777/10000 (98%)\n",
            "\n",
            "Train Epoch: 67 [59968/60000 (100%)]\tLoss: 0.000996\n",
            "\n",
            "Test set: Average loss: 46.9663, Accuracy: 9784/10000 (98%)\n",
            "\n",
            "Train Epoch: 68 [59968/60000 (100%)]\tLoss: 0.000024\n",
            "\n",
            "Test set: Average loss: 46.5534, Accuracy: 9768/10000 (98%)\n",
            "\n",
            "Train Epoch: 69 [59968/60000 (100%)]\tLoss: 0.000283\n",
            "\n",
            "Test set: Average loss: 55.2822, Accuracy: 9746/10000 (97%)\n",
            "\n",
            "Train Epoch: 70 [59968/60000 (100%)]\tLoss: 0.000891\n",
            "\n",
            "Test set: Average loss: 49.9316, Accuracy: 9762/10000 (98%)\n",
            "\n",
            "Train Epoch: 71 [59968/60000 (100%)]\tLoss: 0.000022\n",
            "\n",
            "Test set: Average loss: 55.2552, Accuracy: 9728/10000 (97%)\n",
            "\n",
            "Train Epoch: 72 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 42.3821, Accuracy: 9796/10000 (98%)\n",
            "\n",
            "Train Epoch: 73 [59968/60000 (100%)]\tLoss: 0.005073\n",
            "\n",
            "Test set: Average loss: 46.0316, Accuracy: 9775/10000 (98%)\n",
            "\n",
            "Train Epoch: 74 [59968/60000 (100%)]\tLoss: 0.000378\n",
            "\n",
            "Test set: Average loss: 57.7030, Accuracy: 9734/10000 (97%)\n",
            "\n",
            "Train Epoch: 75 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 46.3468, Accuracy: 9772/10000 (98%)\n",
            "\n",
            "Train Epoch: 76 [59968/60000 (100%)]\tLoss: 0.000073\n",
            "\n",
            "Test set: Average loss: 47.3275, Accuracy: 9775/10000 (98%)\n",
            "\n",
            "Train Epoch: 77 [59968/60000 (100%)]\tLoss: 0.000002\n",
            "\n",
            "Test set: Average loss: 47.7396, Accuracy: 9788/10000 (98%)\n",
            "\n",
            "Train Epoch: 78 [59968/60000 (100%)]\tLoss: 0.000020\n",
            "\n",
            "Test set: Average loss: 47.4088, Accuracy: 9780/10000 (98%)\n",
            "\n",
            "Train Epoch: 79 [59968/60000 (100%)]\tLoss: 0.000003\n",
            "\n",
            "Test set: Average loss: 53.1633, Accuracy: 9770/10000 (98%)\n",
            "\n",
            "Train Epoch: 80 [59968/60000 (100%)]\tLoss: 0.047259\n",
            "\n",
            "Test set: Average loss: 45.1320, Accuracy: 9794/10000 (98%)\n",
            "\n",
            "Train Epoch: 81 [59968/60000 (100%)]\tLoss: 0.000033\n",
            "\n",
            "Test set: Average loss: 48.3372, Accuracy: 9770/10000 (98%)\n",
            "\n",
            "Train Epoch: 82 [59968/60000 (100%)]\tLoss: 0.000120\n",
            "\n",
            "Test set: Average loss: 44.7938, Accuracy: 9783/10000 (98%)\n",
            "\n",
            "Train Epoch: 83 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 44.2140, Accuracy: 9795/10000 (98%)\n",
            "\n",
            "Train Epoch: 84 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 43.9225, Accuracy: 9788/10000 (98%)\n",
            "\n",
            "Train Epoch: 85 [59968/60000 (100%)]\tLoss: 0.001766\n",
            "\n",
            "Test set: Average loss: 46.4367, Accuracy: 9786/10000 (98%)\n",
            "\n",
            "Train Epoch: 86 [59968/60000 (100%)]\tLoss: 0.000026\n",
            "\n",
            "Test set: Average loss: 44.5676, Accuracy: 9795/10000 (98%)\n",
            "\n",
            "Train Epoch: 87 [59968/60000 (100%)]\tLoss: 0.000278\n",
            "\n",
            "Test set: Average loss: 48.0041, Accuracy: 9766/10000 (98%)\n",
            "\n",
            "Train Epoch: 88 [59968/60000 (100%)]\tLoss: 0.000146\n",
            "\n",
            "Test set: Average loss: 43.6016, Accuracy: 9792/10000 (98%)\n",
            "\n",
            "Train Epoch: 89 [59968/60000 (100%)]\tLoss: 0.000025\n",
            "\n",
            "Test set: Average loss: 45.8188, Accuracy: 9788/10000 (98%)\n",
            "\n",
            "Train Epoch: 90 [59968/60000 (100%)]\tLoss: 0.000301\n",
            "\n",
            "Test set: Average loss: 59.9880, Accuracy: 9737/10000 (97%)\n",
            "\n",
            "Train Epoch: 91 [59968/60000 (100%)]\tLoss: 0.000002\n",
            "\n",
            "Test set: Average loss: 47.3077, Accuracy: 9786/10000 (98%)\n",
            "\n",
            "Train Epoch: 92 [59968/60000 (100%)]\tLoss: 0.037149\n",
            "\n",
            "Test set: Average loss: 44.5663, Accuracy: 9785/10000 (98%)\n",
            "\n",
            "Train Epoch: 93 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 43.1514, Accuracy: 9814/10000 (98%)\n",
            "\n",
            "Train Epoch: 94 [59968/60000 (100%)]\tLoss: 0.000003\n",
            "\n",
            "Test set: Average loss: 45.1639, Accuracy: 9809/10000 (98%)\n",
            "\n",
            "Train Epoch: 95 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 45.8800, Accuracy: 9818/10000 (98%)\n",
            "\n",
            "Train Epoch: 96 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 48.1166, Accuracy: 9815/10000 (98%)\n",
            "\n",
            "Train Epoch: 97 [59968/60000 (100%)]\tLoss: 0.000002\n",
            "\n",
            "Test set: Average loss: 50.9069, Accuracy: 9814/10000 (98%)\n",
            "\n",
            "Train Epoch: 98 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 53.5164, Accuracy: 9819/10000 (98%)\n",
            "\n",
            "Train Epoch: 99 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 56.5553, Accuracy: 9822/10000 (98%)\n",
            "\n",
            "Train Epoch: 100 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 53.1098, Accuracy: 9788/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relu_model = model\n",
        "for p in model.parameters():\n",
        "  print(p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLoixyV0GoY9",
        "outputId": "817d2671-deee-4664-966e-1d08c4286a38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0066,  0.0350, -0.0066,  ..., -0.0092, -0.0324, -0.0259],\n",
            "        [-0.0005,  0.0318, -0.0215,  ...,  0.0027,  0.0216, -0.0264],\n",
            "        [ 0.0020, -0.0008, -0.0146,  ..., -0.0097,  0.0312,  0.0331],\n",
            "        ...,\n",
            "        [-0.0247, -0.0067, -0.0246,  ...,  0.0317, -0.0288, -0.0078],\n",
            "        [ 0.0301,  0.0334,  0.0133,  ..., -0.0307,  0.0081,  0.0216],\n",
            "        [ 0.0255,  0.0199, -0.0239,  ..., -0.0074, -0.0154,  0.0319]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 6.8783e-03, -7.1925e-03, -4.0269e-02, -2.0070e-02, -1.1682e-02,\n",
            "        -2.3824e-02,  4.0018e-02, -4.2927e-02, -9.5298e-03,  2.8930e-03,\n",
            "        -2.2812e-02,  1.6409e-02,  1.6204e-02, -1.5028e-02, -1.0336e-02,\n",
            "         2.0358e-02, -4.9963e-03,  3.3359e-02,  1.7695e-02,  1.3409e-02,\n",
            "         2.2361e-02,  3.3353e-02, -2.0574e-02, -2.2784e-02,  6.8703e-03,\n",
            "         3.0454e-02,  1.7202e-02,  3.9294e-02, -5.9534e-03,  6.4410e-03,\n",
            "        -2.2518e-03, -1.2908e-02, -1.6700e-02,  2.3940e-02, -2.9257e-02,\n",
            "        -3.1057e-02, -3.4764e-02, -2.6061e-02,  3.2071e-02, -1.2625e-02,\n",
            "         3.0047e-02, -8.2592e-04,  2.2655e-02, -4.0159e-02, -1.1750e-02,\n",
            "         1.4075e-02, -3.1119e-02, -5.1596e-02,  1.3238e-02, -2.9577e-02,\n",
            "        -2.3452e-02, -2.0008e-02,  1.3088e-02,  4.1105e-02, -2.1669e-02,\n",
            "        -1.3462e-02, -4.5588e-02,  1.5522e-02, -3.3522e-02, -4.2169e-02,\n",
            "         3.0872e-02,  1.7344e-02,  1.4230e-02, -1.7732e-03, -1.2860e-02,\n",
            "         9.8032e-03,  2.8897e-02, -3.3846e-02,  9.3263e-03, -2.2558e-02,\n",
            "        -4.4387e-02,  3.2317e-02, -8.6253e-03, -2.9002e-02,  5.8121e-03,\n",
            "        -3.5973e-02, -1.5162e-02,  2.0311e-02,  3.8490e-03, -3.2894e-02,\n",
            "        -4.3410e-03, -5.4358e-02, -1.6280e-02, -3.0627e-02, -5.2235e-03,\n",
            "        -9.6127e-03, -4.8448e-03, -8.5400e-03,  2.9318e-02,  1.7459e-02,\n",
            "         2.7516e-02, -1.7465e-03,  5.6789e-03, -2.5742e-02,  4.7020e-02,\n",
            "         1.7347e-02,  9.3710e-03,  1.8995e-02,  1.6751e-02,  3.9847e-02,\n",
            "        -3.1643e-02, -2.6155e-02,  2.2837e-02, -1.6493e-02, -1.4350e-02,\n",
            "        -2.4441e-05,  1.1400e-03,  2.0158e-02,  1.3371e-02, -4.5163e-03,\n",
            "         2.6820e-02,  2.2100e-02,  2.2237e-02,  1.5539e-02, -1.4094e-02,\n",
            "        -4.3727e-02,  1.7657e-02, -1.4116e-02,  2.6659e-02, -2.2522e-02,\n",
            "         4.4598e-03, -2.5800e-02, -3.7922e-02,  4.1531e-02, -2.6300e-02,\n",
            "         2.8809e-02, -2.6695e-02,  2.0401e-02], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.0116,  0.0755, -0.0192,  ..., -0.0472, -0.0668,  0.0319],\n",
            "        [ 0.1610, -0.0665,  0.1212,  ...,  0.0016, -0.0306, -0.0350],\n",
            "        [-0.0030,  0.0017, -0.0079,  ..., -0.1033, -0.1164,  0.0627],\n",
            "        ...,\n",
            "        [ 0.1190,  0.0139,  0.0065,  ..., -0.0294, -0.0277,  0.0211],\n",
            "        [-0.0331, -0.0610,  0.1639,  ..., -0.0605, -0.0283,  0.0906],\n",
            "        [ 0.0138, -0.0073, -0.0174,  ...,  0.0639, -0.0364,  0.0087]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0407, -0.0203, -0.0794, -0.0186, -0.0182,  0.0374,  0.0361,  0.1055,\n",
            "        -0.1003, -0.0638, -0.0833, -0.0749,  0.0083,  0.0373,  0.0860,  0.0304,\n",
            "         0.0538, -0.0616, -0.0319, -0.0482,  0.0321, -0.1179,  0.0599, -0.0483,\n",
            "        -0.0518, -0.0087, -0.1028, -0.0311,  0.0315, -0.0302, -0.0410,  0.0135,\n",
            "         0.0907,  0.0788,  0.0962,  0.0961, -0.0920,  0.0301, -0.0099,  0.0438,\n",
            "         0.0703, -0.0816, -0.0353, -0.0187,  0.0441,  0.0203,  0.0501,  0.0394,\n",
            "         0.0295,  0.0173, -0.0646,  0.1196, -0.0222,  0.0101, -0.0700,  0.0631,\n",
            "        -0.0431, -0.0322,  0.0333, -0.0255,  0.0254,  0.0725, -0.0394,  0.0859,\n",
            "         0.0400, -0.0824,  0.0615,  0.0896, -0.0435, -0.0263,  0.0197,  0.0869,\n",
            "         0.0036, -0.0159,  0.0324,  0.1014,  0.0159, -0.0619, -0.0401, -0.0595,\n",
            "        -0.0393, -0.0190, -0.0436,  0.0351, -0.0026,  0.0147, -0.0304,  0.0379,\n",
            "        -0.0507,  0.0219,  0.1251,  0.0006, -0.0831, -0.0188,  0.0154, -0.0050,\n",
            "        -0.0591,  0.0543, -0.0709, -0.0646,  0.0223, -0.0368,  0.0053,  0.0198,\n",
            "         0.0280,  0.0121,  0.0163,  0.0109, -0.0470, -0.0082, -0.0032, -0.0485,\n",
            "         0.0782,  0.0611,  0.0116,  0.0228, -0.0536, -0.0265,  0.0647,  0.0711,\n",
            "        -0.0641, -0.0700, -0.0329,  0.0103,  0.0017,  0.0255,  0.0671,  0.0560],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.0234,  0.1385,  0.0520,  ...,  0.1053,  0.0258, -0.0693],\n",
            "        [-0.0353,  0.0297, -0.0284,  ...,  0.0374, -0.0263, -0.0284],\n",
            "        [-0.0418,  0.0800,  0.0193,  ..., -0.0267, -0.0250,  0.0658],\n",
            "        ...,\n",
            "        [ 0.0339, -0.0384, -0.0728,  ..., -0.0316,  0.0362,  0.0091],\n",
            "        [-0.0257, -0.0556,  0.0788,  ..., -0.0383, -0.0671,  0.0178],\n",
            "        [ 0.1388,  0.0233,  0.0567,  ..., -0.0612, -0.0152, -0.0481]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0928, -0.0273, -0.0037,  0.0673,  0.0162, -0.0049, -0.1350,  0.0566,\n",
            "        -0.0664,  0.0807, -0.0078, -0.0567,  0.0657, -0.0215,  0.0866, -0.0143,\n",
            "         0.0179, -0.1126,  0.0800,  0.0107,  0.0292,  0.0670,  0.0498,  0.0580,\n",
            "        -0.0667, -0.0804, -0.0402, -0.0927, -0.0441, -0.0053,  0.0245,  0.0118,\n",
            "         0.0351,  0.0303,  0.0255,  0.0468,  0.0234, -0.0096,  0.0399,  0.1057,\n",
            "        -0.1176,  0.0226,  0.0285,  0.0030,  0.0271, -0.0654, -0.0203,  0.0368,\n",
            "        -0.1168,  0.1201,  0.0189, -0.0670, -0.0178,  0.0257,  0.0352,  0.0477,\n",
            "         0.0890, -0.0559,  0.1160,  0.0396, -0.0531, -0.0209, -0.0669,  0.0678,\n",
            "        -0.0789,  0.0733, -0.1027, -0.1146,  0.0631, -0.0337,  0.0077, -0.0956,\n",
            "        -0.0208, -0.0512, -0.0253,  0.0192,  0.0653,  0.1110,  0.1046,  0.0727,\n",
            "         0.0774, -0.0341,  0.0551, -0.1076, -0.0963,  0.0576,  0.0466,  0.0805,\n",
            "         0.0210, -0.1080, -0.0306,  0.0265,  0.0336, -0.0717,  0.1499, -0.0135,\n",
            "         0.1433, -0.0661,  0.0239, -0.0401, -0.0370,  0.0939, -0.0553,  0.0738,\n",
            "        -0.0615,  0.0196,  0.0251,  0.0026, -0.0094, -0.0269,  0.0792, -0.0270,\n",
            "         0.0582,  0.0231, -0.0553, -0.0900, -0.0346,  0.0714,  0.1653, -0.0262,\n",
            "        -0.0543, -0.1026,  0.0361, -0.0163,  0.1352, -0.0425, -0.0037,  0.1139],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.0945, -0.0073,  0.0060,  ..., -0.0507, -0.0061, -0.0261],\n",
            "        [-0.0259, -0.0410,  0.0099,  ..., -0.1080,  0.0150,  0.1294],\n",
            "        [-0.0488,  0.0948, -0.0942,  ...,  0.1328,  0.0903,  0.0529],\n",
            "        ...,\n",
            "        [-0.0200,  0.0387, -0.0450,  ...,  0.0320, -0.0836,  0.0626],\n",
            "        [ 0.0508,  0.0641, -0.0465,  ..., -0.0190, -0.0553,  0.0275],\n",
            "        [-0.0905,  0.0577, -0.0148,  ...,  0.0458,  0.0493,  0.0054]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-2.6732e-02,  3.3618e-02,  1.6230e-01, -1.8006e-02,  9.4266e-02,\n",
            "         1.2870e-01,  4.1953e-02,  1.8571e-02, -6.9442e-02, -6.1046e-02,\n",
            "        -1.1366e-01,  4.4115e-02, -1.4598e-01,  7.8354e-02,  5.0901e-02,\n",
            "         5.5590e-03,  2.6557e-02, -2.0229e-02, -7.3345e-02, -6.2781e-03,\n",
            "        -7.3133e-02, -2.3345e-02,  5.8233e-03, -5.6452e-02, -5.4004e-02,\n",
            "         9.9459e-02,  2.6783e-02,  1.1077e-01,  6.3815e-02, -1.1169e-01,\n",
            "        -8.8610e-02, -2.0237e-02,  2.8567e-02, -1.1022e-01,  3.3266e-02,\n",
            "         7.5081e-02,  5.3181e-02,  1.1833e-01,  4.3765e-02, -5.7952e-02,\n",
            "         3.7613e-03,  9.0634e-02, -7.5562e-03,  1.0273e-01,  5.7954e-02,\n",
            "         1.8533e-02, -1.5143e-03, -2.9102e-02,  6.8182e-03, -3.2094e-02,\n",
            "         7.5323e-02, -5.1221e-02, -6.4895e-02,  9.3006e-02,  2.5324e-02,\n",
            "         6.3421e-02, -1.2495e-01,  2.6563e-02,  1.1105e-01, -8.4000e-02,\n",
            "        -7.8676e-03,  9.8055e-02,  5.6289e-02,  5.0465e-02, -3.8593e-02,\n",
            "         1.2763e-01, -9.0113e-02,  9.5688e-02,  6.8099e-02,  2.9740e-02,\n",
            "        -1.1011e-01, -7.0671e-02,  4.0029e-03, -6.1598e-02, -2.7758e-02,\n",
            "         4.2994e-04, -4.3992e-02,  5.2476e-02, -5.4112e-02,  1.3342e-01,\n",
            "        -4.7973e-02,  6.0020e-02, -9.8144e-06,  2.7644e-02,  9.8028e-02,\n",
            "         1.0266e-01, -3.3740e-02, -5.0693e-03, -4.4000e-02,  1.3797e-01,\n",
            "         7.7505e-03,  1.4045e-01,  1.3399e-01,  8.8500e-02, -3.1077e-02,\n",
            "        -5.3079e-02, -9.1085e-02, -3.1752e-02,  6.8742e-02, -1.5681e-02,\n",
            "        -6.6994e-02, -2.2772e-02,  1.2315e-02, -5.9340e-02, -4.3054e-02,\n",
            "        -1.0271e-01,  1.4961e-02,  7.2788e-02,  9.4015e-02,  2.4801e-02,\n",
            "        -1.4709e-02,  1.1085e-01, -4.1692e-02, -2.9042e-02,  1.2722e-02,\n",
            "         7.9530e-02,  7.4522e-02,  9.2837e-03,  1.0683e-01, -3.3884e-02,\n",
            "         1.6124e-01,  2.2546e-02, -3.0963e-02, -7.7412e-03, -5.1594e-02,\n",
            "        -3.6024e-02, -1.6801e-02,  1.1407e-01], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 1.4418e-02, -2.4147e-03,  7.4946e-02,  ...,  9.3371e-03,\n",
            "         -9.9807e-02,  1.2931e-02],\n",
            "        [-2.2908e-02, -1.1436e-01,  5.6915e-02,  ..., -1.1601e-01,\n",
            "          2.2808e-03, -1.2545e-01],\n",
            "        [ 1.1926e-01,  6.4447e-03,  5.9613e-02,  ..., -1.5970e-02,\n",
            "         -7.3798e-02, -7.7291e-02],\n",
            "        ...,\n",
            "        [-1.0157e-04,  4.9720e-03, -5.8807e-03,  ..., -3.2394e-02,\n",
            "         -9.5203e-02,  1.2291e-01],\n",
            "        [ 3.0351e-02,  6.6302e-02, -2.3127e-02,  ..., -1.5792e-02,\n",
            "          9.2223e-02,  2.3751e-02],\n",
            "        [-5.3401e-02, -1.1491e-01, -2.9026e-02,  ..., -3.1966e-02,\n",
            "         -8.0529e-02,  1.9176e-02]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1250, -0.0889, -0.0215, -0.0529, -0.0134,  0.0496, -0.0261,  0.0779,\n",
            "         0.0286, -0.0129,  0.1275,  0.0224, -0.0250,  0.0412, -0.0065, -0.0571,\n",
            "        -0.0150,  0.0123,  0.0855, -0.0385,  0.0066,  0.0150,  0.0137,  0.0567,\n",
            "        -0.0057, -0.0278, -0.0505, -0.0824,  0.1578, -0.0180,  0.1361, -0.0237,\n",
            "        -0.1335, -0.0273, -0.0061,  0.0953,  0.1614,  0.0029, -0.0219, -0.0710,\n",
            "         0.0056, -0.0100,  0.0963, -0.0745, -0.0568,  0.1505,  0.0228, -0.0211,\n",
            "        -0.0115,  0.0224,  0.0573, -0.0083,  0.1020,  0.0555,  0.1736,  0.0350,\n",
            "        -0.0351,  0.0033,  0.0748, -0.0508, -0.0012, -0.0239,  0.1693, -0.1261,\n",
            "         0.0418,  0.0319,  0.0472, -0.0684, -0.0798, -0.0367, -0.0565,  0.0030,\n",
            "        -0.0520,  0.0966,  0.0214,  0.0505,  0.0056,  0.0426,  0.0728,  0.0412,\n",
            "         0.0108,  0.0263,  0.0484, -0.0333,  0.0108, -0.0189, -0.1288,  0.0444,\n",
            "        -0.0167,  0.0189,  0.0394, -0.0018, -0.1035, -0.0221,  0.0465,  0.0372,\n",
            "         0.1679, -0.0083,  0.0866, -0.0660, -0.0710,  0.0751, -0.0106, -0.0789,\n",
            "         0.1873, -0.0455, -0.0126, -0.0716, -0.0927, -0.0290, -0.0589,  0.0120,\n",
            "         0.0746,  0.1065, -0.0244,  0.0364,  0.0451, -0.1071, -0.0418,  0.0074,\n",
            "         0.0327,  0.0121, -0.0381,  0.0148, -0.0373,  0.1152, -0.0850,  0.0029],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.0485, -0.0660, -0.1732,  ...,  0.1032, -0.0573, -0.3837],\n",
            "        [-0.0831,  0.0772, -0.1368,  ..., -0.1602, -0.1623,  0.1035],\n",
            "        [-0.1994, -0.1872,  0.1231,  ..., -0.0908, -0.1141, -0.2827],\n",
            "        ...,\n",
            "        [ 0.1095,  0.0485,  0.0062,  ..., -0.0897, -0.1300, -0.0553],\n",
            "        [-0.0363,  0.0100, -0.0354,  ..., -0.0095, -0.0298, -0.0406],\n",
            "        [-0.0323, -0.0915, -0.1285,  ...,  0.1217, -0.1693,  0.0776]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0087, -0.0990, -0.0225, -0.0853,  0.0463, -0.0702, -0.1226, -0.0462,\n",
            "         0.1070, -0.0003], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Galu(6, 128)"
      ],
      "metadata": {
        "id": "7yZvLMr3Hlo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for p in relu_model.parameters():\n",
        "  print(p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1_6KgTFHo6e",
        "outputId": "bae39ef5-d6aa-4cdb-d769-e134683bb792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0066,  0.0350, -0.0066,  ..., -0.0092, -0.0324, -0.0259],\n",
            "        [-0.0005,  0.0318, -0.0215,  ...,  0.0027,  0.0216, -0.0264],\n",
            "        [ 0.0020, -0.0008, -0.0146,  ..., -0.0097,  0.0312,  0.0331],\n",
            "        ...,\n",
            "        [-0.0247, -0.0067, -0.0246,  ...,  0.0317, -0.0288, -0.0078],\n",
            "        [ 0.0301,  0.0334,  0.0133,  ..., -0.0307,  0.0081,  0.0216],\n",
            "        [ 0.0255,  0.0199, -0.0239,  ..., -0.0074, -0.0154,  0.0319]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 6.8783e-03, -7.1925e-03, -4.0269e-02, -2.0070e-02, -1.1682e-02,\n",
            "        -2.3824e-02,  4.0018e-02, -4.2927e-02, -9.5298e-03,  2.8930e-03,\n",
            "        -2.2812e-02,  1.6409e-02,  1.6204e-02, -1.5028e-02, -1.0336e-02,\n",
            "         2.0358e-02, -4.9963e-03,  3.3359e-02,  1.7695e-02,  1.3409e-02,\n",
            "         2.2361e-02,  3.3353e-02, -2.0574e-02, -2.2784e-02,  6.8703e-03,\n",
            "         3.0454e-02,  1.7202e-02,  3.9294e-02, -5.9534e-03,  6.4410e-03,\n",
            "        -2.2518e-03, -1.2908e-02, -1.6700e-02,  2.3940e-02, -2.9257e-02,\n",
            "        -3.1057e-02, -3.4764e-02, -2.6061e-02,  3.2071e-02, -1.2625e-02,\n",
            "         3.0047e-02, -8.2592e-04,  2.2655e-02, -4.0159e-02, -1.1750e-02,\n",
            "         1.4075e-02, -3.1119e-02, -5.1596e-02,  1.3238e-02, -2.9577e-02,\n",
            "        -2.3452e-02, -2.0008e-02,  1.3088e-02,  4.1105e-02, -2.1669e-02,\n",
            "        -1.3462e-02, -4.5588e-02,  1.5522e-02, -3.3522e-02, -4.2169e-02,\n",
            "         3.0872e-02,  1.7344e-02,  1.4230e-02, -1.7732e-03, -1.2860e-02,\n",
            "         9.8032e-03,  2.8897e-02, -3.3846e-02,  9.3263e-03, -2.2558e-02,\n",
            "        -4.4387e-02,  3.2317e-02, -8.6253e-03, -2.9002e-02,  5.8121e-03,\n",
            "        -3.5973e-02, -1.5162e-02,  2.0311e-02,  3.8490e-03, -3.2894e-02,\n",
            "        -4.3410e-03, -5.4358e-02, -1.6280e-02, -3.0627e-02, -5.2235e-03,\n",
            "        -9.6127e-03, -4.8448e-03, -8.5400e-03,  2.9318e-02,  1.7459e-02,\n",
            "         2.7516e-02, -1.7465e-03,  5.6789e-03, -2.5742e-02,  4.7020e-02,\n",
            "         1.7347e-02,  9.3710e-03,  1.8995e-02,  1.6751e-02,  3.9847e-02,\n",
            "        -3.1643e-02, -2.6155e-02,  2.2837e-02, -1.6493e-02, -1.4350e-02,\n",
            "        -2.4441e-05,  1.1400e-03,  2.0158e-02,  1.3371e-02, -4.5163e-03,\n",
            "         2.6820e-02,  2.2100e-02,  2.2237e-02,  1.5539e-02, -1.4094e-02,\n",
            "        -4.3727e-02,  1.7657e-02, -1.4116e-02,  2.6659e-02, -2.2522e-02,\n",
            "         4.4598e-03, -2.5800e-02, -3.7922e-02,  4.1531e-02, -2.6300e-02,\n",
            "         2.8809e-02, -2.6695e-02,  2.0401e-02], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.0116,  0.0755, -0.0192,  ..., -0.0472, -0.0668,  0.0319],\n",
            "        [ 0.1610, -0.0665,  0.1212,  ...,  0.0016, -0.0306, -0.0350],\n",
            "        [-0.0030,  0.0017, -0.0079,  ..., -0.1033, -0.1164,  0.0627],\n",
            "        ...,\n",
            "        [ 0.1190,  0.0139,  0.0065,  ..., -0.0294, -0.0277,  0.0211],\n",
            "        [-0.0331, -0.0610,  0.1639,  ..., -0.0605, -0.0283,  0.0906],\n",
            "        [ 0.0138, -0.0073, -0.0174,  ...,  0.0639, -0.0364,  0.0087]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0407, -0.0203, -0.0794, -0.0186, -0.0182,  0.0374,  0.0361,  0.1055,\n",
            "        -0.1003, -0.0638, -0.0833, -0.0749,  0.0083,  0.0373,  0.0860,  0.0304,\n",
            "         0.0538, -0.0616, -0.0319, -0.0482,  0.0321, -0.1179,  0.0599, -0.0483,\n",
            "        -0.0518, -0.0087, -0.1028, -0.0311,  0.0315, -0.0302, -0.0410,  0.0135,\n",
            "         0.0907,  0.0788,  0.0962,  0.0961, -0.0920,  0.0301, -0.0099,  0.0438,\n",
            "         0.0703, -0.0816, -0.0353, -0.0187,  0.0441,  0.0203,  0.0501,  0.0394,\n",
            "         0.0295,  0.0173, -0.0646,  0.1196, -0.0222,  0.0101, -0.0700,  0.0631,\n",
            "        -0.0431, -0.0322,  0.0333, -0.0255,  0.0254,  0.0725, -0.0394,  0.0859,\n",
            "         0.0400, -0.0824,  0.0615,  0.0896, -0.0435, -0.0263,  0.0197,  0.0869,\n",
            "         0.0036, -0.0159,  0.0324,  0.1014,  0.0159, -0.0619, -0.0401, -0.0595,\n",
            "        -0.0393, -0.0190, -0.0436,  0.0351, -0.0026,  0.0147, -0.0304,  0.0379,\n",
            "        -0.0507,  0.0219,  0.1251,  0.0006, -0.0831, -0.0188,  0.0154, -0.0050,\n",
            "        -0.0591,  0.0543, -0.0709, -0.0646,  0.0223, -0.0368,  0.0053,  0.0198,\n",
            "         0.0280,  0.0121,  0.0163,  0.0109, -0.0470, -0.0082, -0.0032, -0.0485,\n",
            "         0.0782,  0.0611,  0.0116,  0.0228, -0.0536, -0.0265,  0.0647,  0.0711,\n",
            "        -0.0641, -0.0700, -0.0329,  0.0103,  0.0017,  0.0255,  0.0671,  0.0560],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.0234,  0.1385,  0.0520,  ...,  0.1053,  0.0258, -0.0693],\n",
            "        [-0.0353,  0.0297, -0.0284,  ...,  0.0374, -0.0263, -0.0284],\n",
            "        [-0.0418,  0.0800,  0.0193,  ..., -0.0267, -0.0250,  0.0658],\n",
            "        ...,\n",
            "        [ 0.0339, -0.0384, -0.0728,  ..., -0.0316,  0.0362,  0.0091],\n",
            "        [-0.0257, -0.0556,  0.0788,  ..., -0.0383, -0.0671,  0.0178],\n",
            "        [ 0.1388,  0.0233,  0.0567,  ..., -0.0612, -0.0152, -0.0481]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0928, -0.0273, -0.0037,  0.0673,  0.0162, -0.0049, -0.1350,  0.0566,\n",
            "        -0.0664,  0.0807, -0.0078, -0.0567,  0.0657, -0.0215,  0.0866, -0.0143,\n",
            "         0.0179, -0.1126,  0.0800,  0.0107,  0.0292,  0.0670,  0.0498,  0.0580,\n",
            "        -0.0667, -0.0804, -0.0402, -0.0927, -0.0441, -0.0053,  0.0245,  0.0118,\n",
            "         0.0351,  0.0303,  0.0255,  0.0468,  0.0234, -0.0096,  0.0399,  0.1057,\n",
            "        -0.1176,  0.0226,  0.0285,  0.0030,  0.0271, -0.0654, -0.0203,  0.0368,\n",
            "        -0.1168,  0.1201,  0.0189, -0.0670, -0.0178,  0.0257,  0.0352,  0.0477,\n",
            "         0.0890, -0.0559,  0.1160,  0.0396, -0.0531, -0.0209, -0.0669,  0.0678,\n",
            "        -0.0789,  0.0733, -0.1027, -0.1146,  0.0631, -0.0337,  0.0077, -0.0956,\n",
            "        -0.0208, -0.0512, -0.0253,  0.0192,  0.0653,  0.1110,  0.1046,  0.0727,\n",
            "         0.0774, -0.0341,  0.0551, -0.1076, -0.0963,  0.0576,  0.0466,  0.0805,\n",
            "         0.0210, -0.1080, -0.0306,  0.0265,  0.0336, -0.0717,  0.1499, -0.0135,\n",
            "         0.1433, -0.0661,  0.0239, -0.0401, -0.0370,  0.0939, -0.0553,  0.0738,\n",
            "        -0.0615,  0.0196,  0.0251,  0.0026, -0.0094, -0.0269,  0.0792, -0.0270,\n",
            "         0.0582,  0.0231, -0.0553, -0.0900, -0.0346,  0.0714,  0.1653, -0.0262,\n",
            "        -0.0543, -0.1026,  0.0361, -0.0163,  0.1352, -0.0425, -0.0037,  0.1139],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.0945, -0.0073,  0.0060,  ..., -0.0507, -0.0061, -0.0261],\n",
            "        [-0.0259, -0.0410,  0.0099,  ..., -0.1080,  0.0150,  0.1294],\n",
            "        [-0.0488,  0.0948, -0.0942,  ...,  0.1328,  0.0903,  0.0529],\n",
            "        ...,\n",
            "        [-0.0200,  0.0387, -0.0450,  ...,  0.0320, -0.0836,  0.0626],\n",
            "        [ 0.0508,  0.0641, -0.0465,  ..., -0.0190, -0.0553,  0.0275],\n",
            "        [-0.0905,  0.0577, -0.0148,  ...,  0.0458,  0.0493,  0.0054]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-2.6732e-02,  3.3618e-02,  1.6230e-01, -1.8006e-02,  9.4266e-02,\n",
            "         1.2870e-01,  4.1953e-02,  1.8571e-02, -6.9442e-02, -6.1046e-02,\n",
            "        -1.1366e-01,  4.4115e-02, -1.4598e-01,  7.8354e-02,  5.0901e-02,\n",
            "         5.5590e-03,  2.6557e-02, -2.0229e-02, -7.3345e-02, -6.2781e-03,\n",
            "        -7.3133e-02, -2.3345e-02,  5.8233e-03, -5.6452e-02, -5.4004e-02,\n",
            "         9.9459e-02,  2.6783e-02,  1.1077e-01,  6.3815e-02, -1.1169e-01,\n",
            "        -8.8610e-02, -2.0237e-02,  2.8567e-02, -1.1022e-01,  3.3266e-02,\n",
            "         7.5081e-02,  5.3181e-02,  1.1833e-01,  4.3765e-02, -5.7952e-02,\n",
            "         3.7613e-03,  9.0634e-02, -7.5562e-03,  1.0273e-01,  5.7954e-02,\n",
            "         1.8533e-02, -1.5143e-03, -2.9102e-02,  6.8182e-03, -3.2094e-02,\n",
            "         7.5323e-02, -5.1221e-02, -6.4895e-02,  9.3006e-02,  2.5324e-02,\n",
            "         6.3421e-02, -1.2495e-01,  2.6563e-02,  1.1105e-01, -8.4000e-02,\n",
            "        -7.8676e-03,  9.8055e-02,  5.6289e-02,  5.0465e-02, -3.8593e-02,\n",
            "         1.2763e-01, -9.0113e-02,  9.5688e-02,  6.8099e-02,  2.9740e-02,\n",
            "        -1.1011e-01, -7.0671e-02,  4.0029e-03, -6.1598e-02, -2.7758e-02,\n",
            "         4.2994e-04, -4.3992e-02,  5.2476e-02, -5.4112e-02,  1.3342e-01,\n",
            "        -4.7973e-02,  6.0020e-02, -9.8144e-06,  2.7644e-02,  9.8028e-02,\n",
            "         1.0266e-01, -3.3740e-02, -5.0693e-03, -4.4000e-02,  1.3797e-01,\n",
            "         7.7505e-03,  1.4045e-01,  1.3399e-01,  8.8500e-02, -3.1077e-02,\n",
            "        -5.3079e-02, -9.1085e-02, -3.1752e-02,  6.8742e-02, -1.5681e-02,\n",
            "        -6.6994e-02, -2.2772e-02,  1.2315e-02, -5.9340e-02, -4.3054e-02,\n",
            "        -1.0271e-01,  1.4961e-02,  7.2788e-02,  9.4015e-02,  2.4801e-02,\n",
            "        -1.4709e-02,  1.1085e-01, -4.1692e-02, -2.9042e-02,  1.2722e-02,\n",
            "         7.9530e-02,  7.4522e-02,  9.2837e-03,  1.0683e-01, -3.3884e-02,\n",
            "         1.6124e-01,  2.2546e-02, -3.0963e-02, -7.7412e-03, -5.1594e-02,\n",
            "        -3.6024e-02, -1.6801e-02,  1.1407e-01], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 1.4418e-02, -2.4147e-03,  7.4946e-02,  ...,  9.3371e-03,\n",
            "         -9.9807e-02,  1.2931e-02],\n",
            "        [-2.2908e-02, -1.1436e-01,  5.6915e-02,  ..., -1.1601e-01,\n",
            "          2.2808e-03, -1.2545e-01],\n",
            "        [ 1.1926e-01,  6.4447e-03,  5.9613e-02,  ..., -1.5970e-02,\n",
            "         -7.3798e-02, -7.7291e-02],\n",
            "        ...,\n",
            "        [-1.0157e-04,  4.9720e-03, -5.8807e-03,  ..., -3.2394e-02,\n",
            "         -9.5203e-02,  1.2291e-01],\n",
            "        [ 3.0351e-02,  6.6302e-02, -2.3127e-02,  ..., -1.5792e-02,\n",
            "          9.2223e-02,  2.3751e-02],\n",
            "        [-5.3401e-02, -1.1491e-01, -2.9026e-02,  ..., -3.1966e-02,\n",
            "         -8.0529e-02,  1.9176e-02]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.1250, -0.0889, -0.0215, -0.0529, -0.0134,  0.0496, -0.0261,  0.0779,\n",
            "         0.0286, -0.0129,  0.1275,  0.0224, -0.0250,  0.0412, -0.0065, -0.0571,\n",
            "        -0.0150,  0.0123,  0.0855, -0.0385,  0.0066,  0.0150,  0.0137,  0.0567,\n",
            "        -0.0057, -0.0278, -0.0505, -0.0824,  0.1578, -0.0180,  0.1361, -0.0237,\n",
            "        -0.1335, -0.0273, -0.0061,  0.0953,  0.1614,  0.0029, -0.0219, -0.0710,\n",
            "         0.0056, -0.0100,  0.0963, -0.0745, -0.0568,  0.1505,  0.0228, -0.0211,\n",
            "        -0.0115,  0.0224,  0.0573, -0.0083,  0.1020,  0.0555,  0.1736,  0.0350,\n",
            "        -0.0351,  0.0033,  0.0748, -0.0508, -0.0012, -0.0239,  0.1693, -0.1261,\n",
            "         0.0418,  0.0319,  0.0472, -0.0684, -0.0798, -0.0367, -0.0565,  0.0030,\n",
            "        -0.0520,  0.0966,  0.0214,  0.0505,  0.0056,  0.0426,  0.0728,  0.0412,\n",
            "         0.0108,  0.0263,  0.0484, -0.0333,  0.0108, -0.0189, -0.1288,  0.0444,\n",
            "        -0.0167,  0.0189,  0.0394, -0.0018, -0.1035, -0.0221,  0.0465,  0.0372,\n",
            "         0.1679, -0.0083,  0.0866, -0.0660, -0.0710,  0.0751, -0.0106, -0.0789,\n",
            "         0.1873, -0.0455, -0.0126, -0.0716, -0.0927, -0.0290, -0.0589,  0.0120,\n",
            "         0.0746,  0.1065, -0.0244,  0.0364,  0.0451, -0.1071, -0.0418,  0.0074,\n",
            "         0.0327,  0.0121, -0.0381,  0.0148, -0.0373,  0.1152, -0.0850,  0.0029],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.0485, -0.0660, -0.1732,  ...,  0.1032, -0.0573, -0.3837],\n",
            "        [-0.0831,  0.0772, -0.1368,  ..., -0.1602, -0.1623,  0.1035],\n",
            "        [-0.1994, -0.1872,  0.1231,  ..., -0.0908, -0.1141, -0.2827],\n",
            "        ...,\n",
            "        [ 0.1095,  0.0485,  0.0062,  ..., -0.0897, -0.1300, -0.0553],\n",
            "        [-0.0363,  0.0100, -0.0354,  ..., -0.0095, -0.0298, -0.0406],\n",
            "        [-0.0323, -0.0915, -0.1285,  ...,  0.1217, -0.1693,  0.0776]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0087, -0.0990, -0.0225, -0.0853,  0.0463, -0.0702, -0.1226, -0.0462,\n",
            "         0.1070, -0.0003], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_params = [x.data for x in relu_model.parameters()]\n",
        "\n",
        "i=0\n",
        "for (name, params) in model.named_parameters():\n",
        "  if name[0]=='R':\n",
        "    params.data = trained_params[i]\n",
        "    params.requires_grad = False\n",
        "    i+=1\n",
        "  print(name, params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXPEpZfqGd-L",
        "outputId": "cbab1eb8-85b8-4869-ca03-d4c586a67aa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R1.weight Parameter containing:\n",
            "tensor([[-0.0066,  0.0350, -0.0066,  ..., -0.0092, -0.0324, -0.0259],\n",
            "        [-0.0005,  0.0318, -0.0215,  ...,  0.0027,  0.0216, -0.0264],\n",
            "        [ 0.0020, -0.0008, -0.0146,  ..., -0.0097,  0.0312,  0.0331],\n",
            "        ...,\n",
            "        [-0.0247, -0.0067, -0.0246,  ...,  0.0317, -0.0288, -0.0078],\n",
            "        [ 0.0301,  0.0334,  0.0133,  ..., -0.0307,  0.0081,  0.0216],\n",
            "        [ 0.0255,  0.0199, -0.0239,  ..., -0.0074, -0.0154,  0.0319]])\n",
            "R1.bias Parameter containing:\n",
            "tensor([ 6.8783e-03, -7.1925e-03, -4.0269e-02, -2.0070e-02, -1.1682e-02,\n",
            "        -2.3824e-02,  4.0018e-02, -4.2927e-02, -9.5298e-03,  2.8930e-03,\n",
            "        -2.2812e-02,  1.6409e-02,  1.6204e-02, -1.5028e-02, -1.0336e-02,\n",
            "         2.0358e-02, -4.9963e-03,  3.3359e-02,  1.7695e-02,  1.3409e-02,\n",
            "         2.2361e-02,  3.3353e-02, -2.0574e-02, -2.2784e-02,  6.8703e-03,\n",
            "         3.0454e-02,  1.7202e-02,  3.9294e-02, -5.9534e-03,  6.4410e-03,\n",
            "        -2.2518e-03, -1.2908e-02, -1.6700e-02,  2.3940e-02, -2.9257e-02,\n",
            "        -3.1057e-02, -3.4764e-02, -2.6061e-02,  3.2071e-02, -1.2625e-02,\n",
            "         3.0047e-02, -8.2592e-04,  2.2655e-02, -4.0159e-02, -1.1750e-02,\n",
            "         1.4075e-02, -3.1119e-02, -5.1596e-02,  1.3238e-02, -2.9577e-02,\n",
            "        -2.3452e-02, -2.0008e-02,  1.3088e-02,  4.1105e-02, -2.1669e-02,\n",
            "        -1.3462e-02, -4.5588e-02,  1.5522e-02, -3.3522e-02, -4.2169e-02,\n",
            "         3.0872e-02,  1.7344e-02,  1.4230e-02, -1.7732e-03, -1.2860e-02,\n",
            "         9.8032e-03,  2.8897e-02, -3.3846e-02,  9.3263e-03, -2.2558e-02,\n",
            "        -4.4387e-02,  3.2317e-02, -8.6253e-03, -2.9002e-02,  5.8121e-03,\n",
            "        -3.5973e-02, -1.5162e-02,  2.0311e-02,  3.8490e-03, -3.2894e-02,\n",
            "        -4.3410e-03, -5.4358e-02, -1.6280e-02, -3.0627e-02, -5.2235e-03,\n",
            "        -9.6127e-03, -4.8448e-03, -8.5400e-03,  2.9318e-02,  1.7459e-02,\n",
            "         2.7516e-02, -1.7465e-03,  5.6789e-03, -2.5742e-02,  4.7020e-02,\n",
            "         1.7347e-02,  9.3710e-03,  1.8995e-02,  1.6751e-02,  3.9847e-02,\n",
            "        -3.1643e-02, -2.6155e-02,  2.2837e-02, -1.6493e-02, -1.4350e-02,\n",
            "        -2.4441e-05,  1.1400e-03,  2.0158e-02,  1.3371e-02, -4.5163e-03,\n",
            "         2.6820e-02,  2.2100e-02,  2.2237e-02,  1.5539e-02, -1.4094e-02,\n",
            "        -4.3727e-02,  1.7657e-02, -1.4116e-02,  2.6659e-02, -2.2522e-02,\n",
            "         4.4598e-03, -2.5800e-02, -3.7922e-02,  4.1531e-02, -2.6300e-02,\n",
            "         2.8809e-02, -2.6695e-02,  2.0401e-02])\n",
            "G1.weight Parameter containing:\n",
            "tensor([[-0.0190, -0.0240,  0.0192,  ..., -0.0181,  0.0081,  0.0289],\n",
            "        [ 0.0249, -0.0297, -0.0229,  ..., -0.0173,  0.0010, -0.0165],\n",
            "        [-0.0300, -0.0273, -0.0035,  ...,  0.0141, -0.0233,  0.0148],\n",
            "        ...,\n",
            "        [-0.0253, -0.0050, -0.0330,  ..., -0.0301, -0.0147, -0.0266],\n",
            "        [ 0.0045, -0.0110, -0.0275,  ...,  0.0322,  0.0117,  0.0045],\n",
            "        [ 0.0134, -0.0178,  0.0203,  ...,  0.0245, -0.0191,  0.0234]],\n",
            "       requires_grad=True)\n",
            "G1.bias Parameter containing:\n",
            "tensor([-0.0186,  0.0162,  0.0124, -0.0111,  0.0105,  0.0156, -0.0183, -0.0052,\n",
            "        -0.0256, -0.0093,  0.0245, -0.0343,  0.0315, -0.0090, -0.0003,  0.0142,\n",
            "        -0.0244,  0.0196,  0.0058, -0.0001, -0.0067, -0.0080, -0.0134, -0.0046,\n",
            "        -0.0340, -0.0252, -0.0198, -0.0321,  0.0064, -0.0122, -0.0098, -0.0135,\n",
            "         0.0198, -0.0168,  0.0100, -0.0249, -0.0131,  0.0285,  0.0192, -0.0243,\n",
            "         0.0103,  0.0345, -0.0233,  0.0144,  0.0099,  0.0210,  0.0356, -0.0278,\n",
            "         0.0210, -0.0173, -0.0111, -0.0188, -0.0275,  0.0081,  0.0090, -0.0230,\n",
            "        -0.0340,  0.0010, -0.0132,  0.0186,  0.0039,  0.0257,  0.0261,  0.0117,\n",
            "         0.0063,  0.0188,  0.0312, -0.0286,  0.0304,  0.0087, -0.0016,  0.0065,\n",
            "        -0.0083, -0.0017,  0.0319, -0.0106,  0.0183, -0.0118,  0.0053, -0.0227,\n",
            "        -0.0217, -0.0065, -0.0244, -0.0219,  0.0116,  0.0323,  0.0049, -0.0178,\n",
            "        -0.0181, -0.0068, -0.0055, -0.0279, -0.0133, -0.0177, -0.0327,  0.0110,\n",
            "        -0.0297, -0.0014,  0.0307,  0.0269, -0.0056, -0.0109, -0.0097,  0.0355,\n",
            "         0.0005, -0.0253,  0.0157, -0.0165,  0.0214,  0.0107, -0.0007, -0.0134,\n",
            "        -0.0292, -0.0122, -0.0315, -0.0063,  0.0313,  0.0086,  0.0080, -0.0332,\n",
            "         0.0128, -0.0043,  0.0274,  0.0088, -0.0045,  0.0161, -0.0068, -0.0321],\n",
            "       requires_grad=True)\n",
            "R2.weight Parameter containing:\n",
            "tensor([[-0.0116,  0.0755, -0.0192,  ..., -0.0472, -0.0668,  0.0319],\n",
            "        [ 0.1610, -0.0665,  0.1212,  ...,  0.0016, -0.0306, -0.0350],\n",
            "        [-0.0030,  0.0017, -0.0079,  ..., -0.1033, -0.1164,  0.0627],\n",
            "        ...,\n",
            "        [ 0.1190,  0.0139,  0.0065,  ..., -0.0294, -0.0277,  0.0211],\n",
            "        [-0.0331, -0.0610,  0.1639,  ..., -0.0605, -0.0283,  0.0906],\n",
            "        [ 0.0138, -0.0073, -0.0174,  ...,  0.0639, -0.0364,  0.0087]])\n",
            "R2.bias Parameter containing:\n",
            "tensor([ 0.0407, -0.0203, -0.0794, -0.0186, -0.0182,  0.0374,  0.0361,  0.1055,\n",
            "        -0.1003, -0.0638, -0.0833, -0.0749,  0.0083,  0.0373,  0.0860,  0.0304,\n",
            "         0.0538, -0.0616, -0.0319, -0.0482,  0.0321, -0.1179,  0.0599, -0.0483,\n",
            "        -0.0518, -0.0087, -0.1028, -0.0311,  0.0315, -0.0302, -0.0410,  0.0135,\n",
            "         0.0907,  0.0788,  0.0962,  0.0961, -0.0920,  0.0301, -0.0099,  0.0438,\n",
            "         0.0703, -0.0816, -0.0353, -0.0187,  0.0441,  0.0203,  0.0501,  0.0394,\n",
            "         0.0295,  0.0173, -0.0646,  0.1196, -0.0222,  0.0101, -0.0700,  0.0631,\n",
            "        -0.0431, -0.0322,  0.0333, -0.0255,  0.0254,  0.0725, -0.0394,  0.0859,\n",
            "         0.0400, -0.0824,  0.0615,  0.0896, -0.0435, -0.0263,  0.0197,  0.0869,\n",
            "         0.0036, -0.0159,  0.0324,  0.1014,  0.0159, -0.0619, -0.0401, -0.0595,\n",
            "        -0.0393, -0.0190, -0.0436,  0.0351, -0.0026,  0.0147, -0.0304,  0.0379,\n",
            "        -0.0507,  0.0219,  0.1251,  0.0006, -0.0831, -0.0188,  0.0154, -0.0050,\n",
            "        -0.0591,  0.0543, -0.0709, -0.0646,  0.0223, -0.0368,  0.0053,  0.0198,\n",
            "         0.0280,  0.0121,  0.0163,  0.0109, -0.0470, -0.0082, -0.0032, -0.0485,\n",
            "         0.0782,  0.0611,  0.0116,  0.0228, -0.0536, -0.0265,  0.0647,  0.0711,\n",
            "        -0.0641, -0.0700, -0.0329,  0.0103,  0.0017,  0.0255,  0.0671,  0.0560])\n",
            "G2.weight Parameter containing:\n",
            "tensor([[-0.0519,  0.0718,  0.0028,  ..., -0.0532,  0.0169,  0.0806],\n",
            "        [-0.0449, -0.0286, -0.0528,  ...,  0.0653, -0.0023, -0.0733],\n",
            "        [-0.0576, -0.0226,  0.0569,  ...,  0.0519,  0.0701, -0.0124],\n",
            "        ...,\n",
            "        [-0.0776,  0.0115,  0.0685,  ...,  0.0474, -0.0541, -0.0219],\n",
            "        [-0.0361, -0.0582,  0.0213,  ..., -0.0749,  0.0544, -0.0453],\n",
            "        [ 0.0548,  0.0833,  0.0008,  ...,  0.0722, -0.0037,  0.0371]],\n",
            "       requires_grad=True)\n",
            "G2.bias Parameter containing:\n",
            "tensor([ 0.0230, -0.0305,  0.0694, -0.0165, -0.0774,  0.0080,  0.0720,  0.0808,\n",
            "        -0.0838,  0.0851,  0.0873, -0.0190,  0.0179,  0.0398,  0.0603, -0.0525,\n",
            "         0.0756,  0.0132, -0.0545,  0.0073, -0.0233, -0.0521, -0.0316,  0.0778,\n",
            "         0.0589, -0.0707,  0.0544, -0.0136, -0.0374,  0.0207,  0.0080,  0.0043,\n",
            "         0.0769,  0.0737, -0.0486,  0.0565,  0.0440,  0.0602,  0.0338,  0.0340,\n",
            "         0.0543,  0.0680, -0.0256,  0.0741,  0.0487,  0.0865, -0.0439,  0.0303,\n",
            "         0.0469, -0.0567, -0.0016, -0.0641,  0.0206,  0.0809,  0.0193,  0.0173,\n",
            "         0.0246, -0.0109,  0.0736, -0.0511,  0.0028, -0.0813,  0.0258,  0.0603,\n",
            "         0.0113, -0.0677,  0.0656, -0.0287,  0.0701, -0.0667, -0.0663, -0.0651,\n",
            "        -0.0026,  0.0262, -0.0803,  0.0059, -0.0496,  0.0532,  0.0010, -0.0335,\n",
            "         0.0644,  0.0094, -0.0110, -0.0588, -0.0476, -0.0024,  0.0419,  0.0407,\n",
            "        -0.0835, -0.0724, -0.0152, -0.0742,  0.0669,  0.0314,  0.0455,  0.0523,\n",
            "        -0.0825, -0.0823, -0.0841, -0.0348, -0.0789,  0.0110, -0.0045,  0.0158,\n",
            "         0.0838,  0.0784, -0.0863,  0.0236,  0.0117, -0.0495, -0.0678, -0.0285,\n",
            "        -0.0695,  0.0145, -0.0795,  0.0482, -0.0292,  0.0196,  0.0575,  0.0375,\n",
            "        -0.0137, -0.0626,  0.0140,  0.0378, -0.0670,  0.0545, -0.0497, -0.0174],\n",
            "       requires_grad=True)\n",
            "R3.weight Parameter containing:\n",
            "tensor([[-0.0234,  0.1385,  0.0520,  ...,  0.1053,  0.0258, -0.0693],\n",
            "        [-0.0353,  0.0297, -0.0284,  ...,  0.0374, -0.0263, -0.0284],\n",
            "        [-0.0418,  0.0800,  0.0193,  ..., -0.0267, -0.0250,  0.0658],\n",
            "        ...,\n",
            "        [ 0.0339, -0.0384, -0.0728,  ..., -0.0316,  0.0362,  0.0091],\n",
            "        [-0.0257, -0.0556,  0.0788,  ..., -0.0383, -0.0671,  0.0178],\n",
            "        [ 0.1388,  0.0233,  0.0567,  ..., -0.0612, -0.0152, -0.0481]])\n",
            "R3.bias Parameter containing:\n",
            "tensor([-0.0928, -0.0273, -0.0037,  0.0673,  0.0162, -0.0049, -0.1350,  0.0566,\n",
            "        -0.0664,  0.0807, -0.0078, -0.0567,  0.0657, -0.0215,  0.0866, -0.0143,\n",
            "         0.0179, -0.1126,  0.0800,  0.0107,  0.0292,  0.0670,  0.0498,  0.0580,\n",
            "        -0.0667, -0.0804, -0.0402, -0.0927, -0.0441, -0.0053,  0.0245,  0.0118,\n",
            "         0.0351,  0.0303,  0.0255,  0.0468,  0.0234, -0.0096,  0.0399,  0.1057,\n",
            "        -0.1176,  0.0226,  0.0285,  0.0030,  0.0271, -0.0654, -0.0203,  0.0368,\n",
            "        -0.1168,  0.1201,  0.0189, -0.0670, -0.0178,  0.0257,  0.0352,  0.0477,\n",
            "         0.0890, -0.0559,  0.1160,  0.0396, -0.0531, -0.0209, -0.0669,  0.0678,\n",
            "        -0.0789,  0.0733, -0.1027, -0.1146,  0.0631, -0.0337,  0.0077, -0.0956,\n",
            "        -0.0208, -0.0512, -0.0253,  0.0192,  0.0653,  0.1110,  0.1046,  0.0727,\n",
            "         0.0774, -0.0341,  0.0551, -0.1076, -0.0963,  0.0576,  0.0466,  0.0805,\n",
            "         0.0210, -0.1080, -0.0306,  0.0265,  0.0336, -0.0717,  0.1499, -0.0135,\n",
            "         0.1433, -0.0661,  0.0239, -0.0401, -0.0370,  0.0939, -0.0553,  0.0738,\n",
            "        -0.0615,  0.0196,  0.0251,  0.0026, -0.0094, -0.0269,  0.0792, -0.0270,\n",
            "         0.0582,  0.0231, -0.0553, -0.0900, -0.0346,  0.0714,  0.1653, -0.0262,\n",
            "        -0.0543, -0.1026,  0.0361, -0.0163,  0.1352, -0.0425, -0.0037,  0.1139])\n",
            "G3.weight Parameter containing:\n",
            "tensor([[-0.0154, -0.0283,  0.0090,  ..., -0.0530,  0.0860,  0.0515],\n",
            "        [-0.0775,  0.0357,  0.0129,  ..., -0.0143, -0.0763,  0.0121],\n",
            "        [ 0.0430, -0.0210,  0.0177,  ..., -0.0394, -0.0774,  0.0543],\n",
            "        ...,\n",
            "        [ 0.0781, -0.0441,  0.0711,  ..., -0.0018,  0.0203, -0.0583],\n",
            "        [ 0.0331, -0.0584, -0.0378,  ..., -0.0528,  0.0439,  0.0453],\n",
            "        [-0.0099, -0.0656, -0.0709,  ...,  0.0026,  0.0550, -0.0135]],\n",
            "       requires_grad=True)\n",
            "G3.bias Parameter containing:\n",
            "tensor([-0.0162,  0.0635, -0.0286,  0.0646, -0.0585, -0.0014, -0.0765, -0.0270,\n",
            "        -0.0417,  0.0233,  0.0507, -0.0417, -0.0136, -0.0522,  0.0758,  0.0333,\n",
            "         0.0752, -0.0372, -0.0387,  0.0464, -0.0783,  0.0711,  0.0597, -0.0565,\n",
            "        -0.0504, -0.0305, -0.0286,  0.0795,  0.0637,  0.0563,  0.0720, -0.0467,\n",
            "        -0.0743,  0.0186,  0.0637,  0.0729,  0.0734, -0.0844,  0.0791,  0.0238,\n",
            "        -0.0648, -0.0130,  0.0483,  0.0493, -0.0424, -0.0673, -0.0441,  0.0137,\n",
            "        -0.0640,  0.0027,  0.0404, -0.0201, -0.0727,  0.0334, -0.0851, -0.0382,\n",
            "        -0.0816,  0.0092, -0.0795,  0.0589, -0.0818,  0.0353, -0.0653, -0.0642,\n",
            "         0.0373, -0.0719,  0.0102, -0.0524, -0.0623,  0.0430, -0.0284,  0.0182,\n",
            "        -0.0677,  0.0740, -0.0751,  0.0750, -0.0040,  0.0825, -0.0613,  0.0193,\n",
            "         0.0837,  0.0451, -0.0442,  0.0261, -0.0729, -0.0647,  0.0464, -0.0396,\n",
            "         0.0007,  0.0179, -0.0414, -0.0124, -0.0603, -0.0160,  0.0765,  0.0030,\n",
            "        -0.0440,  0.0033, -0.0273, -0.0239,  0.0086, -0.0447, -0.0052, -0.0538,\n",
            "         0.0404,  0.0530, -0.0558,  0.0641,  0.0449,  0.0788, -0.0302, -0.0509,\n",
            "        -0.0542,  0.0469,  0.0367, -0.0072,  0.0337, -0.0571,  0.0593, -0.0314,\n",
            "         0.0413, -0.0439,  0.0184, -0.0870,  0.0585,  0.0795,  0.0440, -0.0668],\n",
            "       requires_grad=True)\n",
            "R4.weight Parameter containing:\n",
            "tensor([[ 0.0945, -0.0073,  0.0060,  ..., -0.0507, -0.0061, -0.0261],\n",
            "        [-0.0259, -0.0410,  0.0099,  ..., -0.1080,  0.0150,  0.1294],\n",
            "        [-0.0488,  0.0948, -0.0942,  ...,  0.1328,  0.0903,  0.0529],\n",
            "        ...,\n",
            "        [-0.0200,  0.0387, -0.0450,  ...,  0.0320, -0.0836,  0.0626],\n",
            "        [ 0.0508,  0.0641, -0.0465,  ..., -0.0190, -0.0553,  0.0275],\n",
            "        [-0.0905,  0.0577, -0.0148,  ...,  0.0458,  0.0493,  0.0054]])\n",
            "R4.bias Parameter containing:\n",
            "tensor([-2.6732e-02,  3.3618e-02,  1.6230e-01, -1.8006e-02,  9.4266e-02,\n",
            "         1.2870e-01,  4.1953e-02,  1.8571e-02, -6.9442e-02, -6.1046e-02,\n",
            "        -1.1366e-01,  4.4115e-02, -1.4598e-01,  7.8354e-02,  5.0901e-02,\n",
            "         5.5590e-03,  2.6557e-02, -2.0229e-02, -7.3345e-02, -6.2781e-03,\n",
            "        -7.3133e-02, -2.3345e-02,  5.8233e-03, -5.6452e-02, -5.4004e-02,\n",
            "         9.9459e-02,  2.6783e-02,  1.1077e-01,  6.3815e-02, -1.1169e-01,\n",
            "        -8.8610e-02, -2.0237e-02,  2.8567e-02, -1.1022e-01,  3.3266e-02,\n",
            "         7.5081e-02,  5.3181e-02,  1.1833e-01,  4.3765e-02, -5.7952e-02,\n",
            "         3.7613e-03,  9.0634e-02, -7.5562e-03,  1.0273e-01,  5.7954e-02,\n",
            "         1.8533e-02, -1.5143e-03, -2.9102e-02,  6.8182e-03, -3.2094e-02,\n",
            "         7.5323e-02, -5.1221e-02, -6.4895e-02,  9.3006e-02,  2.5324e-02,\n",
            "         6.3421e-02, -1.2495e-01,  2.6563e-02,  1.1105e-01, -8.4000e-02,\n",
            "        -7.8676e-03,  9.8055e-02,  5.6289e-02,  5.0465e-02, -3.8593e-02,\n",
            "         1.2763e-01, -9.0113e-02,  9.5688e-02,  6.8099e-02,  2.9740e-02,\n",
            "        -1.1011e-01, -7.0671e-02,  4.0029e-03, -6.1598e-02, -2.7758e-02,\n",
            "         4.2994e-04, -4.3992e-02,  5.2476e-02, -5.4112e-02,  1.3342e-01,\n",
            "        -4.7973e-02,  6.0020e-02, -9.8144e-06,  2.7644e-02,  9.8028e-02,\n",
            "         1.0266e-01, -3.3740e-02, -5.0693e-03, -4.4000e-02,  1.3797e-01,\n",
            "         7.7505e-03,  1.4045e-01,  1.3399e-01,  8.8500e-02, -3.1077e-02,\n",
            "        -5.3079e-02, -9.1085e-02, -3.1752e-02,  6.8742e-02, -1.5681e-02,\n",
            "        -6.6994e-02, -2.2772e-02,  1.2315e-02, -5.9340e-02, -4.3054e-02,\n",
            "        -1.0271e-01,  1.4961e-02,  7.2788e-02,  9.4015e-02,  2.4801e-02,\n",
            "        -1.4709e-02,  1.1085e-01, -4.1692e-02, -2.9042e-02,  1.2722e-02,\n",
            "         7.9530e-02,  7.4522e-02,  9.2837e-03,  1.0683e-01, -3.3884e-02,\n",
            "         1.6124e-01,  2.2546e-02, -3.0963e-02, -7.7412e-03, -5.1594e-02,\n",
            "        -3.6024e-02, -1.6801e-02,  1.1407e-01])\n",
            "G4.weight Parameter containing:\n",
            "tensor([[-0.0417, -0.0227,  0.0313,  ..., -0.0196,  0.0672, -0.0159],\n",
            "        [-0.0728,  0.0455,  0.0103,  ...,  0.0012,  0.0492,  0.0189],\n",
            "        [ 0.0754,  0.0733,  0.0497,  ..., -0.0623,  0.0143,  0.0094],\n",
            "        ...,\n",
            "        [ 0.0098,  0.0735, -0.0205,  ..., -0.0433, -0.0026, -0.0563],\n",
            "        [ 0.0364,  0.0411,  0.0428,  ...,  0.0126, -0.0631,  0.0126],\n",
            "        [ 0.0823, -0.0490,  0.0365,  ...,  0.0306,  0.0414,  0.0517]],\n",
            "       requires_grad=True)\n",
            "G4.bias Parameter containing:\n",
            "tensor([ 0.0786,  0.0041,  0.0621,  0.0714,  0.0137, -0.0047, -0.0493, -0.0811,\n",
            "        -0.0059,  0.0663,  0.0235, -0.0078, -0.0283, -0.0375, -0.0747, -0.0751,\n",
            "         0.0339,  0.0827, -0.0212, -0.0807, -0.0039,  0.0165,  0.0673,  0.0169,\n",
            "         0.0029, -0.0671,  0.0267,  0.0695,  0.0106, -0.0059,  0.0423,  0.0654,\n",
            "        -0.0497,  0.0001, -0.0080,  0.0440, -0.0624, -0.0696, -0.0358, -0.0316,\n",
            "         0.0075,  0.0804,  0.0864,  0.0802,  0.0644, -0.0290, -0.0489, -0.0635,\n",
            "         0.0086,  0.0333,  0.0550,  0.0828,  0.0572, -0.0686, -0.0004,  0.0684,\n",
            "        -0.0120,  0.0502,  0.0801,  0.0166, -0.0097, -0.0719,  0.0421, -0.0041,\n",
            "         0.0037, -0.0088, -0.0667,  0.0320,  0.0382,  0.0150, -0.0167,  0.0313,\n",
            "        -0.0645, -0.0644, -0.0484,  0.0532,  0.0459,  0.0189, -0.0783,  0.0461,\n",
            "         0.0279, -0.0395, -0.0784,  0.0851,  0.0533,  0.0793,  0.0570, -0.0675,\n",
            "         0.0430,  0.0233, -0.0318, -0.0827, -0.0719, -0.0357, -0.0413, -0.0039,\n",
            "         0.0290, -0.0657,  0.0695,  0.0804, -0.0361, -0.0282,  0.0179, -0.0727,\n",
            "         0.0076,  0.0440, -0.0182,  0.0774,  0.0870,  0.0506, -0.0448, -0.0354,\n",
            "        -0.0635, -0.0061,  0.0175, -0.0710, -0.0266,  0.0482,  0.0134,  0.0333,\n",
            "        -0.0572,  0.0479, -0.0069, -0.0088, -0.0130,  0.0224,  0.0866, -0.0393],\n",
            "       requires_grad=True)\n",
            "R5.weight Parameter containing:\n",
            "tensor([[ 1.4418e-02, -2.4147e-03,  7.4946e-02,  ...,  9.3371e-03,\n",
            "         -9.9807e-02,  1.2931e-02],\n",
            "        [-2.2908e-02, -1.1436e-01,  5.6915e-02,  ..., -1.1601e-01,\n",
            "          2.2808e-03, -1.2545e-01],\n",
            "        [ 1.1926e-01,  6.4447e-03,  5.9613e-02,  ..., -1.5970e-02,\n",
            "         -7.3798e-02, -7.7291e-02],\n",
            "        ...,\n",
            "        [-1.0157e-04,  4.9720e-03, -5.8807e-03,  ..., -3.2394e-02,\n",
            "         -9.5203e-02,  1.2291e-01],\n",
            "        [ 3.0351e-02,  6.6302e-02, -2.3127e-02,  ..., -1.5792e-02,\n",
            "          9.2223e-02,  2.3751e-02],\n",
            "        [-5.3401e-02, -1.1491e-01, -2.9026e-02,  ..., -3.1966e-02,\n",
            "         -8.0529e-02,  1.9176e-02]])\n",
            "R5.bias Parameter containing:\n",
            "tensor([-0.1250, -0.0889, -0.0215, -0.0529, -0.0134,  0.0496, -0.0261,  0.0779,\n",
            "         0.0286, -0.0129,  0.1275,  0.0224, -0.0250,  0.0412, -0.0065, -0.0571,\n",
            "        -0.0150,  0.0123,  0.0855, -0.0385,  0.0066,  0.0150,  0.0137,  0.0567,\n",
            "        -0.0057, -0.0278, -0.0505, -0.0824,  0.1578, -0.0180,  0.1361, -0.0237,\n",
            "        -0.1335, -0.0273, -0.0061,  0.0953,  0.1614,  0.0029, -0.0219, -0.0710,\n",
            "         0.0056, -0.0100,  0.0963, -0.0745, -0.0568,  0.1505,  0.0228, -0.0211,\n",
            "        -0.0115,  0.0224,  0.0573, -0.0083,  0.1020,  0.0555,  0.1736,  0.0350,\n",
            "        -0.0351,  0.0033,  0.0748, -0.0508, -0.0012, -0.0239,  0.1693, -0.1261,\n",
            "         0.0418,  0.0319,  0.0472, -0.0684, -0.0798, -0.0367, -0.0565,  0.0030,\n",
            "        -0.0520,  0.0966,  0.0214,  0.0505,  0.0056,  0.0426,  0.0728,  0.0412,\n",
            "         0.0108,  0.0263,  0.0484, -0.0333,  0.0108, -0.0189, -0.1288,  0.0444,\n",
            "        -0.0167,  0.0189,  0.0394, -0.0018, -0.1035, -0.0221,  0.0465,  0.0372,\n",
            "         0.1679, -0.0083,  0.0866, -0.0660, -0.0710,  0.0751, -0.0106, -0.0789,\n",
            "         0.1873, -0.0455, -0.0126, -0.0716, -0.0927, -0.0290, -0.0589,  0.0120,\n",
            "         0.0746,  0.1065, -0.0244,  0.0364,  0.0451, -0.1071, -0.0418,  0.0074,\n",
            "         0.0327,  0.0121, -0.0381,  0.0148, -0.0373,  0.1152, -0.0850,  0.0029])\n",
            "G5.weight Parameter containing:\n",
            "tensor([[ 0.0438, -0.0703,  0.0445,  ..., -0.0051,  0.0041,  0.0465],\n",
            "        [ 0.0204, -0.0817, -0.0546,  ...,  0.0160, -0.0581, -0.0013],\n",
            "        [ 0.0117, -0.0123,  0.0637,  ..., -0.0532,  0.0149, -0.0878],\n",
            "        ...,\n",
            "        [ 0.0486, -0.0435, -0.0159,  ...,  0.0015, -0.0416, -0.0599],\n",
            "        [ 0.0010, -0.0042, -0.0676,  ..., -0.0251, -0.0354,  0.0282],\n",
            "        [ 0.0334,  0.0123,  0.0358,  ...,  0.0140, -0.0614, -0.0777]],\n",
            "       requires_grad=True)\n",
            "G5.bias Parameter containing:\n",
            "tensor([-0.0165, -0.0634, -0.0733, -0.0460, -0.0563,  0.0565, -0.0865, -0.0657,\n",
            "        -0.0077,  0.0201,  0.0553, -0.0485, -0.0884, -0.0265,  0.0235,  0.0700,\n",
            "         0.0587, -0.0554,  0.0393,  0.0242, -0.0398,  0.0684, -0.0569,  0.0060,\n",
            "        -0.0180,  0.0295,  0.0077, -0.0250,  0.0518, -0.0698,  0.0165, -0.0749,\n",
            "        -0.0582,  0.0434,  0.0680, -0.0424, -0.0437, -0.0483,  0.0091, -0.0457,\n",
            "        -0.0152,  0.0747,  0.0606, -0.0642, -0.0466,  0.0849, -0.0194,  0.0020,\n",
            "        -0.0162,  0.0467, -0.0401, -0.0324, -0.0127, -0.0037,  0.0864, -0.0695,\n",
            "         0.0653,  0.0843,  0.0238, -0.0457,  0.0571,  0.0573,  0.0003, -0.0091,\n",
            "        -0.0757,  0.0838,  0.0402, -0.0438,  0.0019,  0.0535,  0.0628, -0.0167,\n",
            "        -0.0519,  0.0436,  0.0222,  0.0004,  0.0860,  0.0560, -0.0870,  0.0824,\n",
            "         0.0678,  0.0368,  0.0342, -0.0231,  0.0296,  0.0240,  0.0873,  0.0622,\n",
            "         0.0875, -0.0823, -0.0181,  0.0542, -0.0345, -0.0536, -0.0253, -0.0530,\n",
            "         0.0040, -0.0151, -0.0436,  0.0022,  0.0497, -0.0740, -0.0331,  0.0760,\n",
            "         0.0455, -0.0842,  0.0122,  0.0632, -0.0003, -0.0610,  0.0879,  0.0793,\n",
            "         0.0478,  0.0281,  0.0306, -0.0776,  0.0163, -0.0660, -0.0327,  0.0600,\n",
            "        -0.0806,  0.0206, -0.0604,  0.0215,  0.0619,  0.0326, -0.0453,  0.0078],\n",
            "       requires_grad=True)\n",
            "outputs.weight Parameter containing:\n",
            "tensor([[ 0.0594, -0.0124,  0.0533,  ...,  0.0819, -0.0479,  0.0413],\n",
            "        [ 0.0018,  0.0017,  0.0665,  ...,  0.0305, -0.0328,  0.0395],\n",
            "        [ 0.0684,  0.0680, -0.0233,  ..., -0.0843,  0.0210, -0.0048],\n",
            "        ...,\n",
            "        [ 0.0426,  0.0572,  0.0769,  ...,  0.0747, -0.0692, -0.0192],\n",
            "        [-0.0844, -0.0154, -0.0440,  ...,  0.0752,  0.0493,  0.0474],\n",
            "        [-0.0704,  0.0309,  0.0879,  ...,  0.0749, -0.0801, -0.0064]],\n",
            "       requires_grad=True)\n",
            "outputs.bias Parameter containing:\n",
            "tensor([-0.0199,  0.0514,  0.0384, -0.0606, -0.0254, -0.0634,  0.0151,  0.0340,\n",
            "         0.0241,  0.0437], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "      print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMQPH3bkHWBt",
        "outputId": "c7cc1237-9050-4d70-8b08-949ae69f4d36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G1.weight\n",
            "G1.bias\n",
            "G2.weight\n",
            "G2.bias\n",
            "G3.weight\n",
            "G3.bias\n",
            "G4.weight\n",
            "G4.bias\n",
            "G5.weight\n",
            "G5.bias\n",
            "outputs.weight\n",
            "outputs.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lossFn = nn.CrossEntropyLoss()\n",
        "\n",
        "if args['cuda']:\n",
        "    model.cuda()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
        "\n",
        "for epoch in range(1, args['epochs'] + 1):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMIHeVK7HWhU",
        "outputId": "47266131-7fe9-45a0-f010-92de30e0011f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [59968/60000 (100%)]\tLoss: 0.000033\n",
            "\n",
            "Test set: Average loss: 49.7263, Accuracy: 9774/10000 (98%)\n",
            "\n",
            "Train Epoch: 2 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 63.0740, Accuracy: 9778/10000 (98%)\n",
            "\n",
            "Train Epoch: 3 [59968/60000 (100%)]\tLoss: 0.000001\n",
            "\n",
            "Test set: Average loss: 71.9596, Accuracy: 9787/10000 (98%)\n",
            "\n",
            "Train Epoch: 4 [59968/60000 (100%)]\tLoss: 0.000002\n",
            "\n",
            "Test set: Average loss: 82.7685, Accuracy: 9792/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 96.0699, Accuracy: 9780/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 111.0783, Accuracy: 9785/10000 (98%)\n",
            "\n",
            "Train Epoch: 7 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 118.0895, Accuracy: 9785/10000 (98%)\n",
            "\n",
            "Train Epoch: 8 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 126.6504, Accuracy: 9785/10000 (98%)\n",
            "\n",
            "Train Epoch: 9 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 154.4929, Accuracy: 9780/10000 (98%)\n",
            "\n",
            "Train Epoch: 10 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 161.5198, Accuracy: 9788/10000 (98%)\n",
            "\n",
            "Train Epoch: 11 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 165.5563, Accuracy: 9789/10000 (98%)\n",
            "\n",
            "Train Epoch: 12 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 169.7841, Accuracy: 9790/10000 (98%)\n",
            "\n",
            "Train Epoch: 13 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 175.2691, Accuracy: 9788/10000 (98%)\n",
            "\n",
            "Train Epoch: 14 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 193.0224, Accuracy: 9778/10000 (98%)\n",
            "\n",
            "Train Epoch: 15 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 239.0447, Accuracy: 9766/10000 (98%)\n",
            "\n",
            "Train Epoch: 16 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 206.9699, Accuracy: 9783/10000 (98%)\n",
            "\n",
            "Train Epoch: 17 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 206.3796, Accuracy: 9788/10000 (98%)\n",
            "\n",
            "Train Epoch: 18 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 206.9880, Accuracy: 9787/10000 (98%)\n",
            "\n",
            "Train Epoch: 19 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 207.2915, Accuracy: 9788/10000 (98%)\n",
            "\n",
            "Train Epoch: 20 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 208.4963, Accuracy: 9791/10000 (98%)\n",
            "\n",
            "Train Epoch: 21 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 208.4791, Accuracy: 9791/10000 (98%)\n",
            "\n",
            "Train Epoch: 22 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 210.2676, Accuracy: 9791/10000 (98%)\n",
            "\n",
            "Train Epoch: 23 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 211.5020, Accuracy: 9791/10000 (98%)\n",
            "\n",
            "Train Epoch: 24 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 213.1166, Accuracy: 9792/10000 (98%)\n",
            "\n",
            "Train Epoch: 25 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 214.8668, Accuracy: 9792/10000 (98%)\n",
            "\n",
            "Train Epoch: 26 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 217.7233, Accuracy: 9794/10000 (98%)\n",
            "\n",
            "Train Epoch: 27 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 219.4274, Accuracy: 9795/10000 (98%)\n",
            "\n",
            "Train Epoch: 28 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 279.3002, Accuracy: 9783/10000 (98%)\n",
            "\n",
            "Train Epoch: 29 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 273.7279, Accuracy: 9791/10000 (98%)\n",
            "\n",
            "Train Epoch: 30 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 272.4372, Accuracy: 9794/10000 (98%)\n",
            "\n",
            "Train Epoch: 31 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 272.2149, Accuracy: 9795/10000 (98%)\n",
            "\n",
            "Train Epoch: 32 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 272.4320, Accuracy: 9794/10000 (98%)\n",
            "\n",
            "Train Epoch: 33 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 272.6274, Accuracy: 9794/10000 (98%)\n",
            "\n",
            "Train Epoch: 34 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 272.9592, Accuracy: 9793/10000 (98%)\n",
            "\n",
            "Train Epoch: 35 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 273.2053, Accuracy: 9791/10000 (98%)\n",
            "\n",
            "Train Epoch: 36 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 273.6422, Accuracy: 9792/10000 (98%)\n",
            "\n",
            "Train Epoch: 37 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 273.9483, Accuracy: 9792/10000 (98%)\n",
            "\n",
            "Train Epoch: 38 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 274.3653, Accuracy: 9794/10000 (98%)\n",
            "\n",
            "Train Epoch: 39 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 274.2895, Accuracy: 9795/10000 (98%)\n",
            "\n",
            "Train Epoch: 40 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 274.2344, Accuracy: 9795/10000 (98%)\n",
            "\n",
            "Train Epoch: 41 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 273.6861, Accuracy: 9795/10000 (98%)\n",
            "\n",
            "Train Epoch: 42 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 272.7030, Accuracy: 9793/10000 (98%)\n",
            "\n",
            "Train Epoch: 43 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 275.5828, Accuracy: 9794/10000 (98%)\n",
            "\n",
            "Train Epoch: 44 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 273.3154, Accuracy: 9795/10000 (98%)\n",
            "\n",
            "Train Epoch: 45 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 271.0460, Accuracy: 9795/10000 (98%)\n",
            "\n",
            "Train Epoch: 46 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 269.8037, Accuracy: 9795/10000 (98%)\n",
            "\n",
            "Train Epoch: 47 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 268.5970, Accuracy: 9795/10000 (98%)\n",
            "\n",
            "Train Epoch: 48 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 267.6870, Accuracy: 9795/10000 (98%)\n",
            "\n",
            "Train Epoch: 49 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 267.2905, Accuracy: 9794/10000 (98%)\n",
            "\n",
            "Train Epoch: 50 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 264.8668, Accuracy: 9793/10000 (98%)\n",
            "\n",
            "Train Epoch: 51 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 265.6323, Accuracy: 9793/10000 (98%)\n",
            "\n",
            "Train Epoch: 52 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 262.8247, Accuracy: 9793/10000 (98%)\n",
            "\n",
            "Train Epoch: 53 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 261.5369, Accuracy: 9792/10000 (98%)\n",
            "\n",
            "Train Epoch: 54 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 260.6529, Accuracy: 9792/10000 (98%)\n",
            "\n",
            "Train Epoch: 55 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 261.0405, Accuracy: 9794/10000 (98%)\n",
            "\n",
            "Train Epoch: 56 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 258.6430, Accuracy: 9795/10000 (98%)\n",
            "\n",
            "Train Epoch: 57 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 257.5521, Accuracy: 9793/10000 (98%)\n",
            "\n",
            "Train Epoch: 58 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 258.0768, Accuracy: 9793/10000 (98%)\n",
            "\n",
            "Train Epoch: 59 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 255.5495, Accuracy: 9793/10000 (98%)\n",
            "\n",
            "Train Epoch: 60 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 258.9989, Accuracy: 9793/10000 (98%)\n",
            "\n",
            "Train Epoch: 61 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 253.5460, Accuracy: 9793/10000 (98%)\n",
            "\n",
            "Train Epoch: 62 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 252.3033, Accuracy: 9793/10000 (98%)\n",
            "\n",
            "Train Epoch: 63 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 251.2902, Accuracy: 9790/10000 (98%)\n",
            "\n",
            "Train Epoch: 64 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 250.4899, Accuracy: 9791/10000 (98%)\n",
            "\n",
            "Train Epoch: 65 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 249.3661, Accuracy: 9791/10000 (98%)\n",
            "\n",
            "Train Epoch: 66 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 248.4017, Accuracy: 9790/10000 (98%)\n",
            "\n",
            "Train Epoch: 67 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 249.5149, Accuracy: 9789/10000 (98%)\n",
            "\n",
            "Train Epoch: 68 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 249.7583, Accuracy: 9788/10000 (98%)\n",
            "\n",
            "Train Epoch: 69 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 248.5784, Accuracy: 9790/10000 (98%)\n",
            "\n",
            "Train Epoch: 70 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 244.6989, Accuracy: 9788/10000 (98%)\n",
            "\n",
            "Train Epoch: 71 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 245.3678, Accuracy: 9788/10000 (98%)\n",
            "\n",
            "Train Epoch: 72 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 242.6535, Accuracy: 9787/10000 (98%)\n",
            "\n",
            "Train Epoch: 73 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 243.5958, Accuracy: 9786/10000 (98%)\n",
            "\n",
            "Train Epoch: 74 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 261.4502, Accuracy: 9790/10000 (98%)\n",
            "\n",
            "Train Epoch: 75 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 283.3650, Accuracy: 9789/10000 (98%)\n",
            "\n",
            "Train Epoch: 76 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 282.0587, Accuracy: 9788/10000 (98%)\n",
            "\n",
            "Train Epoch: 77 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 281.4972, Accuracy: 9786/10000 (98%)\n",
            "\n",
            "Train Epoch: 78 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 281.2274, Accuracy: 9785/10000 (98%)\n",
            "\n",
            "Train Epoch: 79 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 282.2599, Accuracy: 9787/10000 (98%)\n",
            "\n",
            "Train Epoch: 80 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 280.8226, Accuracy: 9786/10000 (98%)\n",
            "\n",
            "Train Epoch: 81 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 280.7397, Accuracy: 9786/10000 (98%)\n",
            "\n",
            "Train Epoch: 82 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 281.6837, Accuracy: 9787/10000 (98%)\n",
            "\n",
            "Train Epoch: 83 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 280.7599, Accuracy: 9788/10000 (98%)\n",
            "\n",
            "Train Epoch: 84 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 280.9477, Accuracy: 9787/10000 (98%)\n",
            "\n",
            "Train Epoch: 85 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 284.0662, Accuracy: 9788/10000 (98%)\n",
            "\n",
            "Train Epoch: 86 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 280.9265, Accuracy: 9789/10000 (98%)\n",
            "\n",
            "Train Epoch: 87 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 280.9336, Accuracy: 9789/10000 (98%)\n",
            "\n",
            "Train Epoch: 88 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 280.9472, Accuracy: 9788/10000 (98%)\n",
            "\n",
            "Train Epoch: 89 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 281.1006, Accuracy: 9788/10000 (98%)\n",
            "\n",
            "Train Epoch: 90 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 280.7365, Accuracy: 9787/10000 (98%)\n",
            "\n",
            "Train Epoch: 91 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 280.5574, Accuracy: 9788/10000 (98%)\n",
            "\n",
            "Train Epoch: 92 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 282.1611, Accuracy: 9787/10000 (98%)\n",
            "\n",
            "Train Epoch: 93 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 280.2987, Accuracy: 9787/10000 (98%)\n",
            "\n",
            "Train Epoch: 94 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 282.9493, Accuracy: 9787/10000 (98%)\n",
            "\n",
            "Train Epoch: 95 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 280.0119, Accuracy: 9787/10000 (98%)\n",
            "\n",
            "Train Epoch: 96 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 282.0010, Accuracy: 9789/10000 (98%)\n",
            "\n",
            "Train Epoch: 97 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 279.6406, Accuracy: 9790/10000 (98%)\n",
            "\n",
            "Train Epoch: 98 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 279.4489, Accuracy: 9790/10000 (98%)\n",
            "\n",
            "Train Epoch: 99 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 279.2708, Accuracy: 9790/10000 (98%)\n",
            "\n",
            "Train Epoch: 100 [59968/60000 (100%)]\tLoss: 0.000000\n",
            "\n",
            "Test set: Average loss: 279.1579, Accuracy: 9790/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}